### SOLIDISSIMA GBS DATA

### GOALS AND STEPS (to be fleshed out)
#Fast QC and Multiqc on solidissima
	#Compare to similis
#Trim solidissima
	#Does it need the same weird trimming or not?
	#Align to transcriptome?
#Ddocent
#Compare shared samples for batch effects

#Search for TO DO for steps I have not done yet but plan to


#Date: Nov 27th
#15 CPU occupied

#QC on similis and solidissima
export PATH=/programs/FastQC-0.11.8:$PATH
export LC_ALL=en_US.UTF-8
export PYTHONPATH=/programs/multiqc-1.10.1/lib64/python3.6/site-packages:/programs/multiqc-1.10.1/lib/python3.6/site-packages
export PATH=/programs/multiqc-1.10.1/bin:$PATH

fastqc -t 30 *.fastq.gz  -o rawqc
multiqc -n multiqc_rawsolfas_report.html -o raw_multiqc .

#Start thinking about sites and meaning

#Site	#Region
413		GBE
536		NJ (?)
BAR		CCB
BLP		SLI
BRW		CCB
CGC		CCB
CTM		SCC
FNJ		NJ (Delaware?)
GBE		GBE
MCX		SLI
MW		SCC
NAN		SCC	 (Very offshore south)
PEC		NLI
PLY		CCB  (West side of bay)
PT		CCB
PVT		CCB
SHN		SLI
SsLI	SLI
WV		SCC	 (Very West)		

#What next?
	#1.Realign transcriptome similis using paired-end data
	#Date: Nov 27th
		#Use the same data from dDoc_928
		cp dDoc_928/undedup/*.fq.gz GBS2transcriptome/dDoc_aln2transcriptome_1027/
			#Copied and properly duplicated and trimmed
		#Then copy everything else from dDoc_aln_nov13 for the other elements of the run
		#Use 20 cores (48-15-20=13 cores for other stuff)
		#Don't need, they're zipped cp j_zip.txt ../dDoc_aln2transcriptome_1027/
		cp dDoc1113.sh ../dDoc_aln2transcriptome_1027/dDoc1127.sh
		cp config_1113.txt ../dDoc_aln2transcriptome_1027/config_1127.txt
			#Fix MW_017
			mv  MW_01.F.fq.gz MW_017.F.fq.gz
			mv  MW_01.R.fq.gz MW_017.R.fq.gz
			mv  MW_01.R1.fq.gz MW_017.R1.fq.gz
			mv  MW_01.R2.fq.gz MW_017.R2.fq.gz
			#Rename to transcriptome_1127
	#2.Edit number of cores on dDoc run line and config file and run
	#Date: Nov 27th
			#Need reference too
sbatch --nodes=1 --ntasks=20 --mem=120000 \
--job-name=dDoc1127_th20_mem120 --output=dDoc1127.out dDoc1127.sh
		#Wait for fastqc to finish
	#3.When setting up the renaming in excel for solidis, just do left three and then data to columns with _
		#But do not do it with the basic ones, because I just want that after I've fully trimmed them
	#4.Trim Solidissima
	#Date: Nov 27th
		#Does it need the same weird trimming?
			#Pull up the similis multiqc to compare
				#Different versions of multiqc
					#Solidissima done with This is MultiQC v1.10.1
					#Redo similis with that is easiest I suppose
							#Not sure about this because the version is specified, maybe it's an issue of sample size?
						export PYTHONPATH=/programs/multiqc-1.10.1/lib64/python3.6/site-packages:/programs/multiqc-1.10.1/lib/python3.6/site-packages
						export PATH=/programs/multiqc-1.10.1/bin:$PATH
						multiqc -n multiqc_rawsim_temp_report.html -o temp ./similis_seq_raw/raw_fastqc/
							#Yeah, it's a sample size issue
				#COMPARISON: They are similar but the R2 5' end quality is higher but all other issues with the R2 persist. I could try to trim the other way but no thanks - consistency
			#Are the 5' ends a mess again or not?
				#If yes, follow the same protocol
				#If no, go to the email and get the adapter trimmer and do that and then trimmmomatic, with no cutadapt
			#Rerun multiqc after trimming and resyncing
		#Make sample list
			ls *fastq.gz > list #Then edit in excel, remove duplicates
			sol_sample.txt
			#Make this into parallel script? (doing each step for all samples before next step)
				#Change the threading counts on trimmomatic
				#Grab the gbstrim script
		#Make directories
		mkdir solidis_clean
		mkdir solidis_clean/padtrim
		mkdir solidis_clean/gbstrim_stats
		mkdir solidis_clean/ommatic
		mkdir solidis_clean/ommatic_logs
		mkdir solidis_clean/trim_R1s_phase2
		mkdir solidis_clean/ommatic_phase2
		mkdir solidis_clean/sync_trim_6


#START SCRIPT 
#!/bin/bash -l

#SBATCH --partition=regular
#SBATCH --job-name=trim_seq_10t_40G
#SBATCH --output=s_solidistrim.out
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --mail-user=hh693@cornell.edu
export PATH=/programs/seqtk:$PATH
export PATH=/programs/seqkit-0.15.0:$PATH
while read p; do
echo "$p"
#DO NOT remove duplicate reads
#seqkit rmdup similis_seq_raw/"$p"_R1_001.fastq.gz -s -o dedup2_raw/"$p"_R1.rawdedup.fastq -D dups2/"$p"_R1_dups.txt -d dups2/"$p"_R1_dup_seq.fastq -j 10
#seqkit rmdup similis_seq_raw/"$p"_R2_001.fastq.gz -s -o dedup2_raw/"$p"_R2.rawdedup.fastq -D dups2/"$p"_R2_dups.txt -d dups2/"$p"_R2_dup_seq.fastq -j 10	
#remove padding on R1 #possible that I should do this for R2 and just add back in the no cut sites but seems unnessecary
perl gbstrim.pl --enzyme1 psti --enzyme2 mspi --fastqfile solidis_seq_raw/"$p"_R1_001.fastq.gz --read R1 --outputfile solidis_clean/padtrim/"$p"_R1.trim.fastq --verbose --threads 10 --minlength 50 >& solidis_clean/gbstrim_stats/"$p"_R1.log
#remove adapter and padding on R2 and quality check
java -jar /programs/trimmomatic/trimmomatic-0.39.jar SE -phred33 -threads 8 solidis_seq_raw/"$p"_R2_001.fastq.gz solidis_clean/ommatic/"$p"_R2.fastq ILLUMINACLIP:/programs/trimmomatic/adapters/NexteraPE-PE.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:50 >& solidis_clean/ommatic_logs/"$p"_R2_trimmo.log
seqtk trimfq -b 8 -e 8 solidis_clean/ommatic/"$p"_R2.fastq > solidis_clean/ommatic_phase2/"$p"_R2.fastq
#quality check R1
java -jar /programs/trimmomatic/trimmomatic-0.39.jar SE -phred33 -threads 8 solidis_clean/padtrim/"$p"_R1.trim.fastq solidis_clean/trim_R1s_phase2/"$p"_R1.trim.fastq SLIDINGWINDOW:4:15 MINLEN:50 >& solidis_clean/ommatic_logs/"$p"_R1_trimmo.log
#resync
perl resync.pl solidis_clean/trim_R1s_phase2/"$p"_R1.trim.fastq solidis_clean/ommatic_phase2/"$p"_R2.fastq solidis_clean/sync_trim_6/"$p"_R1.trim.sync.fastq solidis_clean/sync_trim_6/"$p"_R2.trim.sync.fastq >& solidis_clean/sync_trim_6/"$p"_resync6.log
done <sol_sample.txt 
#END SCRIPT
#Could parallelize but not important to cause it takes only about an hour anywho
			#Nevermind, turns out it takes FOREVER
			#Just kidding (56min = 40 samples > 1.4 min/sample * 433 samples > 606 min > 10 hours)

sbatch --ntasks=10 --mem=40000 s_solidistrim.sh 

tail s_solidistrim.out
#To check how far along

	#Did not include trimming for STACKS in the loop (only include if needed)
	export PYTHONPATH=/programs/cutadapt-3.4/lib/python3.6/site-packages:/programs/cutadapt-3.4/lib64/python3.6/site-packages
	export PATH=/programs/cutadapt-3.4/bin:$PATH
	while read p; do
	echo "$p"
	cutadapt -l 125 -m 125 -j 8 -o solidis_clean/STACKS_cut6/"$p"_R1.lengthtrim.fastq -p solidis_clean/STACKS_cut6/"$p"_R2.lengthtrim.fastq solidis_clean/sync_trim_6/"$p"_R1.trim.sync.fastq solidis_clean/sync_trim_6/"$p"_R2.trim.sync.fastq
	perl resync.pl solidis_clean/STACKS_cut6/"$p"_R1.lengthtrim.fastq solidis_clean/STACKS_cut6/"$p"_R2.lengthtrim.fastq solidis_clean/resync_STACKS_cut6/"$p"_R1.trim.sync.fastq solidis_clean/resync_STACKS_cut6/"$p"_R2.trim.sync.fastq >& solidis_clean/resync_STACKS_cut6/"$p"_resync6cut.log 
	done <sim_sample.txt
		
	#5.Mark failed sequences from QC
	#Date: Nov 28th
		#Export data from the .txt files created in the folder by multiqc
		#Sequence Counts is the main data I would base it on > 1M seq
			#In excel

	#6.Zip Files in parallel
	#Date: Nov 28th
	parallel -j 24 < j_para4.txt 
	#then move to new names
	rename R1.fastq.gz R1.fq.gz *R1.fastq.gz 
	
	#7.dDocent with some solidissima
	#Date: Nov 28th
		#Create catalog with not all of them
			#Which ones do I not want? > Done in excel
			#640/2 > 320 after removing the baddies
				#Do fewer
					#558/2 > 279 by border ones
				#Randomly remove so that there's approximately the same number per site
					#352/2 > 176 <- still a lot but given that I have >15 sites, I think this is okay...?
					#I want fast, take out a few more per site
					#Remove all 1999 samples
					#132/2 > 66
			#Create Copies for F/R

source /programs/miniconda3/bin/activate dDocent-2.8.13
dDocent
		#Run interactive
			#yes	#samples = 66
			#23		#cores
			#no		#no trim
			#yes	#assemble
			#PE		#paired end
			#no (default c=0.9)
			#no		#map reads
			#no		#call SNPs
		#Cut-offs
			#5 for minimum coverage
			#6 for individuals
		#bg
Press control and Z simultaneously
Type 'bg' without the quotes and press enter
Type 'disown -h' again without the quotes and press enter

		#Looks like it worked! Produced a large reference file, however, is it much larger than my reference from similis?
			#120k vs 6k lines??
			dDocent assembled 228768 sequences (after cutoffs) into 62302 contigs
			#I have no reason to think it failed
			#Although, even though it said c default is 0.9, but it looked like in htop that it was 0.8 

	#8.Align to dDoc solidissima
	#Date: Nov 29th
		#R and F of all samples and move them all to one directory
		#Run Interactive Again
			yes
			24
			no
			no
			yes
			yes
			1
			4
			6
			yes
		#How long it gonna take?
			#Start at Mon Nov 29 06:14:08 EST 2021
			#By 8:24 AM it had finished 43 > 3 min per sample > 433 samples > 1310 min > 22 hours
			#As of November 30th 9:30AM, it is using bedtools to concatenate cat-RGG.bam
				#Actually just finished that (took about 2:30 hours), now moving on to individual sample bams
					#Which means that it probably has yet to do any freebayes - buy more time
				#Now onto freebayes as of 10:20 or so
				#Sidebar: doing control+Z and then bg works for moving other commands to the background too
				#For some reason it is only doing bedtools with -j 12? Why?
			#I just lost access to the server and it overloaded the ram
				#Restart at the the freebayes step
	#Date: Dec 1st
			#Freebayes did not work, not sure what happened, restart and if it doesn't work again, just move what I have so far back to the main server and maybe try just running freebayes
			#Does it take up all of them memory/RAM?
			#Running it on 10 core stays pretty stable at <60G (~42G-59G)
		#Try rerunning ddoc with each step intentionally separate
		#Just align
			#Starting in a new folder with just the reads and the reference
			#Run on full cores
				#3 in 10 minutes > 433 in 24 hours
				#Yeah, this is taking freaking forever, makes me think it didn't work correctly last time.
				#94 in 4 hr 35 min > 94 in 275 min > 21 hours - 4.5 hours so far > 16.6 more
					#3 minutes per one
					#7 hours left
			#Then run freebayes on 15-18 cores
			#19 currently in use on main server
	#Date: Dec 2nd
		#TO DO: mv /fs/cbsuhare/storage/Spisula/Mounting/unmount_dDoc_solidis_1130/* /workdir/hh693/dDoc_sol_121/
			cp *bam* /fs/cbsuhare/storage/Spisula/Mounting/unmount_dDoc_solidis_1130/dDoc_1128/bwa122 &
			cp *bwa* /fs/cbsuhare/storage/Spisula/Mounting/unmount_dDoc_solidis_1130/dDoc_1128/bwa122 &
			#Copied up through NAN_027 so I can copy fewer next time
			#Sent to folder called bwa122
		
		cp -r align121/ /fs/cbsuhare/storage/Spisula/Mounting/unmount_dDoc_solidis_1130/backupalign121 &
		cp -r align121/ /fs/cbsuhare/storage/Spisula/Mounting/unmount_dDoc_solidis_1130/ &
		#For now, I'm only going to copy align121 because that's the important part for freebayes tonight
		#mv /fs/cbsuhare/storage/Spisula/Mounting/unmount_dDoc_solidis_1130/align121/ /workdir/hh693/dDoc_sol_121/

#!/bin/bash -l

#SBATCH --partition=regular
#SBATCH --job-name=dDoc121_sol_th18_mem175
#SBATCH --output=dDoc121_sol.out
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --mail-user=hh693@cornell.edu
source /programs/miniconda3/bin/activate dDocent-2.8.13
dDocent config_freebonly.txt

sbatch --nodes=1 --ntasks=15 --mem=120000 s_freeb_sol.sh

	#Date: Dec 3rd
		#Freebayes output says 20% done as of 8:10am with 1 day 5 hours remaining
	#Date: Dec 4th
		#It got to 72% and then failed due to lack of memory
		7m72% 482:183=13h59m37s 518
		ERROR: freebayes instance DID NOT COMPLETE
		See below:
		/usr/bin/bash: line 1: 45167 Killed                  freebayes -b cat-RRG.bam -t mapped.$1.bed -v raw.$1.vcf -f reference.fasta -m 5 -q 5 -E 3 --min-repeat-entropy 1 -V --populations popmap -n 10 -F 0.1 &>fb.$1.error.log
		parallel: This job failed:
		call_genos 182 
		multiple instances of freebayes  failed.  dDocent will now recalibrate run parameters to use less memory.
		Using FreeBayes to call SNPs again
	#Try again on 10 cores with 150mem
		#8 mem per core got us to 72%
		#11 mem per core (math) to 100?
		#This is 15 mem per core. Ugh so stupid
		#mv all mapped.bed files and fb error logs to a subfolder

sbatch --nodes=1 --ntasks=10 --mem=150000 s_freeb_sol.sh

	#9.Align to Solidissima Transcriptome
	#TO DO
		#What percentage of reads aligned to transcriptome > bwa log?
		
	#10.Filter SNPS
		#a. From similis and transcriptome
		#Date: Nov 29th
			#Follow the same protocol from less stringent filter
			#8-10 cores available
			#Filtering same protocol from undedup dDoc_928
			#Filter name filterDoc_T1129
cp GBS2transcriptome/dDoc_aln2transcriptome_1127/TotalRawSNPs.vcf filterDoc_T1129
cp GBS2transcriptome/dDoc_aln2transcriptome_1127/popmap filterDoc_T1129
vcftools --vcf TotalRawSNPs.vcf --max-missing 0.5 --mac 3 --minQ 30 --recode --recode-INFO-all --out raw.g5mac3
vcftools --vcf raw.g5mac3.recode.vcf --minDP 3 --recode --recode-INFO-all --out raw.g5mac3dp3 
vcftools --vcf raw.g5mac3dp3.recode.vcf --missing-indv
less out.imiss
awk '$5>0.5' out.imiss | cut -f1 > lowDP.indv 
vcftools --vcf raw.g5mac3dp3.recode.vcf --remove lowDP.indv --recode --recode-INFO-all --out raw.g5mac3dplm
vcftools --vcf raw.g5mac3dplm.recode.vcf --max-missing 0.85 --min-meanDP 20 --recode --recode-INFO-all --out DP3g95maf05 --min-meanDP 20
#21273 out of a possible 76285 Sites

#create better keep lists
awk '$2 == "GBE\r" || $2 == "SFJ\r" || $2 == "CCB\r" || awk $2 == "SLI\r"' popmap > 1.keep 
awk '$2 == "SCC\r"' popmap > 5.keep
awk '$2 == "NLI\r"' popmap > 6.keep && awk '$2 == "GA\r"' popmap > 7.keep
#remove specific ones
nano 5.keep
#confirm which to remove
	#CTM_008
	#MW_001
	#MW_002
	#MW_016
	#MW_033
	#FIX MW_017
for f in {5..7}
do
	vcftools --vcf DP3g95maf05.recode.vcf --keep $f.keep --missing-site --out $f
done
cat 5.lmiss 6.lmiss 7.lmiss | awk '!/CHR/' | awk '$6 > 0.1' | cut -f1,2 >> badloci_simonly
cp ../../filterDoc_105/soldis4rm.indv .
nano soldis4rm.indv
#above plus:
BLP0819_180
BLP0819_182 
BLP0819_179
vcftools --vcf DP3g95maf05.recode.vcf --exclude-positions badloci_simonly --remove soldis4rm.indv --recode --recode-INFO-all --out DP3g95p5maf05_simonly
#Sloppy, do fixed similis first and then all
awk '$2 == "GBE\r" || $2 == "SFJ\r" || $2 == "CCB\r"' popmap > 1.keep 
awk '$2 == "SLI\r"' popmap > 4.keep && awk '$2 == "SCC\r"' popmap > 5.keep
awk '$2 == "NLI\r"' popmap > 6.keep && awk '$2 == "GA\r"' popmap > 7.keep
for f in {1..7}
do
	vcftools --vcf DP3g95maf05.recode.vcf --keep $f.keep --missing-site --out $f
done
cat 1.lmiss 4.lmiss 5.lmiss 6.lmiss 7.lmiss | awk '!/CHR/' | awk '$6 > 0.1' | cut -f1,2 >> badloci
vcftools --vcf DP3g95maf05.recode.vcf --exclude-positions badloci --recode --recode-INFO-all --out DP3g95p5maf05
		#Allele Balance
/programs/vcflib-1.0.1/bin/vcffilter -s -f "AB > 0.2 & AB < 0.8 | AB < 0.01" DP3g95p5maf05.recode.vcf > DP3g95p5maf05.fil1.vcf
/programs/vcflib-1.0.1/bin/vcffilter -s -f "AB > 0.2 & AB < 0.8 | AB < 0.01" DP3g95p5maf05_simonly.recode.vcf > DP3g95p5maf05_simonly.fil1.vcf
		#check how many loci now
awk '!/#/' DP3g95p5maf05.recode.vcf | wc -l && awk '!/#/' DP3g95p5maf05.fil1.vcf | wc -l
awk '!/#/' DP3g95p5maf05_simonly.recode.vcf | wc -l && awk '!/#/' DP3g95p5maf05_simonly.fil1.vcf | wc -l
	#The next filter we will apply filters out sites that have reads from both strands.
/programs/vcflib-1.0.1/bin/vcffilter -f "SAF / SAR > 100 & SRF / SRR > 100 | SAR / SAF > 100 & SRR / SRF > 100" -s DP3g95p5maf05.fil1.vcf > DP3g95p5maf05.fil2.vcf
awk '!/#/' DP3g95p5maf05.fil2.vcf | wc -l
/programs/vcflib-1.0.1/bin/vcffilter -f "SAF / SAR > 100 & SRF / SRR > 100 | SAR / SAF > 100 & SRR / SRF > 100" -s DP3g95p5maf05_simonly.fil1.vcf > DP3g95p5maf05_simonly.fil2.vcf
awk '!/#/' DP3g95p5maf05_simonly.fil2.vcf | wc -l
	#The next filter looks at the ratio of mapping qualities between reference and alternate alleles
	#The rationale here is that, again, because RADseq loci and alleles all should start from the same genomic location there should not be large discrepancy between the mapping qualities of two alleles.
/programs/vcflib-1.0.1/bin/vcffilter -f "MQM / MQMR > 0.9 & MQM / MQMR < 1.05" DP3g95p5maf05.fil2.vcf > DP3g95p5maf05.fil3.vcf
awk '!/#/' DP3g95p5maf05.fil3.vcf | wc -l
/programs/vcflib-1.0.1/bin/vcffilter -f "MQM / MQMR > 0.9 & MQM / MQMR < 1.05" DP3g95p5maf05_simonly.fil2.vcf > DP3g95p5maf05_simonly.fil3.vcf
awk '!/#/' DP3g95p5maf05_simonly.fil3.vcf | wc -l
	#Yet another filter that can be applied is whether or not their is a discrepancy in the properly paired status of for reads supporting reference or alternate alleles.
/programs/vcflib-1.0.1/bin/vcffilter -f "PAIRED > 0.05 & PAIREDR > 0.05 & PAIREDR / PAIRED < 1.75 & PAIREDR / PAIRED > 0.25 | PAIRED < 0.05 & PAIREDR < 0.05" -s DP3g95p5maf05.fil3.vcf > DP3g95p5maf05.fil4.vcf			
awk '!/#/' DP3g95p5maf05.fil4.vcf | wc -l
/programs/vcflib-1.0.1/bin/vcffilter -f "PAIRED > 0.05 & PAIREDR > 0.05 & PAIREDR / PAIRED < 1.75 & PAIREDR / PAIRED > 0.25 | PAIRED < 0.05 & PAIREDR < 0.05" -s DP3g95p5maf05_simonly.fil3.vcf > DP3g95p5maf05_simonly.fil4.vcf			
awk '!/#/' DP3g95p5maf05_simonly.fil4.vcf | wc -l
	#In short, with whole genome samples, it was found that high coverage can lead to inflated locus quality scores. Heng proposed that for read depths greater than the mean depth plus 2-3 times the square root of mean depth that the quality score will be twice as large as the depth in real variants and below that value for false variants.
	#I actually found that this is a little too conservative for RADseq data, likely because the reads aren’t randomly distributed across contigs. I implement two filters based on this idea. the first is removing any locus that has a quality score below 1/4 of the depth.
		#This part is complicated - do it with just all samples first
/programs/vcflib-1.0.1/bin/vcffilter -f "QUAL / DP > 0.25" DP3g95p5maf05.fil4.vcf > DP3g95p5maf05.fil5.vcf
cut -f8 DP3g95p5maf05.fil5.vcf | grep -oe "DP=[0-9]*" | sed -s 's/DP=//g' > DP3g95p5maf05.fil5.DEPTH
awk '!/#/' DP3g95p5maf05.fil5.vcf | cut -f1,2,6 > DP3g95p5maf05.fil5.vcf.loci.qual
awk '{ sum += $1; n++ } END { if (n > 0) print sum / n; }' DP3g95p5maf05.fil5.DEPTH
		10212.2 #mean depth			#his was 1.9k so this is a lot higher
#Now the the mean plus 3X the square root of the mean - done myself in google/wolfram
		10212.2 + 3*sqrt(10212.2) = 10515.4
		#Insert that value
paste DP3g95p5maf05.fil5.vcf.loci.qual DP3g95p5maf05.fil5.DEPTH | awk -v x=10515 '$4 > x' | awk '$3 < 2 * $4' > DP3g95p5maf05.fil5.lowQDloci
vcftools --vcf DP3g95p5maf05.fil5.vcf --site-depth --exclude-positions DP3g95p5maf05.fil5.lowQDloci --out DP3g95p5maf05.fil5
cut -f3 DP3g95p5maf05.fil5.ldepth > DP3g95p5maf05.fil5.site.depth
	#Now let’s calculate the average depth by dividing the above file by the number of individuals 31
		#I have 137 individuals
awk '!/D/' DP3g95p5maf05.fil5.site.depth | awk -v x=137 '{print $1/x}' > meandepthpersite
vcftools --vcf  DP3g95p5maf05.fil5.vcf --recode-INFO-all --out DP3g95p5maf05.FIL --max-meanDP 200 --exclude-positions DP3g95p5maf05.fil5.lowQDloci --recode 
		7552 out of a possible 8234 Sites
	#repeat with similis only
/programs/vcflib-1.0.1/bin/vcffilter -f "QUAL / DP > 0.25" DP3g95p5maf05_simonly.fil4.vcf > DP3g95p5maf05_simonly.fil5.vcf
cut -f8 DP3g95p5maf05_simonly.fil5.vcf | grep -oe "DP=[0-9]*" | sed -s 's/DP=//g' > DP3g95p5maf05_simonly.fil5.DEPTH
awk '!/#/' DP3g95p5maf05_simonly.fil5.vcf | cut -f1,2,6 > DP3g95p5maf05_simonly.fil5.vcf.loci.qual
awk '{ sum += $1; n++ } END { if (n > 0) print sum / n; }' DP3g95p5maf05_simonly.fil5.DEPTH
		9205.52
#Now the the mean plus 3X the square root of the mean - done myself in google/wolfram
		9493.36
paste DP3g95p5maf05_simonly.fil5.vcf.loci.qual DP3g95p5maf05_simonly.fil5.DEPTH | awk -v x=9493 '$4 > x' | awk '$3 < 2 * $4' > DP3g95p5maf05_simonly.fil5.lowQDloci
vcftools --vcf DP3g95p5maf05_simonly.fil5.vcf --site-depth --exclude-positions DP3g95p5maf05_simonly.fil5.lowQDloci --out DP3g95p5maf05_simonly.fil5
cut -f3 DP3g95p5maf05_simonly.fil5.ldepth > DP3g95p5maf05_simonly.fil5.site.depth
	#Now let’s calculate the average depth by dividing the above file by the number of individuals 31
		#I have 125 individuals (Might have done this wrong for Doc105)
awk '!/D/' DP3g95p5maf05_simonly.fil5.site.depth | awk -v x=125 '{print $1/x}' > meandepthpersite_simonly
vcftools --vcf  DP3g95p5maf05_simonly.fil5.vcf --recode-INFO-all --out DP3g95p5maf05_simonly.FIL --max-meanDP 200 --exclude-positions DP3g95p5maf05_simonly.fil5.lowQDloci --recode 
#No HWE
#filter one SNP per contig
	grep -v "#" DP3g95p5maf05_simonly.FIL.recode.vcf | wc -l && grep -v "#" DP3g95p5maf05_simonly.FIL.recode.vcf | sort -k1,1 -u | wc -l
	grep -v "#" DP3g95p5maf05.FIL.recode.vcf | wc -l && grep -v "#" DP3g95p5maf05.FIL.recode.vcf | sort -k1,1 -u | wc -l	
#save to new file
grep "#" DP3g95p5maf05_simonly.FIL.recode.vcf > SNP_simonly.head.vcf
grep -v "#" DP3g95p5maf05_simonly.FIL.recode.vcf | sort -k1,1 -u > SNP_simonly.data.vcf
cat SNP_simonly.head.vcf SNP_simonly.data.vcf > SNP_JPfilter1129noLD_simonly.vcf
grep "#" DP3g95p5maf05.FIL.recode.vcf > SNP.head.vcf
grep -v "#" DP3g95p5maf05.FIL.recode.vcf | sort -k1,1 -u > SNP.data.vcf
cat SNP.head.vcf SNP.data.vcf > SNP_JPfilter1129noLD.vcf
#However, these still include multiallelic sites > sloppy as the first time			
			
		#b. From solidissima/ddocent
		#Date: Dec 10th
mkdir filterDoc_sol1212
2637150
#make new popmap - done by site codes
#sneaky sims list
vcftools --vcf TotalRawSNPs.vcf --max-missing 0.5 --mac 3 --minQ 30 --recode --recode-INFO-all --out raw.g5mac3
vcftools --vcf raw.g5mac3.recode.vcf --minDP 3 --recode --recode-INFO-all --out raw.g5mac3dp3 
vcftools --vcf raw.g5mac3dp3.recode.vcf --missing-indv
less out.imiss
awk '$5>0.5' out.imiss | cut -f1 > lowDP.indv 
vcftools --vcf raw.g5mac3dp3.recode.vcf --remove lowDP.indv --recode --recode-INFO-all --out raw.g5mac3dplm
vcftools --vcf raw.g5mac3dplm.recode.vcf --max-missing 0.85 --min-meanDP 20 --recode --recode-INFO-all --out DP3g95maf05 --min-meanDP 20

awk '$2 == "GBE\r"' popmap > 1.keep
awk '$2 == "413\r"' popmap > 2.keep
awk '$2 == "536\r"' popmap > 3.keep
awk '$2 == "CCB\r"' popmap > 4.keep
awk '$2 == "SLI\r"' popmap > 5.keep
awk '$2 == "SIM\r"' popmap > 6.keep #all sim
awk '$2 == "SCC\r"' popmap > 7.keep
awk '$2 == "NAN\r"' popmap > 8.keep
awk '$2 == "FNJ\r"' popmap > 9.keep

for f in {1..9}
do
	vcftools --vcf DP3g95maf05.recode.vcf --keep $f.keep --missing-site --out $f
done
#solonly
cat 1.lmiss 2.lmiss 5.lmiss 3.lmiss 7.lmiss 4.lmiss 8.lmiss 9.lmiss | awk '!/CHR/' | awk '$6 > 0.1' | cut -f1,2 >> badloci_solonly
#all
cat 1.lmiss 2.lmiss 5.lmiss 3.lmiss 6.lmiss 7.lmiss 4.lmiss 8.lmiss 9.lmiss | awk '!/CHR/' | awk '$6 > 0.1' | cut -f1,2 >> badloci_all

cp ../filterDoc_105/soldis4rm.indv .
nano soldis4rm.indv
#save to sim4rm.indv

vcftools --vcf DP3g95maf05.recode.vcf --exclude-positions badloci_solonly --remove sim4rm.indv --recode --recode-INFO-all --out DP3g95p5maf05_solonly
vcftools --vcf DP3g95maf05.recode.vcf --exclude-positions badloci_all --recode --recode-INFO-all --out DP3g95p5maf05

		#Allele Balance
/programs/vcflib-1.0.1/bin/vcffilter -s -f "AB > 0.2 & AB < 0.8 | AB < 0.01" DP3g95p5maf05.recode.vcf > DP3g95p5maf05.fil1.vcf
	#Sloppily I start labeling solidissima to simonly to speed up without having to edit everythin
/programs/vcflib-1.0.1/bin/vcffilter -s -f "AB > 0.2 & AB < 0.8 | AB < 0.01" DP3g95p5maf05_solonly.recode.vcf > DP3g95p5maf05_simonly.fil1.vcf
		#check how many loci now
awk '!/#/' DP3g95p5maf05.recode.vcf | wc -l && awk '!/#/' DP3g95p5maf05.fil1.vcf | wc -l
awk '!/#/' DP3g95p5maf05_simonly.recode.vcf | wc -l && awk '!/#/' DP3g95p5maf05_simonly.fil1.vcf | wc -l
	#The next filter we will apply filters out sites that have reads from both strands.
/programs/vcflib-1.0.1/bin/vcffilter -f "SAF / SAR > 100 & SRF / SRR > 100 | SAR / SAF > 100 & SRR / SRF > 100" -s DP3g95p5maf05.fil1.vcf > DP3g95p5maf05.fil2.vcf
awk '!/#/' DP3g95p5maf05.fil2.vcf | wc -l
/programs/vcflib-1.0.1/bin/vcffilter -f "SAF / SAR > 100 & SRF / SRR > 100 | SAR / SAF > 100 & SRR / SRF > 100" -s DP3g95p5maf05_simonly.fil1.vcf > DP3g95p5maf05_simonly.fil2.vcf
awk '!/#/' DP3g95p5maf05_simonly.fil2.vcf | wc -l
	#The next filter looks at the ratio of mapping qualities between reference and alternate alleles
	#The rationale here is that, again, because RADseq loci and alleles all should start from the same genomic location there should not be large discrepancy between the mapping qualities of two alleles.
/programs/vcflib-1.0.1/bin/vcffilter -f "MQM / MQMR > 0.9 & MQM / MQMR < 1.05" DP3g95p5maf05.fil2.vcf > DP3g95p5maf05.fil3.vcf
awk '!/#/' DP3g95p5maf05.fil3.vcf | wc -l
/programs/vcflib-1.0.1/bin/vcffilter -f "MQM / MQMR > 0.9 & MQM / MQMR < 1.05" DP3g95p5maf05_simonly.fil2.vcf > DP3g95p5maf05_simonly.fil3.vcf
awk '!/#/' DP3g95p5maf05_simonly.fil3.vcf | wc -l
	#Yet another filter that can be applied is whether or not their is a discrepancy in the properly paired status of for reads supporting reference or alternate alleles.
/programs/vcflib-1.0.1/bin/vcffilter -f "PAIRED > 0.05 & PAIREDR > 0.05 & PAIREDR / PAIRED < 1.75 & PAIREDR / PAIRED > 0.25 | PAIRED < 0.05 & PAIREDR < 0.05" -s DP3g95p5maf05.fil3.vcf > DP3g95p5maf05.fil4.vcf			
awk '!/#/' DP3g95p5maf05.fil4.vcf | wc -l
/programs/vcflib-1.0.1/bin/vcffilter -f "PAIRED > 0.05 & PAIREDR > 0.05 & PAIREDR / PAIRED < 1.75 & PAIREDR / PAIRED > 0.25 | PAIRED < 0.05 & PAIREDR < 0.05" -s DP3g95p5maf05_simonly.fil3.vcf > DP3g95p5maf05_simonly.fil4.vcf			
awk '!/#/' DP3g95p5maf05_simonly.fil4.vcf | wc -l
	#In short, with whole genome samples, it was found that high coverage can lead to inflated locus quality scores. Heng proposed that for read depths greater than the mean depth plus 2-3 times the square root of mean depth that the quality score will be twice as large as the depth in real variants and below that value for false variants.
	#I actually found that this is a little too conservative for RADseq data, likely because the reads aren’t randomly distributed across contigs. I implement two filters based on this idea. the first is removing any locus that has a quality score below 1/4 of the depth.
		#This part is complicated - do it with just all samples first
/programs/vcflib-1.0.1/bin/vcffilter -f "QUAL / DP > 0.25" DP3g95p5maf05.fil4.vcf > DP3g95p5maf05.fil5.vcf
cut -f8 DP3g95p5maf05.fil5.vcf | grep -oe "DP=[0-9]*" | sed -s 's/DP=//g' > DP3g95p5maf05.fil5.DEPTH
awk '!/#/' DP3g95p5maf05.fil5.vcf | cut -f1,2,6 > DP3g95p5maf05.fil5.vcf.loci.qual
awk '{ sum += $1; n++ } END { if (n > 0) print sum / n; }' DP3g95p5maf05.fil5.DEPTH
		49451.4 #mean depth
#Now the the mean plus 3X the square root of the mean - done myself in google/wolfram
		50118
		#Insert that value
paste DP3g95p5maf05.fil5.vcf.loci.qual DP3g95p5maf05.fil5.DEPTH | awk -v x=50118 '$4 > x' | awk '$3 < 2 * $4' > DP3g95p5maf05.fil5.lowQDloci
vcftools --vcf DP3g95p5maf05.fil5.vcf --site-depth --exclude-positions DP3g95p5maf05.fil5.lowQDloci --out DP3g95p5maf05.fil5
cut -f3 DP3g95p5maf05.fil5.ldepth > DP3g95p5maf05.fil5.site.depth
	#Now let’s calculate the average depth by dividing the above file by the number of individuals 31
		#I have 402 individuals
awk '!/D/' DP3g95p5maf05.fil5.site.depth | awk -v x=402 '{print $1/x}' > meandepthpersite
vcftools --vcf  DP3g95p5maf05.fil5.vcf --recode-INFO-all --out DP3g95p5maf05.FIL --max-meanDP 200 --exclude-positions DP3g95p5maf05.fil5.lowQDloci --recode 

	#repeat with similis only
/programs/vcflib-1.0.1/bin/vcffilter -f "QUAL / DP > 0.25" DP3g95p5maf05_simonly.fil4.vcf > DP3g95p5maf05_simonly.fil5.vcf
cut -f8 DP3g95p5maf05_simonly.fil5.vcf | grep -oe "DP=[0-9]*" | sed -s 's/DP=//g' > DP3g95p5maf05_simonly.fil5.DEPTH
awk '!/#/' DP3g95p5maf05_simonly.fil5.vcf | cut -f1,2,6 > DP3g95p5maf05_simonly.fil5.vcf.loci.qual
awk '{ sum += $1; n++ } END { if (n > 0) print sum / n; }' DP3g95p5maf05_simonly.fil5.DEPTH
		47803.6
#Now the the mean plus 3X the square root of the mean - done myself in google/wolfram
		48459
paste DP3g95p5maf05_simonly.fil5.vcf.loci.qual DP3g95p5maf05_simonly.fil5.DEPTH | awk -v x=48459 '$4 > x' | awk '$3 < 2 * $4' > DP3g95p5maf05_simonly.fil5.lowQDloci
vcftools --vcf DP3g95p5maf05_simonly.fil5.vcf --site-depth --exclude-positions DP3g95p5maf05_simonly.fil5.lowQDloci --out DP3g95p5maf05_simonly.fil5
cut -f3 DP3g95p5maf05_simonly.fil5.ldepth > DP3g95p5maf05_simonly.fil5.site.depth
	#Now let’s calculate the average depth by dividing the above file by the number of individuals 31
		#I have 125 individuals (Might have done this wrong for Doc105)
awk '!/D/' DP3g95p5maf05_simonly.fil5.site.depth | awk -v x=391 '{print $1/x}' > meandepthpersite_simonly
vcftools --vcf  DP3g95p5maf05_simonly.fil5.vcf --recode-INFO-all --out DP3g95p5maf05_simonly.FIL --max-meanDP 200 --exclude-positions DP3g95p5maf05_simonly.fil5.lowQDloci --recode 
#No HWE
#filter one SNP per contig
	grep -v "#" DP3g95p5maf05_simonly.FIL.recode.vcf | wc -l && grep -v "#" DP3g95p5maf05_simonly.FIL.recode.vcf | sort -k1,1 -u | wc -l
	grep -v "#" DP3g95p5maf05.FIL.recode.vcf | wc -l && grep -v "#" DP3g95p5maf05.FIL.recode.vcf | sort -k1,1 -u | wc -l	
#save to new file
grep "#" DP3g95p5maf05_simonly.FIL.recode.vcf > SNP_simonly.head.vcf
grep -v "#" DP3g95p5maf05_simonly.FIL.recode.vcf | sort -k1,1 -u > SNP_simonly.data.vcf
cat SNP_simonly.head.vcf SNP_simonly.data.vcf > SNP_JPfilter1129noLD_simonly.vcf
grep "#" DP3g95p5maf05.FIL.recode.vcf > SNP.head.vcf
grep -v "#" DP3g95p5maf05.FIL.recode.vcf | sort -k1,1 -u > SNP.data.vcf
cat SNP.head.vcf SNP.data.vcf > SNP_JPfilter1129noLD.vcf
#However, these still include multiallelic sites > sloppy as the first time			

		#c. From solidissima ONLY (no sneak sim)/ddocent
		#TO DO
		#d. From solidissima/transcriptome
		#TO DO
	
		#e. New Filtering Protocol including HWE (or atleast the step that goes along with it)
		#Date: Dec 15th
		#New protocol called filterb1215 (b = both subspecies)
		#copy over the DP3g95p5maf05_simonly.FIL.recode.vcf step from sol1212 and 1012
			#also popmaps
		
		#SNPS only codify
		/programs/vcflib-1.0.1/bin/vcfallelicprimitives DP3g95p5maf05.FIL.recode.vcf --keep-info --keep-geno > DP3g95p5maf05.prim.vcf &
		/programs/vcflib-1.0.1/bin/vcfallelicprimitives DP3g95p5maf05_simonly.FIL.recode.vcf --keep-info --keep-geno > DP3g95p5maf05_simonly.prim.vcf &
		#How many are left?
		grep -v "#" DP3g95p5maf05.prim.vcf | wc -l 
		grep -v "#" DP3g95p5maf05_simonly.prim.vcf | wc -l 
			#similis
			8k		13k
			#solidissima
			36k		53k
		#Remove INDELS
			#sim
			vcftools --vcf DP3g95p5maf05.prim.vcf --remove-indels --recode --recode-INFO-all --out SNP.DP3g95p5maf05
				8031 out of 8383
			vcftools --vcf DP3g95p5maf05_simonly.prim.vcf --remove-indels --recode --recode-INFO-all --out SNP.DP3g95p5maf05_simonly
				12830 out of 13454
	
			#sol
			vcftools --vcf DP3g95p5maf05.prim.vcf --remove-indels --recode --recode-INFO-all --out SNP.DP3g95p5maf05
				34016 out of 36095
			vcftools --vcf DP3g95p5maf05_simonly.prim.vcf --remove-indels --recode --recode-INFO-all --out SNP.DP3g95p5maf05_simonly
				49889 out of 53171
		#HWE
		#curl -L -O https://github.com/jpuritz/dDocent/raw/master/scripts/filter_hwe_by_pop.pl
		#chmod +x filter_hwe_by_pop.pl
		./filter_hwe_by_pop.pl -v SNP.DP3g95p5maf05.recode.vcf -p popmap -o SNP.DP3g95p5maf05.HWE -h 0.001 &
			#Sloppy, but I kept it with the original 1012 popmap which had them by site, not region
		./filter_hwe_by_pop.pl -v SNP.DP3g95p5maf05_simonly.recode.vcf -p popmap -o SNP.DP3g95p5maf05_simonly.HWE -h 0.001 &
		#sim
		7934 of a possible 8031 loci
				957 with 1 per rad tag
		12719 of a possible 12830 loci
				1532 with 1 per rad tag
		#sol
		33677 of a possible 34016 loci
				5220 with 1 per rad tag
		49438 of a possible 49889 loci
				7656 with 1 per rad tag
			#Overwrites HWE results between the two
		#For now, do normal 1 per radtag
			grep -v "#" SNP.DP3g95p5maf05_simonly.HWE.recode.vcf | wc -l && grep -v "#" SNP.DP3g95p5maf05_simonly.HWE.recode.vcf | sort -k1,1 -u | wc -l
			grep -v "#" SNP.DP3g95p5maf05.HWE.recode.vcf | wc -l && grep -v "#" SNP.DP3g95p5maf05.HWE.recode.vcf | sort -k1,1 -u | wc -l	
		#save to new file
			grep "#" SNP.DP3g95p5maf05_simonly.HWE.recode.vcf > SNP_simonly.head.vcf
			grep -v "#" SNP.DP3g95p5maf05_simonly.HWE.recode.vcf | sort -k1,1 -u > SNP_simonly.data.vcf
			cat SNP_simonly.head.vcf SNP_simonly.data.vcf > SNP_HWEfilterb1212noLD_simonly.vcf
			grep "#" SNP.DP3g95p5maf05.HWE.recode.vcf > SNP.head.vcf
			grep -v "#" SNP.DP3g95p5maf05.HWE.recode.vcf | sort -k1,1 -u > SNP.data.vcf
			cat SNP.head.vcf SNP.data.vcf > SNP_HWEfilterb1212noLD_sim.vcf		
		
		#This simonly was per sf so not the right one < for today I kep but just removed the sols, lost some SNPs
		#names
		#cp don't move
		SNP_HWE_b1215_sol_noLD.vcf
		SNP_HWE_b1215_sol.vcf
		SNP_HWE_b1215_solonly_noLD.vcf
		SNP_HWE_b1215_solonly.vcf
		SNP_HWE_b1215_sim_noLD.vcf
		SNP_HWE_b1215_sim.vcf
		SNP_HWE_b1215_simonly_noLD.vcf
		SNP_HWE_b1215_simonly.vcf
		
		#to PCA
			#Start with the no LD ones for simplicity
			#First left( ,3)
			#Copy and paste values
			#find replace | with / <- DOESN'T WORK turns to Jan-1 so just find replace 1|1 and 1/1 = 2
			#STILL NEEDS SOME FILTERING FOR LOCI WITH ONLY TWO ALLELES
		#The solidissima vcf are too big and get really clunky in excel, even the no LDs
			#Count NAs in a countif collumn and remove any locus with more than 25 after stuff	
		#Get new popmap by 
		bcftools query -l SNP_HWE_b1215_sol_noLD.vcf #and copy and paste into excel
		
		#NOTE: the SHN site was completely removed by quality filtering
		
		
		grep -v "#" SNP_HWE_b1212_sim_noLD.vcf | wc -l #confirm lengths to make sure I've got the right ones named the right thing
		#Filter to one per contig by high minor allele frequency
		#TO DO
		
	#11.Analyze Similis
		#a. Count the % of reads aligned to transcriptome
		#Date: Nov 29th
		samtools flagstat GA12_010-RG.bam > temp
			645605 + 0 mapped (100.00% : N/A)
			645599 + 0 primary mapped (100.00% : N/A)
				#Uhhh?
			#Compare to reads from trimmed fq file 
				#samtools view wc -l /4
				zcat GA12_010.R1.fq.gz| echo $((`wc -l`/4))
				1842541
				#Are their the same number of forward vs reverse reads? Should be? Resynced?
				zcat GA12_010.R2.fq.gz| echo $((`wc -l`/4))
				#yes
			#For GA_010
				#645599/1842541 > 35%
			#I can do this for 10-20 samples or write a grep protocol to do it for all
			samtools flagstat GA12_010-RG.bam | grep "primary mapped" 
						
			while read p; do
			echo "$p"
			samtools flagstat "$p" | grep "primary mapped" 
			done <listbams > bamreads &
			while read p; do
			echo "$p"
			zcat "$p" | echo $((`wc -l`/4)) 
			done <listfq > fqreads &
			#Then go to excel
		
		#b. Compare the SNP results from similis ddocent vs similis trans
			#Date: Dec 2nd
			#New folder: dDoc2transcompare_similis
			#filtered vcf from filterDoc_T1129 and filterDoc_1012
			cp ../filterDoc_1012/final_filter1012/SNP_JPfilter1012sf_noLD_simonly.vcf .
			cp ../filterDoc_1012/final_filter1012/SNP_JPfilter1012noLD.vcf .
			cp ../filterDoc_T1129/SNP_JPfilter1129noLD_simonly.vcf .
			cp ../filterDoc_T1129/SNP_JPfilter1129noLD.vcf .
			#Rename for my own sanity
			mv SNP_JPfilter1012sf_noLD_simonly.vcf SNP1012_dD_simonly.vcf
			mv SNP_JPfilter1012noLD.vcf SNP1012_dD.vcf
			mv SNP_JPfilter1129noLD_simonly.vcf SNP1129_T_simonly.vcf
			mv SNP_JPfilter1129noLD.vcf SNP1129_T.vcf
			#Rename MW_017? < might not be needed because I'll just be graphing them in parallel
			
			
			#i. Are the SNPs the same? <- align the catalog to the transcriptome
			#TO DO
			
			#ii. PCA
			#Date: Dec 2nd
			#My Process using ggplot
				#Download vcfs and put into excel to get to into matrix form
				#Open in Excel (Just doing sim only for now)
					#Remove header (and # on first row so that R can read it)
					#Copy over the first few columns and header to new sheet
					#Left(, 3)
					#Copy and Paste Values
					#Find replace ./. with NA
						#1/1 = 2, 0/0 = 0, 0/1 = 1, /2 = NA
						# NA = ~3500 alleles in each list, 0 = ~89k T ~158k dD, 18k T 26k dD
						#Replace ./0 NA
						#Delete rows with multiple alleles if needed
							#Search for lines within the /
							#Just replace remaining */* with NA <- nice
					#Import into R as .txt (readtable) or .csv and they 
			
			#iii. STRUCTURE
			#Date: Dec 3rd
			#Download vcfs and using computer PGDspider to convert to .structure
				#Set popmap
				#To compare structure plots between the two, I should have the same 121 samples in each?
					#Edit the vcf columns in excel
					#no LD vcfs
			#Server's all full up. Can I run on PC using STRUCUTRE program?
				#Need number of samples and number of loci
				#for T SNPs simonly: 121 samples and 966 loci
					#There is a name for loci
					#Extra columns for Indiviudal ID and putative pop
					#SNPs are numbered so just use that - had to remove sneaky sols
				#Make my parameter set 
					#Length of Burnin: 10,000
						#Typically a burnin of 10,000—100,000 is more than adequate. (from manual: https://www.ccg.unam.mx/~vinuesa/tlem09/docs/structure_doc.pdf)
					#Number of MCMC reps
						#This website used 50,000k (https://pbgworks.org/sites/pbgworks.org/files/Tutorial%20of%20STRUCTURE%20software.pdf)
					#Use Admixture Model
					#Allele Frequencies Correlated (????)
					#Compute probability (for estimating K)
				#Start Run
					#Choose "Project" >"Start a job" to run multiple Ks
					#Set K from 2 to 10 (up to 14 later but just want it to work)
					#Number of iterations is 1 (but could do more later)
				#Took about 5 minutes per K value
				
				#Set up other projects
					#SNP1012
						#1474 SNPS
						#121? (Might be 125)
					#HapsT
						#768 SNPS
						#121
					#HapsD
						#1243 SNPS
						#121/125
			#You can run multiple at once by openning different iterations of the program and making a new project in each
			
			#For Transcriptome H test the K likelihood of K = 1
			
			#Clean this up?
			#TO DO?
				#Should I redo this, having removed high kinship individuals 
				#I should remove SNPs will all missing data but I don't know how cause I don't see them in the vcfs
				 		
			
			#iv. IBD
			#TO DO
			
		#c. Compare the haplotype data from similis ddoc and trans
			#i. Calculate haplotype data from transcriptome SNPS
			#Date: Dec 3rd
			#Run haplotyper in the original folder with the bams but bring in the filtered bam
			cp filterDoc_T1129/DP3g95p5maf05_simonly.FIL.recode.vcf GBS2transcriptome/dDoc_aln2transcriptome_1127/haptest_JPfilter_sim_T.vcf
			cp /workdir/hh693/dDoc_928/undedup/rad_haplotyper.pl GBS2transcriptome/dDoc_aln2transcriptome_1127
			cd GBS2transcriptome/dDoc_aln2transcriptome_1127
			
			export PERL5LIB=/programs/PERL/lib/perl5
			perl rad_haplotyper.pl #test if it's ready to work
			
			#put into  slurm script
			perl rad_haplotyper.pl -v haptest_JPfilter_sim_T.vcf -x 8 -n -o haptest_sim_T_haps.vcf -r reference.fasta
			
			sbatch --nodes=1 --ntasks=8 --mem=35000 s_haps_simT.sh
				#I. Check in on the hap vcf
				grep -v "##" haptest_sim_T_haps.vcf | wc -l
				3864
				#II. Save to new file
				grep "#" haptest_sim_T_haps.vcf > haps_sim.head.vcf
				grep -v "#" haptest_sim_T_haps.vcf | sort -k1,1 -u > haps_simT.data.vcf
				cat haps_sim.head.vcf haps_simT.data.vcf > haps_simT_noLD.vcf
				grep -v "##" haps_simT_noLD.vcf | wc -l
				808
				
				#III. Save both with and without noLD and get both for the dDoc too
				#under haplotypecalling/richnesscalc
			
			#ii. Measurements of allelic/haplotype richness
				#I. How do you measure it?
				#Date: Dec 3rd
					#A. Difference between diversity and richness?
						#I think richness is just the number different things at a locus per population
						#"Mean number of alleles"
					#B. Difference between allelic/haplotype?
						#Specificity?
					#So do I just count number of variants per population from vcf? What program does that?
					#If you have variable numbers of samples per population it needs to be corrected with a rarefaction approach
					#C. What is a rarefaction approach?
						#R package hierfstat
						#https://cran.r-project.org/web/packages/hierfstat/vignettes/hierfstat.html
						#https://cran.rstudio.org/web/packages/hierfstat/hierfstat.pdf
					
				#II. Filter vcfs so that I have the same 121 samples in both
				#TO DO
				
				#III. Calculate
				#TO DO
				#Can't figure out how to import correctly into R
		
	#12.Check for anomalies in solidissima data
		#After the very interesting looking PCA, then set up to check two things before reporting
		#a. LD test, vcftools will do R^2 correlation
			#Remove all with high R^2
			vcftools --vcf MyVariants.vcf --hap-r2 --ld-window-bp 10000 --out MyVariants.LD.10Kbp
			#andor bcftools cause vcftools is old now
				#By my window is 150 bp?
			#name prunedLD
			
			#start with 
			#33k SNPs for all		49k SNPs for sol only
			
			#code
			--hap-r2 #outputs file with the LD values ASSUMES PHASED HAPLOTYPES
			vcftools --vcf frag.vcf --hap-r2 --min-r2 .7 --ld-window-bp 50000 --out minr2_ld_window_50000
					#Min r2 does not filter the vcf, just reports only r2s greater than that because you are comparing all snps so it could be a lot

			vcftools --vcf SNP_HWE_b1215_sol.vcf --hap-r2 --ld-window-bp 600 --out SNP_solall.LD
			vcftools --vcf SNP_HWE_b1215_solonly.vcf --hap-r2 --ld-window-bp 600 --out SNP_solonly.LD &
			
			#Check the max and min location on the radtag for max ld window? Is it <300?
			grep -v "#" SNP_HWE_b1215_solonly.vcf | awk 'BEGIN{a=0}{if ($2>0+a) a=$2} END{print a}'
			545
			grep -v "#" SNP_HWE_b1215_solonly.vcf | awk 'BEGIN{a=10000}{if ($2<0+a) a=$2} END{print a}'
			2
			#set 600
			
			#For sol all
			After filtering, kept 8608 out of a possible 33677 Sites
			After filtering, kept 12808 out of a possible 49438 Sites
			#Meaning it could only calculate it for 8608 of them because they had phasing?
			grep -v "##" SNP_HWE_b1215_solonly.vcf | grep -e "|" | wc -l
			15951
			#Not quite I guess
			#Download, filter to min-r2 is 
			
			#Options?
			#i. Work with the SNPs that are in the calculations and also have a min-r2 of 0.7
				#Is high R2 yes LD or no LD?
				https://pbgworks.org/sites/pbgworks.org/files/measuresoflinkagedisequilibrium-111119214123-phpapp01_0.pdf
				#Complete LD (not separated by recombination) means D=1 and r^2 = 1
				#So remove high r2 values
					#r2 =0 means in complete equilibrium
				https://www.ndsu.edu/pubweb/~mcclean/plsc731/Linkage%20Disequilibrium%20-%20Association%20Mapping%20in%20Plants-lecture-overheads.pdf
				
				https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2735096/
				#First, before running PCA we used the PLINK28 software to exclude SNPs with pairwise genotypic r2 greater than 80% within sliding windows of 50 SNPs (with a 5-SNP increment between windows). Second, we took an iterative approach by running an initial PCA and removing chromosomal regions that showed evidence of reflecting regions of exceptional long-range linkage disequilibrium rather than genome-wide patterns of structure.
				
				#Other method, does not require phased
				--geno-r2
				#Calculates the squared correlation coefficient between genotypes encoded as 0, 1 and 2 to represent the number of non-reference alleles in each individual. This is the same as the LD measure reported by PLINK. The D and D’ statistics are only available for phased genotypes. The output file has the suffix ".geno.ld".
				#Since I already filtered for minor allele frquency, I do not need D and D'
			vcftools --vcf SNP_HWE_b1215_sol.vcf --geno-r2 --ld-window-bp 600 --out SNP_solall.LD &
			vcftools --vcf SNP_HWE_b1215_solonly.vcf --geno-r2 --ld-window-bp 600 --out SNP_solonly.LD &
				#Has some nans but is generally butter at only eliminating 20% by this mean as opposed to all
				
				#A. Plot
					#Runs for a long time. Start on a subet?
					#Why does solall have more markers?
						#Because solonly was still processing cause it takes longer cause more > I went with the intermediate because time pressure
				#B. Filter R2 < 0.5
				#C. Given what the output file looks like, I could just remove the second locus from the high LD pairs
					#Since I have to do it as two sets for pos 1 and pos 2 anyway, include p1 and p2 in keep and only p2s in remove
				#D. Filter to the list of keeps only then filter to remove anything from the remove list (i.e. loci that were in one good filter but were elsewhere in one bad one) <- could do in reverse order
					#vcftools keep from list
					#vcftools remove from list
					--positions <filename>
					--exclude-positions <filename>
				vcftools --vcf SNP_HWE_b1215_sol.vcf --positions Solall_keep_geno.txt --recode --recode-INFO-all --out SNP_HWE_b1215_sol_k1 &
				#After filtering, kept 17172 out of a possible 33677 Sites
				vcftools --vcf SNP_HWE_b1215_sol_k1.recode.vcf --exclude-positions Solall_remove_geno.txt --recode --recode-INFO-all --out SNP_HWE_b1215_sol_k2 &
				#After filtering, kept 12990 out of a possible 17172 Sites
				
				#Redo with more LD estimations
				vcftools --vcf SNP_HWE_b1215_solonly.vcf --positions Solonly_keep_geno2.txt --recode --recode-INFO-all --out SNP_HWE_b1215_solonly_k3 &
				#After filtering, kept 17336 out of a possible 49438 Sites
				vcftools --vcf SNP_HWE_b1215_solonly_k3.recode.vcf --exclude-positions Solonly_remove_geno2.txt --recode --recode-INFO-all --out SNP_HWE_b1215_solonly_k4 &
				#After filtering, kept 11218 out of a possible 17336 Sites
				
			#ii. Haplotype first? What about my haplotype data, is that all phased? <- Does not work ont he similis haplotype data because none of it is phased.
			
			#iii. Impute2? On the server makes data into phased
			export PATH=/programs/impute2:$PATH
			
			#iv. Look into other papers
				#A. Plough et al https://onlinelibrary.wiley.com/doi/pdf/10.1111/eva.13322
				#Thinning of the neutral dataset by chromosome was performed in VCFTOOLS using the thin function (Danecek et al., 2011).
				#Inclusion of loci that are strongly linked (high linkage disequilibrium) can lead to biases in downstream analyses if independence of loci is assumed (Willis et al., 2017)
				#The appropriate thinning distance was determined by calculating R2 separately for SNPs on the same chromosome (intrachromosomal pairs) and for unlinked SNPs (interchromosomal pairs). The critical R2 was estimated from the unlinked loci by root transforming the R2 values and taking the 95th percentile of the distribution as the threshold beyond which the LD is caused by physical linkage (Breseghello & Sorrells, 2006). The relationship between LD decay and genetic distance was summarized by fitting a second-degree smoothed locally weighted linear regression (LOESS) curve (Cleveland, 1979) to intrachromosomal R2 data in R. The distance the loess curve intercepted the critical R2 was identified as the threshold for LD decay (Figure S1).
					#Uh no
				#Can I thin while setting an arbitary R2 cut off
					#No, thin is just to filter loci that aren't that close to eachother and then they did that by decay distance?
				#B. https://onlinelibrary.wiley.com/doi/pdf/10.1111/1755-0998.12270
					#Pairwise linkage disequilibrium in the form of r 2 was calculated for collared flycatcher using Haploview version 4.2 (Barrett et al. 2005) for pairs of SNPs that were uniquely mapped onto the genome and had MAF >10% and genotyping rate >90%. 
				
		#b. One per radtag
			#Radtag by FST (works if I do one SNP per radtag)
			#Redefine populations as left and right. Barnstable is left? Just the few on the right are separate?
			SNP_HWE_b1215_sol_k2.recode.vcf
			SNP_HWE_b1215_solonly_k4.recode.vcf
			
			#i. Output minor allele frequency and filter to highest maf per radtag
			vcftools --vcf SNP_HWE_b1215_sol_k2.recode.vcf --freq --out freqlist_solall &
			vcftools --vcf SNP_HWE_b1215_solonly_k4.recode.vcf --freq --out freqlist_solonly &
				#Open in excel
				#Data by tabs and :
				#Sort by max minor allele frequency then sort by unique contig
					#By removeing duplicates, expand selection, only select column A
				#About 2.6k SNPs for each
				#1fprad = 1 (by frequency) per radtag
			vcftools --vcf SNP_HWE_b1215_sol_k2.recode.vcf --positions solall_freq_keep.txt --recode --recode-INFO-all --out SNP_HWE_b1215_solall_1fprad 
			vcftools --vcf SNP_HWE_b1215_solonly_k4.recode.vcf --positions solonly_freq_keep.txt --recode --recode-INFO-all --out SNP_HWE_b1215_solonly_1fprad 
			
			#ii. PCA first, then try fixed loci of high effect
			
			#Includes more phased than normal but not entirely:
			grep -v "#" SNP_HWE_b1215_solonly_1fprad.recode.vcf | grep -v -e "|" | wc -l
			1980


		#c. Fixed loci of high effect
			#Radtag by FST (works if I do one SNP per radtag)
			#Start in bash with 
			SNP_HWE_b1215_solonly_1fprad.recode.vcf
			SNP_HWE_b1215_solall_1fprad.recode.vcf
			#i. Make a pop file from the PCA data
				#Where did my PCA data go?
				#Saved to Sol_b1215PCA_coordinates.xlsx under advanced filtering
				#The same pop file works for both solall and solonly because I don't care about FST comparisons with similis and the same ones are RH/LH regardless of whether similis is there
				
			#ii. Vcftools FST per pop with each pop as its own file
			vcftools --vcf SNP_HWE_b1215_solonly_1fprad.recode.vcf --weir-fst-pop LH_pops_MidCCBsLH_1218.txt --weir-fst-pop RH_pops_1218.txt --out LHRH_midLH_solonly.fst 
				Weir and Cockerham mean Fst estimate: 0.13922
				Weir and Cockerham weighted Fst estimate: 0.27422
			
			vcftools --vcf SNP_HWE_b1215_solall_1fprad.recode.vcf --weir-fst-pop LH_pops_MidCCBsLH_1218.txt --weir-fst-pop RH_pops_1218.txt --out LHRH_midLH_solall.fst 
				Weir and Cockerham mean Fst estimate: 0.15557
				Weir and Cockerham weighted Fst estimate: 0.28583
			
			vcftools --vcf SNP_HWE_b1215_solonly_1fprad.recode.vcf --weir-fst-pop LH_pops_MidCCBsRemoved_1218.txt --weir-fst-pop RH_pops_1218.txt --out LHRH_midRemove_solonly.fst
			vcftools --vcf SNP_HWE_b1215_solall_1fprad.recode.vcf --weir-fst-pop LH_pops_MidCCBsRemoved_1218.txt --weir-fst-pop RH_pops_1218.txt --out LHRH_midRemove_solall.fst 

			#iii. Plot density first then decide cut off
				#High FST means super differenciated
				#What about negative FST?
					#Fst values using the calculations provided by Wier and Cockeham (1984) and most (all) programs that calculate Fst use these. It generally means there is more variation within than between populations (which is why most people report it as Fst=0)
			
			#iv. Remove loci based on FST Values from my computer rather than feeding back into vcftools
				#a. grep/awk? a list of row headers to pull
				#b. excel add in an extra column, filter and remove column
					#Does not work, not all sites in excel
					#Instead search and see if it in the thing
					#Using https://www.extendoffice.com/documents/excel/3466-excel-check-if-value-is-in-list.html
					#Then filter by REMOVE and delete those rows from excel
					#Then remove column
				
				#c. start with solonly
					#save as FilteringbyFST
				#FST085 - 2169 SNPs
				#FST05 - 1987 SNPs
				#FST01 - 984 SNPs
					
			
			#In r?
			#https://speciationgenomics.github.io/per_site_Fst/
			
			
			
	#TODO: Depth graphs of solidissima for filtering

#Date: Jan 3rd

#b1215
	#Freq by radtag
	filterDoc_b1215/advancedfil_1218/
		SNP_HWE_b1215_solall_1fprad.recode.vcf
		SNP_HWE_b1215_solonly_1fprad.recode.vcf
	#Maybe also do LH/RH (use midLH)
	#Don't want one per
	vcftools --vcf SNP_HWE_b1215_sol_k2.recode.vcf
	vcftools --vcf SNP_HWE_b1215_solonly_k4.recode.vcf
	

#similis 1fprad and HWE?
	filterDoc_b1215/sim/
		#SNP_HWEfilterb1215noLD_simonly.vcf
		#SNP_HWEfilterb1215noLD_sim.vcf
	#Set up, just not by frequency
	#Which alignment are they based on? 
	#same as 1012
		#Do it first in dDoc_928/undedup > AKA filterDoc_1012
		#928
	cp ../../filterDoc_b1215/sim/SNP_HWE_b1215_sim*.vcf .
	#NOT noLD
	SNP_HWE_b1215_simonly.vcf
	SNP_HWE_b1215_sim.vcf
	
perl rad_haplotyper.pl -v SNP_HWE_b1215_simonly.vcf -x 8 -n -o simonly_b1215_haps.vcf -r reference.fasta
perl rad_haplotyper.pl -v SNP_HWE_b1215_sim.vcf -x 8 -n -o simall_b1215_haps.vcf -r reference.fasta

sbatch --nodes=1 --ntasks=8 --mem=35000 haps_sim_1322.sh

	
	

#Haplotype Data
	export PERL5LIB=/programs/PERL/lib/perl5
	perl rad_haplotyper.pl #test if it's ready to work
			
	#put into  slurm script
	perl rad_haplotyper.pl -v SNP_HWE_b1215_sol_k2.recode.vcf -x 8 -n -o solall_b1215_haps.vcf -r reference.fasta
	perl rad_haplotyper.pl -v SNP_HWE_b1215_solonly_k4.recode.vcf -x 8 -n -o solonly_b1215_haps.vcf -r reference.fasta

sbatch --nodes=1 --ntasks=8 --mem=45000 haps1322.sh

#Richness
simall_b1215_haps.vcf
simonly_b1215_haps.vcf 	#9527 haplotypes from 12719 SNPs
	#Date: Jan 8th
	dDoc_928/undedup/simall_b1215_haps.vcf
	dDoc_928/undedup/simonly_b1215_haps.vcf #This includes BLP
	#Fix
		#Run with sf
		filterdoc_1012/filterdoc_1012sf/DP3g95p5maf05_simonly.FIL.recode.vcf
		#This is the good one
		#rename to 
		simONLY_sf_1012.vcf
		#Then have to do the b1215 stuff
		/programs/vcflib-1.0.1/bin/vcfallelicprimitives simONLY_sf_1012.vcf --keep-info --keep-geno > simONLY_sf_1012.prim.vcf &
		vcftools --vcf simONLY_sf_1012.prim.vcf --remove-indels --recode --recode-INFO-all --out SNP.simONLY_sf_1012
			After filtering, kept 12862 out of a possible 13486 Sites
		./filter_hwe_by_pop.pl -v SNP.simONLY_sf_1012.recode.vcf -p popmap -o SNP.simONLY_sf_1012.HWE -h 0.001 &
			Kept 12751 of a possible 12862 loci (filtered 111 loci)
		
	export PERL5LIB=/programs/PERL/lib/perl5
	perl rad_haplotyper.pl -v SNP.simONLY_sf_1012.HWE.recode.vcf -x 10 -n -o simONLY_1012sf_b1215_haps.vcf -r reference.fasta &> log1012sfhaps.log &


	
	#Date: Jan 8th
	bcftools convert --hapsample --vcf-ids solonly_b1215_haps.vcf -o solonly_hapsconvert1
	#Outputs information like haplotypes
	dDoc_sol_121/align121/solonly_b1215_haps.vcf
	
	#Outputs in different file format
	#Could do
	--haplegendsample
	#and then
	--haplegendsample2vcf
	bcftools convert --haplegendsample2vcf solonly_hapsconvert1 -o solonly_hapsconvert2
	#Does not work
	
	#Maybe Plink can do it but let's try me doing it first instead
	
	
	#download
	/workdir/hh693/dDoc_sol_121/align121/solonly_b1215_haps.vcf
	/workdir/hh693/dDoc_928/undedup/simONLY_1012sf_b1215_haps.vcf
		
	#Look for their log files
	haps1322.out
	ind_stats.out
	
	#or 
	stats.out
	#This works! Just run for each population separately?
		#Except that might change which things are haplotyped?
	
	#Okay so there's more to his haplotype caller so lets resume there with
	/workdir/hh693/dDoc_sol_121/align121/solonly_b1215_haps.vcf
	/workdir/hh693/dDoc_928/undedup/simONLY_1012sf_b1215_haps.vcf
	#in the haplotype caller with their respective stats.out files
	
	#Make list to remove
	grep FILTERED stats.out | awk '!/Complex/' | cut -f1 > loci.to.remove
	curl -L -O https://github.com/jpuritz/dDocent/raw/master/scripts/remove.bad.hap.loci.sh
	chmod +x remove.bad.hap.loci.sh
	
	./remove.bad.hap.loci.sh loci.to.remove simONLY_1012sf_b1215_haps.vcf
	
	#mv names
	mv solonly_b1215_haps.vcf.filtered.vcf solonly_b1215_hapsfiltered.vcf
	
	awk '!/#/' solonly_b1215_hapsfiltered.vcf | wc -l
	10129
	awk '!/#/' simONLY_1012sf_b1215_hapsfiltered.vcf | wc -l
	9931
		
		
	awk '!/#/' simONLY_1012sf_b1215_haps.vcf | wc -l	
	9931
	#but
	wc -l loci.to.remove
	271 loci.to.remove
	
	#Did it actually filter anything?
	
	
	awk '!/#/' solonly_b1215_haps.vcf | wc -l
	
	
	
	#Still confused but what I do know is that I can repeat the analysis with -s a list of samples which only includes each population so that I can compare the stats.out files
	#How can I get it to save as a different stats file name?
		#Edit the script?

#Similis
#Date: Jan 8th

	#Go to
	/workdir/hh693/dDoc_928/undedup/
	export PERL5LIB=/programs/PERL/lib/perl5
	
	perl rad_haplotyper.pl -v SNP.simONLY_sf_1012.HWE.recode.vcf -x 8 -n -o simONLY_1012sf_b1215_haps_GA.vcf \
	-s GA12_001 -s GA12_002 -s GA12_003 -s GA12_004 -s GA12_006 \
	-s GA12_007 -s GA12_008 -s GA12_009 -s GA12_010 -s GA12_012 \
	-s GA12_013 -s GA12_014 -s GA12_015 -s GA12_016 -s GA12_017 \
	-s GA12_018 -s GA12_019 -s GA12_020 -s GA12_021 -s GA12_022 \
	-s GA12_023 -s GA12_024 -s GA12_025 \
	-r reference.fasta &> log1012sfhaps_GA.log &
mv stats.out statshap_GA.out
		
		#Got a no file found line 628 in radhaplotyper error
			#I accidentally included a sample ID twice so maybe fixing that will be better.
			#Can't find BAM file for individual: GA12_025-r at rad_haplotyper.pl line 1345
			#Not sure why, looks like it exists
			#Might be because I had it at -s GA12_025\ without the space so it was interpretted as MW_025linebreak
			

perl rad_haplotyper.pl -v SNP.simONLY_sf_1012.HWE.recode.vcf -x 10 -n -o simONLY_1012sf_b1215_haps_NLI.vcf \
	-s GLD0819_001 -s GLD0819_002 -s GLD0819_003 -s GLD0819_004 -s GLD0819_005 \
	-s GLD0819_006 -s GLD0819_007 -s GLD0819_008 -s PEC0819_011 -s PEC0819_012 \
	-s PEC0819_013 -s RP20_009 -s RP20_010 \
	-r reference.fasta &> log1012sfhaps_NLI.log &
mv stats.out statshap_NLI.out

#Check progress
tail log1012sfhaps_NLI.log | less


perl rad_haplotyper.pl -v SNP.simONLY_sf_1012.HWE.recode.vcf -x 10 -n -o simONLY_1012sf_b1215_haps_SCC.vcf \
	#INSERT ALL AS SEPARATE LINES FROM EXCEL
-r reference.fasta &> log1012sfhaps_SCC.log &
mv stats.out statshap_SCC.out



#Solidissima
	#Go to
	/workdir/hh693/dDoc_sol_121/align121/
	SNP_HWE_b1215_solonly_k4.recode.vcf -x 8 -n -o solonly_b1215_haps.vcf -r reference.fasta
	#May have to rerun because the saved stats file might be the one for solall
		#Check total sample numbers: 391
		#Also at the top of the file it says rad_haplotyper -v SNP_HWE_b1215_solonly_k4.recode.vcf -x 8 -n -o solonly_b1215_haps.vcf -r reference.fasta 
	#AWESOME!
	
	mv stats.out statshap_ALLsol.out
	
	#Unlesss... Does it stack them/append?
	#No
	

#Decide on Solidissima population designations
	#Coast vs Shelf
	
	#GBE vs CCB vs SC vs SLI vs NJ
perl rad_haplotyper.pl -v SNP_HWE_b1215_solonly_k4.recode.vcf -x 10 -n -o solonly_b1215_haps_GB.vcf \
		#INSERT ALL AS SEPARATE LINES FROM EXCEL
-r reference.fasta &> logsolhaps_GB.log &
mv stats.out statshap_GB.out

	#1999 vs 2012 vs 2019?
	
#Put into a script
#!/bin/bash -l

#SBATCH --partition=regular
#SBATCH --job-name=hapbysite_10t
#SBATCH --output=s_hapbysite.out
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --mail-user=hh693@cornell.edu

echo "log SHELF START"
perl rad_haplotyper.pl -v SNP_HWE_b1215_solonly_k4.recode.vcf -x 10 -n -o solonly_b1215_haps_SHELF.vcf \

-r reference.fasta
echo "log SHELF END"
mv stats.out statshap_SHELF.out

#Confirm that CCB has less than 66 samples in it so it isn't adding extra stuff that isn't in the vcf

sbatch --nodes=1 --ntasks=10 --mem=25000 s_hapsbysite.sh

#FST for solidissima
vcftools --vcf SNP_HWE_b1215_solonly_k4.recode.vcf --max-alleles 2 --recode --out SNPONLY_solonly.vcf
#THIS HAD NO AFFECT, KEPT ALL SITES?

bcftools query -l input.vcf

vcftools --vcf SNP_HWE_b1215_solonly_k4.recode.vcf \
--weir-fst-pop x413 \
--weir-fst-pop GB \
--out temp

SLI		SSLI
CCB		BAR, PLY (PVT), BRW
SCC		MW, CTM
WV
NAT		
GB	    GBE + x536
x413
FNJ

#Other metrics of allelic richness?
#Diversity




#Date: Jan 18th
#Align some sim/sol to opposite reference
	#The machine is my oyster
	dDoc_0118
	#Use similis reference because these are similis questions
		#copy all *.fa* files from dDoc_928 and some samples from dDoc_sol121 from both coast and shelf
	cp dDoc_928/undedup/*.cov.stats dDoc_0118 &
		#*.bam*
		#*.fq*
		#*reference* and popmap (eventhough I will change the popmap)
	
	#Create a README_dDoc_0118.txt
	
	
#In order to just align the solidissima samples to this reference, because the similis are already aligned, temporarily put them into a different folder so	that they do not waste time
	#Which solidissima samples do I choose?
	#Only ones that survived the other filtering, so I can choose ones from the PCA
	GBE_028	GBE_171	GBE_172	GBE_182	GBE_183	GBE_184	GBE_191	GBE_192	GBE_193	GBE_194	GBE_195	GBE_196	GBE_201	GBE_203	GBE_261	GBE_262	GBE_263	GBE_291	GBE_332	GBE_333	GBE_334	GBE_335	GBE_336	GBE_371	GBE_373	GBE_381	GBE_382	GBE_383	GBE_384	GBE_385	GBE_401	GBE_402	GBE_441	GBE_442	GBE_443	GBE_471	GBE_472	GBE_473	GBE_481	GBE_482	GBE_483	GBE_484	GBE_521	GBE_522	GBE_523	
	MCX_015	MCX_016	MCX_017	MCX_018	MCX_019	MCX_020	MCX_021	MCX_027	MCX_029	
	NAN_001	NAN_002	NAN_003	NAN_007	NAN_008	NAN_009	
	PLY_001	PLY_002	PLY_003	PLY_004	PLY_005	PLY_006	PLY_007	PLY_008	PLY_009	PLY_010	PT_001	PT_003	PT_006	PT_007	PT_008	PT_009	PT_010	PT_011	PT_012	PT_013	PT_014	PT_016	PT_017	PT_018	PT_019	PT_020	PVT_001	PVT_002	PVT_003	PVT_004	PVT_005	PVT_006	PVT_007	PVT_009	PVT_010	
	SHN_102	SHN_103	SHN_104	SHN_151	SHN_152	SsLI1101_1	SsLI1102_1	SsLI1102_2	SsLI1102_3	SsLI1102_4	SsLI1107_1	SsLI1107_2	SsLI1107_3	SsLI1112_1	SsLI1112_2	SsLI1113_4	SsLI1113_5	SsLI1116_2	SsLI1117_2	SsLI1117_4	SsLI2101_7	SsLI2107_1	SsLI2107_2	SsLI2108_1	SsLI2108_2	SsLI2108_3	SsLI2109_2	SsLI2109_3	SsLI2109_4	SsLI3105_2	SsLI3105_3	SsLI3105_5	SsLI3109_1	SsLI3109_2	SsLI3109_3	SsLI3109_4	SsLI3109_5	SsLI4106_1	SsLI4106_2	SsLI4106_3	SsLI4106_4	SsLI4106_5	SsLI4109_1	SsLI4109_3	SsLI4121_2	SsLI4121_3	SsLI4121_4	SsLI4121_5	SsLI4129_1	SsLI4129_2	SsLI4129_3	SsLI4139_1	SsLI4146_1	SsLI4146_2	WV1_001	WV1_002	WV1_003	WV1_004	WV1_005	WV1_006	WV1_007	WV1_008	WV1_009	WV1_010	WV5_001	WV5_002	WV5_003	WV5_004	WV5_005	WV5_006	WV5_007	WV5_008	WV5_009	WV5_010
	
	#Shelf
	PT, FNJ, NAN, GBE, x413, CGC, SsLI (some), CTM008/MW, BAR (even though the middle ones would be cool, they could make it complicated - align but maybe filter later)
		cp dDoc_sol_121/align121/BAR*fq* dDoc_0118 &
		cp dDoc_sol_121/align121/413*5*fq* dDoc_0118 &
		cp dDoc_sol_121/align121/BRW*fq* dDoc_0118 &
		cp dDoc_sol_121/align121/CGC_00*fq* dDoc_0118 &
		cp dDoc_sol_121/align121/CTM*fq* dDoc_0118 &
		cp dDoc_sol_121/align121/FNJ_00*fq* dDoc_0118 &
		cp dDoc_sol_121/align121/GBE*3.*fq* dDoc_0118 & #check
		cp dDoc_sol_121/align121/NAN_00*.*fq* dDoc_0118 & #check
		cp dDoc_sol_121/align121/P*_01*.*fq* dDoc_0118 & #check
	cp dDoc_sol_121/align121/SsLI2101_7*fq* dDoc_0118 &

		
	#Coast
	MCX, WV, SHN, BLP, SsLI (some)
		cp dDoc_sol_121/align121/BLP_190*fq* dDoc_0118 &
		cp dDoc_sol_121/align121/BLP_21*fq* dDoc_0118 &
		cp dDoc_sol_121/align121/MCX_02*.*fq* dDoc_0118 & #check
		cp dDoc_sol_121/align121/SHN*.*fq* dDoc_0118 & #check
		cp dDoc_sol_121/align121/WV*_007.*fq* dDoc_0118 & #also 6 and 8
	cp dDoc_sol_121/align121/SsLI4129_*fq* dDoc_0118 &

	#This brings me to 107 samples from Coast and Shelf, reduce
		#Down to 65 (including some BAR that won't make it much further)
	
	#Great, start dDocent run

#!/bin/bash -l

#SBATCH --partition=regular
#SBATCH --job-name=dDoc0118_th32_mem160
#SBATCH --output=dDoc0118_freebayes_solandsim2sim.out
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --mail-user=hh693@cornell.edu
source /programs/miniconda3/bin/activate dDocent-2.8.13
dDocent config_alignandfree2sim.txt

sbatch --nodes=1 --ntasks=40 --mem=180000 s_freebayes_solandsim2sim.sh

Number of Processors
32
Maximum Memory
0
Trimming
no
Assembly?
no
Type_of_Assembly
PE
Clustering_Similarity%
0.85
Minimum within individual coverage level to include a read for assembly (K1)
2
Minimum number of individuals a read must be present in to include for assembly (K2)
2
Mapping_Reads?
yes
Mapping_Match_Value
1
Mapping_MisMatch_Value
4
Mapping_GapOpen_Penalty
6
Calling_SNPs?
yes
Email
hh693@cornell.edu
	

#Then after I have alignments for all 130 or so similis and probably ~20 solidissima from shelf and 20 from coast
        #Bring all together in the same folder
        	#I have 193 total samples (125 simlis + 68 solidissima)
        	#My brilliant plan did not work because it creates the cat_RRG file etc. when they are all aligned together
        #Pull out their mapping stats by contig
                #To save time, only call vcfs on	the contigs with good consistent mappings               
                #(but I do not think that will be easy to adjust using dDocent to run Freebayes so not this time)
        #Remove similis that I already know are bad eggs
        #Adjust the popmap file if needed (or start working on it while bwa is running)
        #Call vcfs from the ~110 good similis, ~20 shelf, and ~20 coast
                #Weirdly, I think after a dDocent run, it deletes all files in the folder named config so create a new one?
                #Filter to only contigs with consistent % mapped 
                #Perform normal vcf filtering with a focus on
                        #<10% missing data
                        #exactly 60 mapping score
                        #LD filtering
                        #Look for high minor allele frequency in both subspecies/all three

#Test how to get mapping quality data
samtools view -bq 2 file.bam > filtered.bam
#-q INT Skip alignments with MAPQ smaller than INT [0]
samtools view -bq 60 ELP_003-RG.bam > ELP_test.bam
	#If you are variant calling they will be filtered. <- Good to know anyway. But this also is not % mapped
	#There is come variation in the middle, things 0> <60, but I can look for just 60s?
	#from wc -l for all map quality and then to only 60+
	1,239,430 > 985,569 #so that's most of them! Cool!
#For % mapped, I can use samtools flagstat	
	1239430 + 0 in total (QC-passed reads + QC-failed reads)
	1239427 + 0 primary
	3 + 0 secondary
	0 + 0 supplementary
	0 + 0 duplicates
	0 + 0 primary duplicates
	1239430 + 0 mapped (100.00% : N/A)
	1239427 + 0 primary mapped (100.00% : N/A)
	1239427 + 0 paired in sequencing
	610129 + 0 read1
	629298 + 0 read2
	1117995 + 0 properly paired (90.20% : N/A)
	1187571 + 0 with itself and mate mapped
	51856 + 0 singletons (4.18% : N/A)
	42121 + 0 with mate mapped to a different chr
	38050 + 0 with mate mapped to a different chr (mapQ>=5)
#Example output for ELP_003-RG.bam. So then can I do this by chromosome/by contig? That's a lot of output files?
#Below, example for filtered to only mapping Q = 60
	985569 + 0 in total (QC-passed reads + QC-failed reads)
	985569 + 0 primary
	0 + 0 secondary
	0 + 0 supplementary
	0 + 0 duplicates
	0 + 0 primary duplicates
	985569 + 0 mapped (100.00% : N/A)
	985569 + 0 primary mapped (100.00% : N/A)
	985569 + 0 paired in sequencing
	501281 + 0 read1
	484288 + 0 read2
	931845 + 0 properly paired (94.55% : N/A)
	967503 + 0 with itself and mate mapped
	18066 + 0 singletons (1.83% : N/A)
	14457 + 0 with mate mapped to a different chr
	14457 + 0 with mate mapped to a different chr (mapQ>=5)
#Someone said the following https://www.biostars.org/p/335579/ (numbers from a different example)
Number of input reads = total - secondary - supplementary 
                      = 24887959 - 18203689 - 210060 
                      = 6474210

Number of unmapped reads = total - mapped
                         = 24887959 - 22937586
                         = 1950373

Number of mapped reads (primary excluding supplementary and secondary) = input - unmapped
                         = 6474210 - 1950373
                         = 4523837

% mapping = mapped (only primary) / input*100
          = 4523837 / 6474210 *100
          = 69.9%
#BUT that including the flag -F 260 in your flagstat should exclude unmapped reads and non-primary reads.
#Oh, also, because I have no secondary/supplementary, my math is much simplier
#input = total - secondary - supplementary
i = 985569 - 0 - 0 = 985569
u = 985569 - 985569 = 0
m = 985569 - 0
% = 985569/985569 * 100 = 100%
#But I suppose it might be more complicated for a solidissima
	631800 + 0 in total (QC-passed reads + QC-failed reads)
	631799 + 0 primary
	1 + 0 secondary
	0 + 0 supplementary
	0 + 0 duplicates
	0 + 0 primary duplicates
	631799 + 0 primary mapped (100.00% : N/A)
	631799 + 0 paired in sequencing
	307889 + 0 read1
	323910 + 0 read2
	566543 + 0 properly paired (89.67% : N/A)
	605872 + 0 with itself and mate mapped
	25927 + 0 singletons (4.10% : N/A)
	26323 + 0 with mate mapped to a different chr
	23300 + 0 with mate mapped to a different chr (mapQ>=5)
#Still 100% mapping, though messier pairing

#How to do per chromosome though?
samtools idxstats aln.bam | cut -f 1,3
samtools idxstats 413_005-RG.bam | cut -f 1,3 > teststats_413.txt 
	#This is the number of reads assigned to each chromosome
#Try the whole idx thing
	#Creates 4 column table:
	#reference sequence name, sequence length, # mapped read-segments and # unmapped read-segments

#By default the mapping quality for Freebayes as used by dDocent is 10 (90%) so I could still try my search by region?
	#Or I could go into dDocent by installing it myself and changing the parameters
	
#Date: Jan 19th
	#Freebayes has finished - it needs to run with the alignment
	#Align all similis and all solidissima to the similis reference (in the background)
	
	#What's next?
	#Notes from yesterday:
#Then after I have alignments for all 130 or so similis and probably ~20 solidissima from shelf and 20 from coast
        #Adjust the popmap file if needed (or start working on it while bwa is running)
        #Call vcfs from the ~110 good similis, ~20 shelf, and ~20 coast
                #Weirdly, I think after a dDocent run, it deletes all files in the folder named config so create a new one?
                #Filter to only contigs with consistent % mapped 
                #Perform normal vcf filtering with a focus on
                        #<10% missing data
                        #exactly 60 mapping score
                        #LD filtering
                        #Look for high minor allele frequency in both subspecies/all three

	#Start by making a popmap of sim/coast/shelf and one with simGA/simSCC/simNLI/Coast/Shelf
		#BAR_001 is marked as Shelf but it's the one in the middle
		#BLP_213 is from SLI but is Shelf
		#The SHN from shelf is not included here
	#Then filter vcf
	#filterDoc_b0119 #b is for both
cp dDoc_0118/TotalRawSNPs.vcf filterDoc_b0119

#Step 1
#Basic Filtering
vcftools --vcf TotalRawSNPs.vcf --max-missing 0.5 --mac 3 --minQ 30 --recode --recode-INFO-all --out raw.g5mac3
vcftools --vcf raw.g5mac3.recode.vcf --minDP 3 --recode --recode-INFO-all --out raw.g5mac3dp3 
vcftools --vcf raw.g5mac3dp3.recode.vcf --missing-indv
less out.imiss
awk '$5>0.5' out.imiss | cut -f1 > lowDP.indv 
vcftools --vcf raw.g5mac3dp3.recode.vcf --remove lowDP.indv --recode --recode-INFO-all --out raw.g5mac3dplm
vcftools --vcf raw.g5mac3dplm.recode.vcf --max-missing 0.90 --min-meanDP 20 --recode --recode-INFO-all --out DP3g95maf05 --min-meanDP 20
#Step 2
#Missing by Site/Population/Region/Species
	#First do by Similis sites and solidissima separate
		#If that does not work, then put all similis together
awk '$2 == "GA\r"' popmap_simGASCCCCB_ShelfCoast.txt > 1.keep
awk '$2 == "SCC\r"' popmap_simGASCCCCB_ShelfCoast.txt > 2.keep
awk '$2 == "NLI\r"' popmap_simGASCCCCB_ShelfCoast.txt > 3.keep
awk '$2 == "Shelf\r"' popmap_simGASCCCCB_ShelfCoast.txt > 4.keep
awk '$2 == "Coast\r"' popmap_simGASCCCCB_ShelfCoast.txt > 5.keep

for f in {1..5}
do
	vcftools --vcf DP3g95maf05.recode.vcf --keep $f.keep --missing-site --out $f
done
cat 1.lmiss 2.lmiss 3.lmiss 4.lmiss 5.lmiss | awk '!/CHR/' | awk '$6 > 0.1' | cut -f1,2 >> badloci
vcftools --vcf DP3g95maf05.recode.vcf --exclude-positions badloci --recode --recode-INFO-all --out DP3g95p5maf05

#Steps 3-7
		#Allele Balance
/programs/vcflib-1.0.1/bin/vcffilter -s -f "AB > 0.2 & AB < 0.8 | AB < 0.01" DP3g95p5maf05.recode.vcf > DP3g95p5maf05.fil1.vcf
		#check how many loci now
awk '!/#/' DP3g95p5maf05.recode.vcf | wc -l && awk '!/#/' DP3g95p5maf05.fil1.vcf | wc -l
	#The next filter we will apply filters out sites that have reads from both strands.
/programs/vcflib-1.0.1/bin/vcffilter -f "SAF / SAR > 100 & SRF / SRR > 100 | SAR / SAF > 100 & SRR / SRF > 100" -s DP3g95p5maf05.fil1.vcf > DP3g95p5maf05.fil2.vcf
awk '!/#/' DP3g95p5maf05.fil2.vcf | wc -l
	#The next filter looks at the ratio of mapping qualities between reference and alternate alleles
	#The rationale here is that, again, because RADseq loci and alleles all should start from the same genomic location there should not be large discrepancy between the mapping qualities of two alleles.
/programs/vcflib-1.0.1/bin/vcffilter -f "MQM / MQMR > 0.9 & MQM / MQMR < 1.05" DP3g95p5maf05.fil2.vcf > DP3g95p5maf05.fil3.vcf
awk '!/#/' DP3g95p5maf05.fil3.vcf | wc -l
	#Yet another filter that can be applied is whether or not their is a discrepancy in the properly paired status of for reads supporting reference or alternate alleles.
/programs/vcflib-1.0.1/bin/vcffilter -f "PAIRED > 0.05 & PAIREDR > 0.05 & PAIREDR / PAIRED < 1.75 & PAIREDR / PAIRED > 0.25 | PAIRED < 0.05 & PAIREDR < 0.05" -s DP3g95p5maf05.fil3.vcf > DP3g95p5maf05.fil4.vcf			
awk '!/#/' DP3g95p5maf05.fil4.vcf | wc -l
	#In short, with whole genome samples, it was found that high coverage can lead to inflated locus quality scores. Heng proposed that for read depths greater than the mean depth plus 2-3 times the square root of mean depth that the quality score will be twice as large as the depth in real variants and below that value for false variants.
	#I actually found that this is a little too conservative for RADseq data, likely because the reads aren’t randomly distributed across contigs. I implement two filters based on this idea. the first is removing any locus that has a quality score below 1/4 of the depth.
		#This part is complicated - do it with just all samples first
/programs/vcflib-1.0.1/bin/vcffilter -f "QUAL / DP > 0.25" DP3g95p5maf05.fil4.vcf > DP3g95p5maf05.fil5.vcf
cut -f8 DP3g95p5maf05.fil5.vcf | grep -oe "DP=[0-9]*" | sed -s 's/DP=//g' > DP3g95p5maf05.fil5.DEPTH
awk '!/#/' DP3g95p5maf05.fil5.vcf | cut -f1,2,6 > DP3g95p5maf05.fil5.vcf.loci.qual
awk '{ sum += $1; n++ } END { if (n > 0) print sum / n; }' DP3g95p5maf05.fil5.DEPTH
		134152 #mean depth			#his was 1.9k so this is a lot higher
#Now the the mean plus 3X the square root of the mean - done myself in google/wolfram
		134152 + 3*sqrt(134152) = 135250.8
		#Insert that value
paste DP3g95p5maf05.fil5.vcf.loci.qual DP3g95p5maf05.fil5.DEPTH | awk -v x=135251 '$4 > x' | awk '$3 < 2 * $4' > DP3g95p5maf05.fil5.lowQDloci
vcftools --vcf DP3g95p5maf05.fil5.vcf --site-depth --exclude-positions DP3g95p5maf05.fil5.lowQDloci --out DP3g95p5maf05.fil5
cut -f3 DP3g95p5maf05.fil5.ldepth > DP3g95p5maf05.fil5.site.depth
	#Now let’s calculate the average depth by dividing the above file by the number of individuals 31
		#I have 191 individuals
		#Insert number
awk '!/D/' DP3g95p5maf05.fil5.site.depth | awk -v x=191 '{print $1/x}' > meandepthpersite
vcftools --vcf  DP3g95p5maf05.fil5.vcf --recode-INFO-all --out DP3g95p5maf05.FIL --max-meanDP 200 --exclude-positions DP3g95p5maf05.fil5.lowQDloci --recode 

#Step #8
	#Recode to SNPs only and remove indels
/programs/vcflib-1.0.1/bin/vcfallelicprimitives DP3g95p5maf05.FIL.recode.vcf --keep-info --keep-geno > DP3g95p5maf05.prim.vcf
grep -v "#" DP3g95p5maf05.prim.vcf | wc -l 
vcftools --vcf DP3g95p5maf05.prim.vcf --remove-indels --min-alleles 2 --max-alleles 2 --recode --recode-INFO-all --out SNP.DP3g95p5maf05
	#Filter to two alleles only

#Step #9
	#HWE
	#curl -L -O https://github.com/jpuritz/dDocent/raw/master/scripts/filter_hwe_by_pop.pl
	#chmod +x filter_hwe_by_pop.pl
	./filter_hwe_by_pop.pl -v SNP.DP3g95p5maf05.recode.vcf -p popmap_simGASCCCCB_ShelfCoast.txt -o SNP.DP3g95p5maf05.HWE -h 0.001
	#Separates by popmap so it is okay they they are different species

#Step #10
	#Filter to only one per radtag by minor allele frequency
		#Temporary estimate
		grep -v "#" SNP.DP3g95p5maf05.HWE.recode.vcf | wc -l && grep -v "#" SNP.DP3g95p5maf05.HWE.recode.vcf | sort -k1,1 -u | wc -l	
		#Looks like I'll only be left with 55 SNPs so that's eh
	#Actual calculation
vcftools --vcf SNP.DP3g95p5maf05.HWE.recode.vcf --freq --out freqlist
		#Open in excel
		#Data by tabs and :
		#Before sorting, measure the minimum between the two (because it isn't minor allele, it's reference vs alternate)
		#Sort by max minor allele frequency then sort by unique contig
			#By removeing duplicates > expand selection > only select column A
		#Export by keeping column A and B without headers
		freq_keep.txt
			#1fprad = 1 (by frequency) per radtag
vcftools --vcf SNP.DP3g95p5maf05.HWE.recode.vcf --positions freq_keep.txt --recode --recode-INFO-all --out SNP_HWE_b0119_1fprad 

#Step #11
	#Haplotype caller from SNP.DP3g95p5maf05.HWE.vcf
	#https://github.com/chollenbeck/rad_haplotyper
	#Move back into alignment folder
	cp SNP.DP3g95p5maf05.HWE.recode.vcf ../dDoc_0118
	cd ../dDoc_0118/

#!/bin/bash -l

#SBATCH --partition=regular
#SBATCH --job-name=haps_b0119_th30_mem100
#SBATCH --output=haps_b0119.out
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --mail-user=hh693@cornell.edu
export PERL5LIB=/programs/PERL/lib/perl5
perl rad_haplotyper.pl -v SNP.DP3g95p5maf05.HWE.recode.vcf -r reference.fasta -x 30 \
-p popmap_simGASCCCCB_ShelfCoast.txt --genepop haps_simsol2sim.gp \
--ima haps_simsol2sim.ima
mv stats.out haps_stats_simsol2sim.out

sbatch --nodes=1 --ntasks=30 --mem=100000 s_haps_b0119.sh

	#Outputs
	mv stats.out haps_stats_simsol2sim.out
	Writing Genepop file: haps_simsol2sim.gp
	Writing IMa file: haps_simsol2sim.ima
	#Not sure what the file names should end with
	
#To do next:
	#Repeat filtering without allele balance for haplotype data set
		#Examine the effect of AB
	#Align all samples to solidissima catalog
	#Explore IMA2 : https://bio.cst.temple.edu/~tuf29449/software
		#Make tree: https://academic.oup.com/mbe/article/31/6/1622/2925701
		#Slides on trees: https://academic.oup.com/mbe/article/31/6/1622/2925701
			#Other tools, PAUP, P
		#I think I should use IMA3
			#Either way, the windows exe has example files and stuff so lets start there
	#Nick's Methods:
		#Genetic Diversity
			#genetic diversity for microsatellites was estimated in terms of observed and expected heterozygosity 
			#as allelic richness (average number of alleles per locus after controlling for sample size variation)
			#all calculated with Fstat ver. 2.9.32 (Goudet 2005). 
			#Differences in diversity between regions was tested with 10,000 permuations in Fstat
			#quantified in terms of observed and expected heterozygosity (gene diversity) in addition to nucleotide diversity (pie), haplotype diversity (h), and theta (θ) in terms of segregating sites (S) and homozygosity (Hom), calculated using ARLEQUIN ver. 3.5 (Excoffier and Lischer 2010). 
		#Population Structure
			#hierarchical Analysis of Molecular Variance (AMOVA) in ARLEQUIN 3.5, with significance testing over 1000 permutations
			#microsatellite data we estimated differentiation among population samples using Fst as calculated in Genepop, with significance testing based on an exact test
			#Assignment tests implemented in STRUCTURE v. 2.3.3 also were used to infer the number of differentiated populations and test for admixture between populations based on microsatellite data.
			#To test for admixture between subspecies, we first analyzed the 6 microsatellites that had the lowest missing data (< 5%) for both subspecies together
			#analyzed all 7 populations and set K = 1-7 with an admixture model, 100,000 steps and a burn-in of 20,000 steps
			#To infer the number of differentiated populations among the 5 S. s. similis population samples (Sim-GA, Sim-MA, Sim-LIS1-3), we analyzed all 9 microsatellites and set K = 1-5 with an admixture model, 100,000 steps and a burn-in of 20,000 steps. 
		#Demographic History
			#contemporary effective population sizes based on linkage disequilibrium occurring in the microsatellite data using NeEstimator ver. 2 (Do et al 2014), ignoring alleles with frequency < 0.02 to minimize bias
			#demographic expansion using Tajima’s D, and Fu’s FS using ARLEQUIN 3.5 (. Tajima’s D and Fu’s FS statistics were tested against a neutral model using 1000 simulated samples. )
			#used IMa2 (Hey 2010) to estimate demographic parameters of contemporary and ancestral relative population sizes (θ), migration rates (m), and divergence times (t) for Sim-GA, Sim-MA and Sol-NY populations
			#After exploring parameter space with a series of pilot IMa2 runs, we used priors on the model parameters in M-mode with maximum migration (m)= 2, max population size (q) =40 and maximum splitting time (t)= 20
			#implemented a divergence with gene flow model such that historical migration was tested between Sim-GA and Sim-MA sister populations, as well as between Sol-NY and the lineage that includes Sim-GA+Sim-MA
			#estimated population sizes, migration rates, and divergence times including only the Sim-GA and Sim-MA populations
			#Analyses were run with 1 X 107 MCMC steps following a burn-in of 10,000 steps, with an HKY mutation model
		#Results
			#nucleotide diversity at mtCOI was 4 times higher in Sol-NY S. s. solidissima (π=0.012, 15 haplotypes) than in Sim-GA with π=0.0032. Sim-GA had only 2 mtCOI haplotypes and only one haplotype was found in Massachusetts S.s. similis.
			#The reverse trend was found for observed heterozygosity across the three nDNA intron loci, with Sim-GA 22% higher than in Sol-NY on average, but other metrics did not show a consistent trend
			#no significant difference in allelic richness (southern (S)= 6.330 and northern (N)= 5.648), expected heterozygosity (HS) (S=0.704 and N= 0.666), or inbreeding coefficient (FIS) (S=0.173 and N=0.181)
			#all 7 population samples with STRUCTURE, K = 2 had the highest probability, with populations clustering by subspecies (Figure 2
			#In the microsatellite STRUCTURE analysis restricted to S. s. similis populations K = 3 had the highest probability, but a similar 3-way admixture pattern was present in all individuals 
			#Results from Fst analyses for mtCOI, NuSsim3823, NuSsim9017, and CAL-A are summarized in Table 3
		
			
		

#Step #12
	#Tests for loci of high effect and LD
	#a. LD test, vcftools will do R^2 correlation
			#Remove all with high R^2
			vcftools --vcf MyVariants.vcf --hap-r2 --ld-window-bp 10000 --out MyVariants.LD.10Kbp
			#andor bcftools cause vcftools is old now
				#By my window is 150 bp?
			#name prunedLD
			
			#start with 
			#33k SNPs for all		49k SNPs for sol only
			
			#code
			--hap-r2 #outputs file with the LD values ASSUMES PHASED HAPLOTYPES
			vcftools --vcf frag.vcf --hap-r2 --min-r2 .7 --ld-window-bp 50000 --out minr2_ld_window_50000
					#Min r2 does not filter the vcf, just reports only r2s greater than that because you are comparing all snps so it could be a lot

			vcftools --vcf SNP_HWE_b1215_sol.vcf --hap-r2 --ld-window-bp 600 --out SNP_solall.LD
			vcftools --vcf SNP_HWE_b1215_solonly.vcf --hap-r2 --ld-window-bp 600 --out SNP_solonly.LD &
			
			#Check the max and min location on the radtag for max ld window? Is it <300?
			grep -v "#" SNP_HWE_b1215_solonly.vcf | awk 'BEGIN{a=0}{if ($2>0+a) a=$2} END{print a}'
			545
			grep -v "#" SNP_HWE_b1215_solonly.vcf | awk 'BEGIN{a=10000}{if ($2<0+a) a=$2} END{print a}'
			2
			#set 600
			
			#For sol all
			After filtering, kept 8608 out of a possible 33677 Sites
			After filtering, kept 12808 out of a possible 49438 Sites
			#Meaning it could only calculate it for 8608 of them because they had phasing?
			grep -v "##" SNP_HWE_b1215_solonly.vcf | grep -e "|" | wc -l
			15951
			#Not quite I guess
			#Download, filter to min-r2 is 
			
			#Options?
			#i. Work with the SNPs that are in the calculations and also have a min-r2 of 0.7
				#Is high R2 yes LD or no LD?
				https://pbgworks.org/sites/pbgworks.org/files/measuresoflinkagedisequilibrium-111119214123-phpapp01_0.pdf
				#Complete LD (not separated by recombination) means D=1 and r^2 = 1
				#So remove high r2 values
					#r2 =0 means in complete equilibrium
				https://www.ndsu.edu/pubweb/~mcclean/plsc731/Linkage%20Disequilibrium%20-%20Association%20Mapping%20in%20Plants-lecture-overheads.pdf
				
				https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2735096/
				#First, before running PCA we used the PLINK28 software to exclude SNPs with pairwise genotypic r2 greater than 80% within sliding windows of 50 SNPs (with a 5-SNP increment between windows). Second, we took an iterative approach by running an initial PCA and removing chromosomal regions that showed evidence of reflecting regions of exceptional long-range linkage disequilibrium rather than genome-wide patterns of structure.
				
				#Other method, does not require phased
				--geno-r2
				#Calculates the squared correlation coefficient between genotypes encoded as 0, 1 and 2 to represent the number of non-reference alleles in each individual. This is the same as the LD measure reported by PLINK. The D and D’ statistics are only available for phased genotypes. The output file has the suffix ".geno.ld".
				#Since I already filtered for minor allele frquency, I do not need D and D'
			vcftools --vcf SNP_HWE_b1215_sol.vcf --geno-r2 --ld-window-bp 600 --out SNP_solall.LD &
			vcftools --vcf SNP_HWE_b1215_solonly.vcf --geno-r2 --ld-window-bp 600 --out SNP_solonly.LD &
				#Has some nans but is generally butter at only eliminating 20% by this mean as opposed to all
				
				#A. Plot
					#Runs for a long time. Start on a subet?
					#Why does solall have more markers?
						#Because solonly was still processing cause it takes longer cause more > I went with the intermediate because time pressure
				#B. Filter R2 < 0.5
				#C. Given what the output file looks like, I could just remove the second locus from the high LD pairs
					#Since I have to do it as two sets for pos 1 and pos 2 anyway, include p1 and p2 in keep and only p2s in remove
				#D. Filter to the list of keeps only then filter to remove anything from the remove list (i.e. loci that were in one good filter but were elsewhere in one bad one) <- could do in reverse order
					#vcftools keep from list
					#vcftools remove from list
					--positions <filename>
					--exclude-positions <filename>
				vcftools --vcf SNP_HWE_b1215_sol.vcf --positions Solall_keep_geno.txt --recode --recode-INFO-all --out SNP_HWE_b1215_sol_k1 &
				#After filtering, kept 17172 out of a possible 33677 Sites
				vcftools --vcf SNP_HWE_b1215_sol_k1.recode.vcf --exclude-positions Solall_remove_geno.txt --recode --recode-INFO-all --out SNP_HWE_b1215_sol_k2 &
				#After filtering, kept 12990 out of a possible 17172 Sites
				
				#Redo with more LD estimations
				vcftools --vcf SNP_HWE_b1215_solonly.vcf --positions Solonly_keep_geno2.txt --recode --recode-INFO-all --out SNP_HWE_b1215_solonly_k3 &
				#After filtering, kept 17336 out of a possible 49438 Sites
				vcftools --vcf SNP_HWE_b1215_solonly_k3.recode.vcf --exclude-positions Solonly_remove_geno2.txt --recode --recode-INFO-all --out SNP_HWE_b1215_solonly_k4 &
				#After filtering, kept 11218 out of a possible 17336 Sites
				
			#ii. Haplotype first? What about my haplotype data, is that all phased? <- Does not work ont he similis haplotype data because none of it is phased.
			
			#iii. Impute2? On the server makes data into phased
			export PATH=/programs/impute2:$PATH
			
			#iv. Look into other papers
				#A. Plough et al https://onlinelibrary.wiley.com/doi/pdf/10.1111/eva.13322
				#Thinning of the neutral dataset by chromosome was performed in VCFTOOLS using the thin function (Danecek et al., 2011).
				#Inclusion of loci that are strongly linked (high linkage disequilibrium) can lead to biases in downstream analyses if independence of loci is assumed (Willis et al., 2017)
				#The appropriate thinning distance was determined by calculating R2 separately for SNPs on the same chromosome (intrachromosomal pairs) and for unlinked SNPs (interchromosomal pairs). The critical R2 was estimated from the unlinked loci by root transforming the R2 values and taking the 95th percentile of the distribution as the threshold beyond which the LD is caused by physical linkage (Breseghello & Sorrells, 2006). The relationship between LD decay and genetic distance was summarized by fitting a second-degree smoothed locally weighted linear regression (LOESS) curve (Cleveland, 1979) to intrachromosomal R2 data in R. The distance the loess curve intercepted the critical R2 was identified as the threshold for LD decay (Figure S1).
					#Uh no
				#Can I thin while setting an arbitary R2 cut off
					#No, thin is just to filter loci that aren't that close to eachother and then they did that by decay distance?
				#B. https://onlinelibrary.wiley.com/doi/pdf/10.1111/1755-0998.12270
					#Pairwise linkage disequilibrium in the form of r 2 was calculated for collared flycatcher using Haploview version 4.2 (Barrett et al. 2005) for pairs of SNPs that were uniquely mapped onto the genome and had MAF >10% and genotyping rate >90%. 
				
		#b. One per radtag
			#Radtag by FST (works if I do one SNP per radtag)
			#Redefine populations as left and right. Barnstable is left? Just the few on the right are separate?
			SNP_HWE_b1215_sol_k2.recode.vcf
			SNP_HWE_b1215_solonly_k4.recode.vcf
			
			#i. Output minor allele frequency and filter to highest maf per radtag
			vcftools --vcf SNP_HWE_b1215_sol_k2.recode.vcf --freq --out freqlist_solall &
			vcftools --vcf SNP_HWE_b1215_solonly_k4.recode.vcf --freq --out freqlist_solonly &
				#Open in excel
				#Data by tabs and :
				#Sort by max minor allele frequency then sort by unique contig
					#By removeing duplicates, expand selection, only select column A
				#About 2.6k SNPs for each
				#1fprad = 1 (by frequency) per radtag
			vcftools --vcf SNP_HWE_b1215_sol_k2.recode.vcf --positions solall_freq_keep.txt --recode --recode-INFO-all --out SNP_HWE_b1215_solall_1fprad 
			vcftools --vcf SNP_HWE_b1215_solonly_k4.recode.vcf --positions solonly_freq_keep.txt --recode --recode-INFO-all --out SNP_HWE_b1215_solonly_1fprad 
			
			#ii. PCA first, then try fixed loci of high effect
			
			#Includes more phased than normal but not entirely:
			grep -v "#" SNP_HWE_b1215_solonly_1fprad.recode.vcf | grep -v -e "|" | wc -l
			1980


		#c. Fixed loci of high effect
			#Radtag by FST (works if I do one SNP per radtag)
			#Start in bash with 
			SNP_HWE_b1215_solonly_1fprad.recode.vcf
			SNP_HWE_b1215_solall_1fprad.recode.vcf
			#i. Make a pop file from the PCA data
				#Where did my PCA data go?
				#Saved to Sol_b1215PCA_coordinates.xlsx under advanced filtering
				#The same pop file works for both solall and solonly because I don't care about FST comparisons with similis and the same ones are RH/LH regardless of whether similis is there
				
			#ii. Vcftools FST per pop with each pop as its own file
			vcftools --vcf SNP_HWE_b1215_solonly_1fprad.recode.vcf --weir-fst-pop LH_pops_MidCCBsLH_1218.txt --weir-fst-pop RH_pops_1218.txt --out LHRH_midLH_solonly.fst 
				Weir and Cockerham mean Fst estimate: 0.13922
				Weir and Cockerham weighted Fst estimate: 0.27422
			
			vcftools --vcf SNP_HWE_b1215_solall_1fprad.recode.vcf --weir-fst-pop LH_pops_MidCCBsLH_1218.txt --weir-fst-pop RH_pops_1218.txt --out LHRH_midLH_solall.fst 
				Weir and Cockerham mean Fst estimate: 0.15557
				Weir and Cockerham weighted Fst estimate: 0.28583
			
			vcftools --vcf SNP_HWE_b1215_solonly_1fprad.recode.vcf --weir-fst-pop LH_pops_MidCCBsRemoved_1218.txt --weir-fst-pop RH_pops_1218.txt --out LHRH_midRemove_solonly.fst
			vcftools --vcf SNP_HWE_b1215_solall_1fprad.recode.vcf --weir-fst-pop LH_pops_MidCCBsRemoved_1218.txt --weir-fst-pop RH_pops_1218.txt --out LHRH_midRemove_solall.fst 

			#iii. Plot density first then decide cut off
				#High FST means super differenciated
				#What about negative FST?
					#Fst values using the calculations provided by Wier and Cockeham (1984) and most (all) programs that calculate Fst use these. It generally means there is more variation within than between populations (which is why most people report it as Fst=0)



#Date: Jan 22nd
#IMa2 testing
./src/IMa2

#Using 51 haplotype loci
#used priors on the model parameters in M-mode with maximum migration (m)= 2, max population size (q) =40 and maximum splitting time (t)= 20
#implemented a divergence with gene flow model such that historical migration was tested between Sim-GA and Sim-MA sister populations, as well as between Sol-NY and the lineage that includes Sim-GA+Sim-MA
#estimated population sizes, migration rates, and divergence times including only the Sim-GA and Sim-MA populations
#Analyses were run with 1 X 10^7 MCMC steps following a burn-in of 10,000 steps, with an HKY mutation model

./src/IMa2 -b 100000 \ #Number of burnin steps (small to start, increase later)
-c 1 \ # Include ranges on mutation rates as priors on mutation rate scalars
-j 2 \ #One migration parameter for each pair of populations (do not use with -p5)
-r 2 \ # Save the state of the Markov chain in a file - named with extension .mcf (MCMC mode only)
-o outputtest1.out \
-i ./examplefiles/ima2_testinput.u
#Line breaks/andor () in the comments do not work

./src/IMa2 -b 100000 -c 1 -j 2 -r 2 -o outputtest1.out -i ./examplefiles/ima2_testinput.u
#IMa2: not enough information provided on command line -  No prior information provided for population splitting times (-t)
	#The upper bound on the prior distribution for splitting times. This applies to all
	#splitting times in the model. If the user wishes to set priors individually for each
	#splitting time then a priorfile should be used (see –c3).

#With more than two populations it is important that the user know the true population phylogeny, including the sequence in time of the population splitting events in that phylogeny. 
#Also of note: number of loci/populations: a data set of 73 loci was only marginally adequate for three and four population models (Hey 2010a).

#Doesn't matter for the example set?
./src/IMa2 -b 100000 -c 1 -j 2 -r 2 -t 1000000 -o outputtest1.out -i ./examplefiles/ima2_testinput.u
#IMa2: not enough information provided on command line -  No information provided for maximum value for migration parameter (-m)
./src/IMa2 -b 100000 -c 1 -j 2 -r 2 -t 1000000 -m 3 -o outputtest1.out -i ./examplefiles/ima2_testinput.u
#Setting m to 2 because Nick
#And max population size to 40 and splitting time to 20
./src/IMa2 -b 100000 -c 1 -j 2 -r 2 -t 20 -m 2 -q 40 -o outputtest1.out -i ./examplefiles/ima2_testinput.u >& outputtest1.log

#I think that burnin only stopped once I changed the file IMburn > no. 
	#Eventhough it's in a different directory?
	#Or maybe because that's when it reached 1000000 steps? <- .ti file
	
#Phylogenetic Notation
(((NLI, SCC), GA), (Coast, Shelf))
((NLI, (SCC, GA)), (Coast, Shelf))
Note: naturally it had set it to (0,1):2 and for a more complex population 
((1,2):4,(3,0):5):6

pop1 = Coast, pop2 = GA, pop3 = NLI, pop4 = SCC, pop5 = Shelf
#I think the : is for other points
#Since it starts with pop0
pop1 = Coast, pop2 = GA, pop3 = NLI, pop4 = SCC, pop5 = Shelf

(3,2): (0,4)

4 
pop0  pop1  pop2  pop3  
((1,2):4,(3,0):5):6

5
Coast GA NLI SCC Shelf
0	  1   2   3   4

(((3,2):5,1):6,(0,4):7):8

#The GLD samle IDs were too long and got cut off?
#Also PEC?


#Run first test
./src/IMa2 -b 10000 -c 1 -j 2 -r 2 -t 20 -m 2 -q 40 -o test0123_a.out -i ./haps_simsol2sim.ima >& test0123_a.log
#IMa2: error in data - locus 0, data line 6: non-white space characters extend past position 308 in sequence executing program ...
#Locus 0 : dDocent_Contig_820, Infinite Sites model, 176 sites are variable

#Too many variable sites on the same contig? So it looks like recombination
#Or population names are too long?
	#It's assuming microsattelite loci? - page 49
	
	#To set parameters - page 49
		#assume geometric mean of 4Nu = population estimate at each locus
		
	#Oh! HKY model was not set, infinite sites (IS) model is used
	#How do I set?
	
	#Example data set - page 30-32
		#You can set the model by locus by setting the I in the line to an H (for HKY) OR IS = HapSTR for r HapSTR (J), the letter is followed immediately (no spaces) by the number of linked STR markers within the locus. 
grep " I " haps_simsol2sim.ima
sed -i 's/ I / H /g' haps_simsol2sim.ima 

#Tried with that fix but new error	
#IMa2: error reading data, too many lines or line too long - HKY data problem locus 0 gene# 6 site# 308 executing program ...

#Word counter says theres 308

#Replace long names
C0819_ with PEC_
D0819_ with GLD_
SsLI with LI
#May mess with the spacing
sed -i 's/C0819_/PEC_/g' haps_simsol2sim.ima
sed -i 's/D0819_/GLD_/g' haps_simsol2sim.ima
sed -i 's/SsLI/LI/g' haps_simsol2sim.ima

#IMa2: error reading data, too many lines or line too long - HKY data problem locus 15 gene# 0 site# 304 executing program ...
Locus 0: dDocent_Contig_820, HKY  model 304 sites after elimination of gaps 
Locus 1: dDocent_Contig_1102, HKY  model 309 sites after elimination of gaps 
Locus 2: dDocent_Contig_1041, HKY  model 300 sites after elimination of gaps 
Locus 3: dDocent_Contig_1115, HKY  model 300 sites after elimination of gaps 
Locus 4: dDocent_Contig_1166, HKY  model 300 sites after elimination of gaps 
Locus 5: dDocent_Contig_1067, HKY  model 300 sites after elimination of gaps 
Locus 6: dDocent_Contig_1186, HKY  model 300 sites after elimination of gaps 
Locus 7: dDocent_Contig_1192, HKY  model 300 sites after elimination of gaps 
Locus 8: dDocent_Contig_1218, HKY  model 300 sites after elimination of gaps 
Locus 9: dDocent_Contig_1161, HKY  model 300 sites after elimination of gaps 
Locus 10: dDocent_Contig_1210, HKY  model 304 sites after elimination of gaps 
Locus 11: dDocent_Contig_1074, HKY  model 300 sites after elimination of gaps 
Locus 12: dDocent_Contig_982, HKY  model 303 sites after elimination of gaps 
Locus 13: dDocent_Contig_984, HKY  model 300 sites after elimination of gaps 
Locus 14: dDocent_Contig_981, HKY  model 300 sites after elimination of gaps 
#New error, new sample name issue?
dDocent_Contig_813 54 46 26 158 92 304 H 1 #<- locus 15
#Yup sample is still too long
#AND OR, ALSO A SPACING ISSUE
LI1117_4A
sed -i 's/LI/I/g' haps_simsol2sim.ima
sed -i 's/GLD/D/g' haps_simsol2sim.ima
sed -i 's/PEC/P/g' haps_simsol2sim.ima
#Same error

#Download and remove the locus and then re-upload
#Also check #14 for errors at the end

#Restart from original

#Change each name specifically and with spaces
#Start with three loci
./src/IMa2 -b 10000 -c 1 -j 2 -r 2 -t 20 -m 2 -q 40 -o test0123_a.out -i ./haps_3locitest.txt >& test0123_a.log
#IMa2: error reading data, too many lines or line too long - HKY data problem locus 0 gene# 6 site# 308
#Fix names an spacing

#Try again with 3
#IMa2: error reading data, too many lines or line too long - HKY data problem locus 2 gene# 381 site# 304 executing program ...
Locus 0: dDocent_Contig_820, HKY  model 306 sites after elimination of gaps 
Locus 1: dDocent_Contig_1102, HKY  model 311 sites after elimination of gaps 

#add an enter at the end of the IMa2 file with 3 loci
#It already had one...
#But I still deleted and put in a fresh one and it looks like it might be working!

(((3,2):5,1):6,(0,4):7):8
#Test with larger data set after using the same name treatment
./src/IMa2 -b 10000 -c 1 -j 2 -r 2 -t 20 -m 2 -q 40 -o test0123_b.out -i ./haps_simsol2sim.ima >& test0123_b.log &

#Analyses were run with 1 X 107 MCMC steps following a burn-in of 10,000 steps, with an HKY mutation model.


./src/IMa2 -b 10000 -c 1 -j 2 -r 2 -t 20 -m 2 -q 40 -o test0123_c.out -i ./haps_simsol2sim.ima >& test0125_c.log
./src/IMa2 -b 10000 -c 1 -j 2 -r 2 -t 40 -m 4 -q 80 -o test0123_d.out -i ./haps_simsol2sim.ima >& test0125_d.log

parallel -j 2 < j_ima_cd.txt 

sbatch --nodes=1 --ntasks=2 --mem=20000 s_ima_0125.sh


#Date: Jan 26th
	#Get Similis only haplotype data in genepop and ima format
	#Which is the best set of similis filtered vcf?
	#Likely b1215 simonly
	SNP.simONLY_sf_1012.HWE.recode.vcf
	#In dDoc_928
	#Upload new popmap doc
	
#!/bin/bash -l

#SBATCH --partition=regular
#SBATCH --job-name=haps_simsf1012_th30_mem100
#SBATCH --output=haps_simsf1012_0126.out
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --mail-user=hh693@cornell.edu
export PERL5LIB=/programs/PERL/lib/perl5
perl rad_haplotyper.pl -v SNP.simONLY_sf_1012.HWE.recode.vcf -r reference.fasta -x 30 \
-p popmap2.txt --genepop haps_simONLY_0126.gen \
--ima haps_simONLY_0126.ima -o haps_0126.vcf
mv stats.out haps_stats_simONLY.out

	sbatch --nodes=1 --ntasks=30 --mem=100000 s_haps_simsf1012.sh


#Date: Feb 5th
#download the whole github repo as a zip https://github.com/arunsethuraman/ima2p
unzip ima2p-master.zip
cd ima2p-master/
chmod +x *
./configure -with-mpi=yes -prefix=/path/to/install
make
#Script is in src

#Start by running the same ima input file
#Server is full

#I want to get emailed when it finishes even though I'm on a different server
manage_slurm new cbsumm09
#Takes a minute

#to run in parallel
mpirun -np 11 ./src/IMa2p -b 10000 -hn 100 -hfg -hk 10 -ha 0.99 -hb 0.75 -c 1 -j 2 -r 2 -t 20 -m 2 -q 40 -o test0205_basic.out -i ./haps_simsol2sim.ima >& test0205_basic.log

mpirun -np 11 ./src/IMa2p -b 10000 -hn 100 -hfg -hk 10 -ha 0.99 -hb 0.75 -c 1 -j 2 -r 2 -t 80 -m 10 -q 40 -o test0205_b_bigger.out -i ./haps_simsol2sim.ima >& test0205_b_bigger.log


#!/bin/bash -l

#SBATCH --partition=regular
#SBATCH --job-name=JOBNAME_0205
#SBATCH --output=JOBNAME_0205.out
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --mail-user=hh693@cornell.edu

sbatch --nodes=1 --ntasks=11 --mem=60000 basic.sh
sbatch --nodes=1 --ntasks=11 --mem=60000 b_bigger.sh

#Run 2 on single thread
./src/IMa2 -b 10000 -c 1 -j 2 -r 2 -t 40 -m 4 -q 80 -o test0123_d.out -i ./haps_simsol2sim.ima >& test0125_d.log

#!/bin/bash -l

#SBATCH --partition=regular
#SBATCH --job-name=c_single_0205
#SBATCH --output=c_single_0205.out
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --mail-user=hh693@cornell.edu
./src/IMa2 -b 10000 -c 1 -j 2 -r 2 -t 80 -m 8 -q 160 -o test0205_c_single.out -i ./haps_simsol2sim.ima >& test00205_c_singleout.log

sbatch --nodes=1 --ntasks=1 --mem=6000 test_c.sh

#Test d: test multiple on small number of loci to confirm that it works

#!/bin/bash -l

#SBATCH --partition=regular
#SBATCH --job-name=d_multi3loci_0205
#SBATCH --output=d_multi3loci_0205.out
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --mail-user=hh693@cornell.edu
mpirun -np 20 ./src/IMa2p -b 100 -hn 50 -hfg -hk 5 -ha 0.90 -hb 0.75 -c 1 -j 2 -r 2 -t 20 -m 2 -q 40 -o test0205_d_3loci.out -i ./haps_3locitest.ima >& test0205_d_3loci.log

manage_slurm new cbsumm31
sbatch --nodes=1 --ntasks=20 --mem=60000 test_d.sh

#Test e: migration prior
nano e_migrationprior.txt
#m is the mean of the prior distribution or if a prior file is used the values given for migration rates are means of exponential distribution priors

mpirun -np 19 ./src/IMa2p -b 10000 -hn100 -hfg -hk10 -ha0.99 -hb0.75 -c 3 -j 7 -g e_migrationprior.txt -r 2 -t 40 -q 80 -o test0205_e.out -i ./haps_simsol2sim.ima >& test0205_e.log
sbatch --nodes=1 --ntasks=20 --mem=130000 test_e.sh

#does not look like the ima2p is working
#Nothing is coming into the log file....


#Try IMgui? It can also run parallel and graphical on mac
https://github.com/jaredgk/IMgui-electron-packages
#Download Zip
tar -xvzf IMgui-linux-x64.tgz


./IMa2 -b10000 -c3 -d1000 -g/workdir/hh693/ima2-p/pm_migration1.txt -hfl -hn100 -hk10 -ha0.99 -hb0.75 -i/workdir/hh693/ima2-p/haps_simsol2sim.ima -l100 -m8 -o/workdir/hh693/ima2-p/outputtest_pm_g -q80 -t100 -z1000 -j2 -p27 -r25 

#HKY is recommended for mtDNA data whereas mine is not

#How do I actually use the priors file?
	#Now that I've added time and population size priors
	-c3 -g./priorsall_test_h.txt

#Create a series of runs for over night (9pm + 18 hours = 9am + 6 hours = 2pm)

#The only ones that survived
a
c
elin
o
p
r
t
u

#Test list
	####pma	Nick settings	linear	
	#pmb	Nick settings 	geom	lh	
	####pmc	Nick settings 	linear		migration prior m=2 (modern sim to sim and sol to sol only)
	#pmd	Nick settings 	geom	lh	migration prior m=2
	#pme	Nick 	 		geom	lh	migration prior m=4
		####pmelin				linear
	#pmf	Nick 	 		geom	lh	migration prior m=6
	#pmg	Nick 	 		geom	lh	migration prior m=8	
		#pmglin				linear
	#pmh	Nick 			geom	lh	migration prior m=12
	#pmi	Nick 	 		geom	lh	migration prior m=20
	#pmj	Nick m=8	 	geom	lh
		#pmjlin				linear		Also remove -j2 cause that might be wrong
	#pmk	Nick m=12	 	geom	lh
		#pmklin				linear
	#pml	Nick m=20	 	geom	lh
	#pmm	Nick m=6	 	geom	ll
	#pmn	Nick m=12		geom	ll
	####pmo	Nick 			geom	ll migration prior m=4
	####pmp	Nick 			geom	ll migration prior m=12	
	#pmq	Nick m=8		linear
	####pmr	Nick 			linear	   migration prior m=4
	####pmt	Nick 	 		geom	lh	migration prior m=4 with no old population stuff
	####pmu	Nick 	 		geom	lh	migration prior m=8	 with no old population stuff
	#pmv	Nick 			geom	lh	migration prior m=12 with no old population stuff
	#pmw	Nick basic		linear 	test setting l

#lh = larger and high heating as recommended in the document
-hfg -hn150 -ha0.999 -hb0.3
#ll - large data set and low heating
-hfg -hn40 -ha0.96 -hb0.9

#One strategy is to do multiple M mode runs that save a lot of genealogies but that do only basic analyses (e.g. don’t use –c2 which can take a long time)
	#In the end you will want to have at least 20,000 saved genealogies from at least two well mixed runs, and if you plan to estimate joint posteriors and do likelihood-ratio tests you will need at least 100,000 genealogies
	#and then after all these runs are done to do an L mode run that uses all the .ti files and that invokes all the analyses that you want. When you do an L mode run you can specify how many genealogies to load. 
#data set of 73 loci was only marginally adequate for three and four population models (Hey 2010a)
	#I have 51 for 5...

#remove -j 2 from the scripts with priors
sed -e 's/ -j 2//g' -i s_pmt.sh
sbatch --nodes=1 --ntasks=1 --mem=10000 s_pmc.sh

#Notes
#q does not yet need to be higher than 40
#m needs to be higher than 4
	#6
	#8
	#12
	#20
#t does not need to be higher either


#!/bin/bash -l

#SBATCH --partition=regular
#SBATCH --job-name=c_single_0205
#SBATCH --output=c_single_0205.out
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --mail-user=hh693@cornell.edu
../src/IMa2 -b 10000 -c 1 -j 2 -r 2 -t 80 -m 8 -q 160 -o test0205_c_single.out -i ./haps_simsol2sim.ima >& test00205_c_singleout.log

manage_slurm new cbsumm13
sbatch --nodes=1 --ntasks=1 --partition=regular --mem=10000 s_pma.sh
sbatch --nodes=1 --ntasks=1 --mem=10000


#at the end of this all, do 
*.mcf ../outandlogs_0202_pm
*.ti ../outandlogs_0202_pm

#Failed overnihgt
m
q
#Same error on both? - IMa2: update_t_NW.cpp:630: double update_mig_tNW(int, edge*, int): Assertion `minfo[j].upb[i] == nowedgepop (ci, &gtree[ei], minfo[j].uptime[i])' failed.
#The geom files are not outputting anything to their logs
most others failed
#Same error - but for geom does not appear until later: 
#IMa2: update_t_NW.cpp:630: double update_mig_tNW(int, edge*, int): Assertion `minfo[j].upb[i] == nowedgepop (ci, &gtree[ei], minfo[j].uptime[i])' failed.
	#This is a cluster only error, so I could run it not in the slurm and it might work?
	#This also emails me with the echo in the message and the mail thing in the header
	echo "test" | mail -s "Test Done" hh693@cornell.edu
		#Does not tell me if it fails/succeeds
	#Change where ".out" is saved


#The only ones that survived
a
c
elin
o
p
r
t
u

#Date: Feb 6th
#The new ones I started outside of slurm
am26jlin	#pmj linear, I also removed -j 2 incase that is throwing things off
am26b		#test most basic geometric mode
am26d		#basic geometric m=2 prior (these already without -j2)
am26dlin	#linear with m=2 prior
am26e		#geom with m=4
am26g		#geom with m=8
am26q		#linear with m=8
am26qnj		#linear with m=8 with no j
am26v		#no old populations prior m=12
am26vm2		#no old populations prior m=4 (cnonfusing name)

	#pmh	Nick 			geom	lh	migration prior m=12
	#pmi	Nick 	 		geom	lh	migration prior m=20
	#pmk	Nick m=12	 	geom	lh
		#pmklin				linear
	#pml	Nick m=20	 	geom	lh
	#pmm	Nick m=6	 	geom	ll
	#pmn	Nick m=12		geom	ll
	#pmw	Nick basic		linear 	test setting l

#STRUCUTURE also on node 31

#Ended early/failed
am26e
	#IMa2: update_t_NW.cpp:630: double update_mig_tNW(int, edge*, int): Assertion `minfo[j].upb[i] == nowedgepop (ci, &gtree[ei], minfo[j].uptime[i])' failed.
am26g
	#IMa2: update_t_NW.cpp:630: double update_mig_tNW(int, edge*, int): Assertion `minfo[j].upb[i] == nowedgepop (ci, &gtree[ei], minfo[j].uptime[i])' failed.
am26b
	#IMa2: update_t_NW.cpp:630: double update_mig_tNW(int, edge*, int): Assertion `minfo[j].upb[i] == nowedgepop (ci, &gtree[ei], minfo[j].uptime[i])' failed.
am26dlin
	#IMa2: update_t_NW.cpp:630: double update_mig_tNW(int, edge*, int): Assertion `minfo[j].upb[i] == nowedgepop (ci, &gtree[ei], minfo[j].uptime[i])' failed.
am26d
	#Failed in a new way:
	#IMa2: update_gtree_common.cpp:1902: void treeweight(int, int): Assertion `jj < npops - k' failed.
	
	
#Try another method? AIM? MIST?
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6944047/


#Still running
pmo
pmp
am26q
am26jlin



#Date: Feb 8th
#To fix the fomatting issue in .ima from rad haplotyper
#Can replace all 1BN and 2BN and 1AN etc with 1A N
#However, this does not solve the problem of things that are too long:
#SOOOO, just adjust the specific IDs that are too long
#Replace	#With
SsLI		sLI
D0819		GLD0
C0819		PEC0
1BN			1B N
#This may not work, it is likely safer to only reduce each sample name by one digit
#_fixed indicates that I've made these changes

#Done #Matt Hare Are threshold effects expected for metapopulation network stability? Can you explain the role of habitat heterogeneity that you mentioned?
#Done #Lars R. Temperate lakes have a large ranges of temperature available, and fish can select by moving vertically during the summer.  At least deep freshwater environments may be more similar to terrestrial that temperature selection based on movement?
#Nina Hand When talking about distribution shifts, you mentioned population extirpation and expansion. At this point, how much data is available to partition species-level responses to population differences? (e.g. are all populations just moving or are we losing some unique populations?
#Rebecca in the clown fish study, are there peak years in reproduction,  ie pulses that may drive overall longer term persistence?
#Elise Given that thermal sensitivity is more sensitive at different latitudes have you found in this work or thought about the role of disease in your work specifically in relation to certain diseases that often have a thermal component (I think of chytrid fungus effects in frogs for example)?

#The simONLY haplotypes includes some confusing things and 5 populations where it should only have 3
#Remove lines that start with MW_01A or MW_01B
#Grep all lines that include 304 ( for number of samples ) <- that does not work
#Replace 304 with 302


#Replace " 2 " with " "
sed -e 's/ 2 / /g' haps_simONLY_0126_fixed.ima > test.u
#Remove lines that start with MW_01A or MW_01B
grep -v "MW_01A" test.u | grep -v "MW_01B" > test2.u

#Oh! And I do not have to change 304 cause it is not the sum of the number of populations but rather the length oof the radtag
#This explains why it would always fail at line 77, because that's where the 4th population showed up and it was expecting only 3 because that's what it said in the header


#Date: Feb 10th
#Long ima runs for testing
((2,1):3,0):4
#Just similis
#Remove lines sed -e '5,10d;12d' file > filenew
sed -e '14032,305345d' haps_simONLY_fixed2full.u > hapssim_20loci.u
sed -e '38033,305345d' haps_simONLY_fixed2full.u > hapssim_100loci.u
	#Did not work quite as expected but I think that mostly was due to my own estimating issues, the command works

#Run ideas				signifier
	#1260 loci			12
	#33 loci			33
	#156 loci			56
	#basic Nick			b
	#m rate #			m2/m#
	#m prior complex	mp2
	#heating (geom)		h
		#type			z


#If they start failing (via quality metrics)
	#Try different phylogeny with GA paired to SCC
	#For migration upper: e. I suggest 2 times the inverse, but 5 might also be fine. Unless you have a lot of data, starting with much higher values is likely to lead to a poorly mixing MCMC simulation. 
	
#Trial Run
../src/IMa2 -otrialrun.out -q2 -m1 -t3 -b10000 -l100 -s123
	#Took 5:33
	

#!/bin/bash -l

#SBATCH --partition=regular
#SBATCH --job-name=ima_trial
#SBATCH --output=../outs_0210/ima_trial.out
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --mail-user=hh693@cornell.edu
../src/IMa2 -i ../hapssim_33loci.u -o../outs_0210/trialrun.out -q2 -m1 -t3 -b10000 -l100 -s123
sbatch --nodes=1 --ntasks=1 --partition=regular --mem=10000 trialrun.sh

#Data is not compatible with I model so it needs to be HK " I ", " H "
#One strategy is to do multiple M mode runs that save a lot of genealogies but that do only basic analyses (e.g. don’t use –c2 which can take a long time), and then after all these runs are done to do an L mode run that uses all the .ti files and that invokes all the analyses that you want. When you do an L mode run you can specify how many genealogies to load. 
#Took 5:33
#What is -s? The seed, do not need to provide if they start at least a minute apart

#in order to use the IMRUN file, it has to be in the same directory as the stuff for the run
#under the runfile directory
#../runfiles/run_JOB

nano s_JOB.sh
#!/bin/bash -l

#SBATCH --partition=regular
#SBATCH --job-name=ima_JOB
#SBATCH --output=../outs_0210/ima_JOB_s.out
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --mail-user=hh693@cornell.edu

../src/IMa2 -b 10000 \
-i ../runfiles/run_JOB/hapssim_33loci.u \
-c 1 -r 2 -t 20 -m 2 -q 40 \
-o ../outs_0210/JOB.out >& ../outs_0210/JOB.log

mkdir ../runfiles/run_JOB
cp ../hapssim_33loci.u ../runfiles/run_JOB
nano ../runfiles/run_JOB/IMrun

nano s_33b.sh
#!/bin/bash -l

#SBATCH --partition=regular
#SBATCH --job-name=ima_33b
#SBATCH --output=../outs_0210/ima_JOB_s.out
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --mail-user=hh693@cornell.edu

../src/IMa2 -b 10000 -l1.0 \
-i ../runfiles/run_33b/hapssim_33loci.u \
-c 1 -r 2 -t 20 -m 2 -q 40 \
-o ../outs_0210/33b.out >& ../outs_0210/33b.log

mkdir ../runfiles/run_33b
cp ../hapssim_33loci.u ../runfiles/run_33b
nano ../runfiles/run_33b/IMrun

sbatch --nodes=1 --ntasks=1 --partition=regular --mem=10000 s_33b.sh


../src/IMa2 -b 10000 \
-c 3 -g../priors/p_m4.txt -r 2 \
-o ../outs_0210/JOB.out -i ../hapssim_20loci.u >& ../outs_0210/JOB.log

nano s_39b.sh
#!/bin/bash -l

#SBATCH --partition=regular
#SBATCH --job-name=ima_39b
#SBATCH --output=../outs_0210/ima_JOB_s.out
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --mail-user=hh693@cornell.edu

../src/IMa2 -b 10000 -l1.0 \
-i ../runfiles/run_39b/hapssim_399loci.u \
-c 1 -r 2 -t 20 -m 2 -q 40 \
-o ../outs_0210/39b.out >& ../outs_0210/39b.log

mkdir ../runfiles/run_39b
cp ../hapssim_399loci.u ../runfiles/run_39b
nano ../runfiles/run_39b/IMrun

sbatch --nodes=1 --ntasks=1 --partition=regular --mem=10000 s_39b.sh
#400 loci max
sed -e '6,208345d' hapssim_1260loci.u > hapssim_399loci.u 



#Other useful things to do:
	#Page 55
		#create mcf files that do burnin

#B = Burnin run
#1-10 = need several

nano s_33b_B3.sh
#!/bin/bash -l

#SBATCH --partition=regular
#SBATCH --job-name=ima_33b_B3
#SBATCH --output=../outs_0210/ima_33b_B3_s.out
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --mail-user=hh693@cornell.edu

../src/IMa2 -b4.0 -l10 \
-i ../runfiles/run_33b_B3/hapssim_33loci.u \
-c 1 -r 2 -t 20 -m 2 -q 40 \
-o ../outs_0210/33b_B3.out >& ../outs_0210/33b_B3.log

mkdir ../runfiles/run_33b_B3
cp ../hapssim_33loci.u ../runfiles/run_33b_B3
nano ../runfiles/run_33b_B3/IMburn

sbatch --nodes=1 --ntasks=1 --partition=regular --mem=10000 s_33b_B3.sh


#tmrca Acceptance % are still really really low

#geometric
nano s_33b_hz.sh
#!/bin/bash -l

#SBATCH --partition=regular
#SBATCH --job-name=ima_33b_hz
#SBATCH --output=../outs_0210/ima_33b_hz_s.out
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --mail-user=hh693@cornell.edu

../src/IMa2 -b1.0 -l2.0 \
-i ../runfiles/run_33b_hz/hapssim_33loci.u \
-c 1 -r 2 -t 20 -m 2 -q 40 \
-hfg -hn20 -ha0.96 -hb0.9 \
-o ../outs_0210/33b_hz.out >& ../outs_0210/33b_hz.log

mkdir ../runfiles/run_33b_hz
cp ../hapssim_33loci.u ../runfiles/run_33b_hz
nano ../runfiles/run_33b_hz/IMrun
nano ../runfiles/run_33b_hz/IMburn

sbatch --nodes=1 --ntasks=1 --partition=regular --mem=10000 s_33b_hz.sh

#hz
	#Small to medium sized data set (e.g. < 15 loci, < 20 individuals per locus), low heating: 
	# -hfg -hn20 -ha0.96 -hb0.9
#hy
	#Small to medium sized data set, high heating:
	#-hfg -hn80 -ha0.999 -hb0.3
#hx
	#Larger data set, low heating:
	#-hfg -hn40 -ha0.96 -hb0.9
#hw
	#Larger data set, high heating:
	#-hfg -hn150 -ha0.999 -hb0.3
	

#Job finished!!!
#ima_56b finished? After 1.5 hours? How?
	#It did not print any graphs. I think it just printed after the end of the burnin?
	#Use prior ranges on mutation rates specified in input file 
	#!Less than 2 prior ranges given in input file,  mutation rate priors not used! 
	#Number of steps in chain following burnin:      29030
	#Number of steps between recording : 10  Number of record steps: 2903 
	#Number of steps between saving genealogy information: 100  Number of genealogies saved per locus: 291 

#Try adding new IMrun file in the directory with all the scripts
#Run something with absolutely everything in the same directory == #od

nano s_39b_od.sh
#!/bin/bash -l

#SBATCH --partition=regular
#SBATCH --job-name=ima_od_39b
#SBATCH --output=ima_od_39b_s.out
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --mail-user=hh693@cornell.edu

./src/IMa2 -b10000 -l2.0 \
-i hapssim_399loci.u \
-c 1 -r 2 -t 20 -m 2 -q 40 \
-o 39b_od.out >& 39b_od.log


sbatch --nodes=1 --ntasks=1 --partition=regular --mem=10000 s_39b_od.sh


#Make burn in much longer
#Keep it all in the same directory
#LB = long burnin

nano s_33b_odLB_m4.sh
#!/bin/bash -l

#SBATCH --partition=regular
#SBATCH --job-name=ima_od_33b_LB_m4
#SBATCH --output=ima_od_33b_LB_m4_s.out
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --mail-user=hh693@cornell.edu

./src/IMa2 -b100000 -l1000000 \
-i hapssim_33loci.u \
-c 1 -r 2 -t 20 -m 4 -q 40 \
-o 33b_odLB_m4.out >& 33b_odLB_m4.log


sbatch --nodes=1 --ntasks=1 --partition=regular --mem=10000 s_33b_odLB_m4.sh









#Arelequin
#Install on Mac
chmod +x arlecoremac_64bit
#Back to PC
#Try to make areliquin file from genpop file


#FST per radtag
vcftools --vcf SNP_HWE_b1215_solonly_1fprad.recode.vcf --weir-fst-pop LH_pops_MidCCBsLH_1218.txt --weir-fst-pop RH_pops_1218.txt --out LHRH_midLH_solonly.fst 

#Use the vcf file from radhaplotyper
less dDoc_928/undedup/simonly_b1215_haps.vcf
vcftools --vcf dDoc_928/undedup/simonly_b1215_haps.vcf --weir-fst-pop pop_SCC.txt --weir-fst-pop pop_GA.txt --weir-fst-pop pop_NLI.txt --out simonly_hapsvcf.fst 
Weir and Cockerham mean Fst estimate: 0.023624
Weir and Cockerham weighted Fst estimate: 0.05096
After filtering, kept 9527 out of a possible 9527 Sites


CTM_001
CTM_002
CTM_003
CTM_004
CTM_005
CTM_006
CTM_007
CTM_008
CTM_009
CTM_010
ELP_001
ELP_002
ELP_003
ELP_004
ELP_005
ELP_006
ELP_007
ELP_008
ELP_009
ELP_010
GA12_001
GA12_002
GA12_003
GA12_004
GA12_006
GA12_007
GA12_008
GA12_009
GA12_010
GA12_012
GA12_013
GA12_014
GA12_015
GA12_016
GA12_017
GA12_018
GA12_019
GA12_020
GA12_021
GA12_022
GA12_023
GA12_024
GA12_025
GLD0819_001
GLD0819_002
GLD0819_003
GLD0819_004
GLD0819_005
GLD0819_006
GLD0819_007
GLD0819_008
MW_001
MW_002
MW_003
MW_004
MW_005
MW_006
MW_007
MW_008
MW_010
MW_011
MW_014
MW_015
MW_016
MW_018
MW_019
MW_01
MW_022
MW_023
MW_024
MW_025
MW_026
MW_027
MW_028
MW_029
MW_030
MW_031
MW_032
MW_033
MW_034
MW_035
MW_036
MW_037
MW_038
MW_039
MW_040
MW_041
MW_042
MW_043
MW_044
MW_045
MW_046
MW_047
MW_048
MW_049
MW_050
MW_051
MW_052
MW_053
MW_054
MW_057
MW_058
MW_059
MW_060
MW_061
MW_062
MW_063
MW_064
MW_065
MW_066
MW_067
PEC0819_011
PEC0819_012
PEC0819_013
PPB_001
PPB_002
PPB_003
PPB_004
PPB_005
PPB_006
RP20_009
RP20_010
WFH_001
WFH_002
WFH_003
WFH_004
WFH_005
WFH_006
WFH_007
WFH_008


#Create Treemix
Quick installation:

chmod +x *
./configure
make
make install

Full instructions and additional downloads available at:
http://bitbucket.org/nygcresearch/treemix

tar -xvzf treemix_test_files.tar.gz

#Convert vcf to treemix input file
https://github.com/speciationgenomics/scripts/blob/master/vcf2treemix.sh
#What is the best similis only vcf file?
../filterDoc_b1215/sim/SNP.simONLY_sf_1012.HWE.recode.vcf
#Is at least pretty good
 echo "Please provide the following arguments: <vcf file> <clust file>"
 echo "The .clust file contains three columns: samplename\tsamplename\tgroup"
 

After filtering, kept 10762 out of a possible 12751 Sites

#LD assumptions so one per radtag
vcftools --vcf SNP.simONLY_sf_1012.HWE.recode.vcf --freq --out freqlist_sim &
				#Open in excel
				#Data by tabs and :
				#Sort by max minor allele frequency then sort by unique contig
					#By removeing duplicates, expand selection, only select column A
				#About 1.5k SNPs for each
				#1fprad = 1 (by frequency) per radtag
vcftools --vcf SNP.simONLY_sf_1012.HWE.recode.vcf --positions snps_0214_keepbyfreq.txt --recode --recode-INFO-all --out SNP_HWE_tm214


bash vcf2treemix_2.sh SNP_HWE_tm214.recode.vcf treeclust_1.txt
	#vcf2treemix 2 fixed by removing plink --allowchrom and adding "./" before the the python script
	
#output is
SNP_HWE_tm214.recode.treemix.frq.gz
treemix -i SNP_HWE_tm214.recode.treemix.frq.gz -root GA -m 3 -o out_stem



#Keep all snps
#And incorporate -k10 (SNPs close together can be considered LD - not a clean way to do it)
bash vcf2treemix_2.sh SNP.simONLY_sf_1012.HWE.recode.vcf treeclust_1.txt

treemix -i SNP.simONLY_sf_1012.HWE.recode.treemix.frq.gz -root GA -m 3 -k 20 -o out_stem_allSNPs


#Date: Feb 15th
#Set up pairwise ima

#From the main files
grep -v "GA12" hapssim_33loci.u > haps_NLI_SCC_33loci_prep.u
#Replace " 44 " with " "
#Also " 46 "
grep -v "PEC" hapssim_33loci.u | grep -v "RP20" | grep -v "GLD" > haps_GA_SCC_33loci_prep.u
#" 24 "
#" 26 "
#" 16 "
grep -v "MW" hapssim_33loci.u | grep -v "PPB" | grep -v "ELP" | grep -v "CTM" | grep -v "WFH" > haps_GA_NLI_33loci_prep.u
# 156 
# 170 
# 172 
# 174 
# 168 
# 176 
# 162 
# 166 
# 160 

#Do also with the 399 loci set
grep -v "GA12" hapssim_399loci.u > haps_NLI_SCC_399loci_prep.u
grep -v "PEC" hapssim_399loci.u | grep -v "RP20" | grep -v "GLD" > haps_GA_SCC_399loci_prep.u
grep -v "MW" hapssim_399loci.u | grep -v "PPB" | grep -v "ELP" | grep -v "CTM" | grep -v "WFH" > haps_GA_NLI_399loci_prep.u


nano s_pw_399_GANLI.sh
nano IMrun

#!/bin/bash -l

#SBATCH --partition=regular
#SBATCH --job-name=pw_GA_NLI_399
#SBATCH --output=pw_GA_NLI_399_s.out
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --mail-user=hh693@cornell.edu

./src/IMa2 -b10000 -l2.0 \
-i haps_GA_NLI_399loci_prep.u \
-c 1 -r 2 -t 20 -m 2 -q 40 \
-o pw_GA_NLI_399.out >& pw_GA_NLI_399.log

sbatch --nodes=1 --ntasks=1 --partition=regular --mem=10000 s_pw_399_GANLI.sh


#!/bin/bash -l

#SBATCH --partition=regular
#SBATCH --job-name=ima_od_33b
#SBATCH --output=ima_od_33b_s.out
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --mail-user=hh693@cornell.edu

./src/IMa2 -b10000 -l2.0 \
-i hapssim_33loci.u \
-c 1 -r 2 -t 20 -m 2 -q 40 \
-o 33b_od.out >& 33b_od.log




#More Treemix
#Date: Feb 17th
#Now I want to do: 
#First, migration events were set to 0 (no admixture) and model residuals compared. 
#Migration was then increased until model residuals were equivalent between ecotypes (Pickrell & Pritchard, 2012).

bash vcf2treemix_2.sh SNP_HWE_tm214.recode.vcf treeclust_1.txt
	#vcf2treemix 2 fixed by removing plink --allowchrom and adding "./" before the the python script
#output is
SNP_HWE_tm214.recode.treemix.frq.gz

#Using the same SNP file of 12,751 SNPs
#Did before: treemix -i SNP.simONLY_sf_1012.HWE.recode.treemix.frq.gz -root GA -m 3 -k 20 -o out_stem_allSNPs
treemix -i SNP.simONLY_sf_1012.HWE.recode.treemix.frq.gz -root GA -m 0 -k 311 -o out_stem_r


#k is 311 (for max contig length)
#Run several different
TreeMix v. 1.12
$Revision: 231 $

Options:
-i [file name] input file
-o [stem] output stem (will be [stem].treeout.gz, [stem].cov.gz, [stem].modelcov.gz)
-k [int] number of SNPs per block for estimation of covariance matrix (1)
-global Do a round of global rearrangements after adding all populations
-tf [file name] Read the tree topology from a file, rather than estimating it
-m [int] number of migration edges to add (0)
-root [string] comma-delimited list of populations to set on one side of the root (for migration)
-g [vertices file name] [edges file name] read the graph from a previous TreeMix run
-se Calculate standard errors of migration weights (computationally expensive)
-micro microsatellite data
-bootstrap Perform a single bootstrap replicate
-cor_mig [file] list of known migration events to include (also use -climb)
-noss Turn off sample size correction
-seed [int] Set the seed for random number generation

#Don't root m=0
#nr0
treemix -i SNP.simONLY_sf_1012.HWE.recode.treemix.frq.gz -m 0 -k 311 -o out_stem_nr0
	#-nan
#Does not work with k = 311 because there are no clear positions so it is just number of SNPs
treemix -i SNP.simONLY_sf_1012.HWE.recode.treemix.frq.gz -m 0 -k 20 -o out_stem_nr020
	#Starting ln(likelihood) with 0 migration events: 38.6572 
	#Exiting ln(likelihood) with 0 migration events: 38.6572
#Try with the only one per radtag
treemix -i SNP_HWE_tm214.recode.treemix.frq.gz -m 0 -o out_stem_nr0nk
	#Estimating covariance matrix in 287 blocks of size 1
	#Starting ln(likelihood) with 0 migration events: 36.5725 
	#Exiting ln(likelihood) with 0 migration events: 36.5725 

#Plot residuals
plot resid("outstem", "poporder")


treemix -i SNP.simONLY_sf_1012.HWE.recode.treemix.frq.gz -m 0 -o out_stem_nr0nk_all
#Even without setting the root, it roots to GA



treemix -i SNP.simONLY_sf_1012.HWE.recode.treemix.frq.gz -m 0 -k 10 -o out_stem_nr0k10_all
#Estimating covariance matrix in 29 blocks of size 10
treemix -i SNP_HWE_tm214.recode.treemix.frq.gz -m 0 -k 10 -o out_stem_nr0k10
#Estimating covariance matrix in 28 blocks of size 10
#???


#Try what I actually intend
#Root on GA
#Start with m0
#Then increase m by 1 
# -se #calculate errors of migration weights (unless it takes more than a few minutes)
treemix -i SNP.simONLY_sf_1012.HWE.recode.treemix.frq.gz -root GA -m 0 -se -o out_stem_rGA_m0
treemix -i SNP.simONLY_sf_1012.HWE.recode.treemix.frq.gz -root GA -m 1 -se -o out_stem_rGA_m1
treemix -i SNP.simONLY_sf_1012.HWE.recode.treemix.frq.gz -root GA -m 2 -se -o out_stem_rGA_m2
treemix -i SNP.simONLY_sf_1012.HWE.recode.treemix.frq.gz -root GA -m 3 -se -o out_stem_rGA_m3
treemix -i SNP.simONLY_sf_1012.HWE.recode.treemix.frq.gz -root GA -m 4 -se -o out_stem_rGA_m4
treemix -i SNP.simONLY_sf_1012.HWE.recode.treemix.frq.gz -root GA -m 5 -se -o out_stem_rGA_m5
treemix -i SNP.simONLY_sf_1012.HWE.recode.treemix.frq.gz -root GA -m 6 -se -o out_stem_rGA_m6


#Later try also with -bootstrap
treemix -i SNP.simONLY_sf_1012.HWE.recode.treemix.frq.gz -root GA -m 0 -se -bootstrap -o out_stem_rGA_m0b
treemix -i SNP.simONLY_sf_1012.HWE.recode.treemix.frq.gz -root GA -m 1 -se -bootstrap -o out_stem_rGA_m1b
treemix -i SNP.simONLY_sf_1012.HWE.recode.treemix.frq.gz -root GA -m 2 -se -bootstrap -o out_stem_rGA_m2b
treemix -i SNP.simONLY_sf_1012.HWE.recode.treemix.frq.gz -root GA -m 3 -se -bootstrap -o out_stem_rGA_m3b
treemix -i SNP.simONLY_sf_1012.HWE.recode.treemix.frq.gz -root GA -m 4 -se -bootstrap -o out_stem_rGA_m4b

#Try with solidissima
cp filterDoc_b0119/SNP.DP3g95p5maf05.HWE.recode.vcf treemix
mv SNP.DP3g95p5maf05.HWE.recode.vcf SNP.HWE.simandsol.vcf
bash vcf2treemix_2.sh SNP.HWE.simandsol.vcf popmapcluster_simGASCCCCB_ShelfCoast.txt
#332 sites
treemix -i SNP.HWE.simandsol.treemix.frq.gz -m 0 -se -o out_stem_simsol


#Loci with maximum FST
#From important graphs, find the high fst loci
#Blast those radtags
#Reference fasta from 928
#

cat list.txt | awk '{gsub("_","\\_",$0);$0="(?s)^>"$0".*?(?=\\n(\\z|>))"}1' | pcregrep -oM -f - reference.fasta > newfile.fasta

#Create vcfs
#Or can I just do it by not including some samples in the cluster?
cp SNP_HWE_tm214.recode.vcf SNP_HWE_tm222_GANLI.recode.vcf
cp SNP_HWE_tm214.recode.vcf SNP_HWE_tm222_GASCC.recode.vcf
cp SNP_HWE_tm214.recode.vcf SNP_HWE_tm222_NLISCC.recode.vcf

bash vcf2treemix_2.sh SNP_HWE_tm222_GANLI.recode.vcf treeclust_GANLI.txt
bash vcf2treemix_2.sh SNP_HWE_tm222_NLISCC.recode.vcf treeclust_NLISCC.txt &
bash vcf2treemix_2.sh SNP_HWE_tm222_GASCC.recode.vcf treeclust_GASCC.txt &

#Treemix pairwise
treemix -i SNP_HWE_tm222_GANLI.recode.treemix.frq.gz -m 0 -se -o out_pw_GANLIm0

#Treemmix with all and full tree


bash vcf2treemix_2.sh SNP_HWE_tm214.recode.vcf treeclust_1.txt
	#vcf2treemix 2 fixed by removing plink --allowchrom and adding "./" before the the python script
#output is
SNP_HWE_tm214.recode.treemix.frq.gz

#Using the same SNP file of 12,751 SNPs
#Did before: treemix -i SNP.simONLY_sf_1012.HWE.recode.treemix.frq.gz -root GA -m 3 -k 20 -o out_stem_allSNPs
treemix -i SNP.simONLY_sf_1012.HWE.recode.treemix.frq.gz -root GA -m 0 -k 311 -o out_stem_r

#SNE,Sol
cp SNP.HWE.simandsol.vcf SNP_NWE_simsol_SNE.vcf
bash vcf2treemix_2.sh SNP_NWE_simsol_SNE.vcf treeclust_simsol_SNE.txt
cp SNP.HWE.simandsol.vcf SNP_NWE_shelfcoast_SNE.vcf
bash vcf2treemix_2.sh SNP_NWE_shelfcoast_SNE.vcf treeclust_shelfcoastSNE.txt &


treemix -i SNP_NWE_simsol_SNE.treemix.frq.gz -m 0 -se -o out_solSNEm0
treemix -i SNP_NWE_simsol_SNE.treemix.frq.gz -m 0 -root Solidissima -se -o out_solSNEm0r


treemix -i SNP_NWE_simsol_SNE.treemix.frq.gz -bootstrap -m 0 -se -o out_solSNEm0b
treemix -i SNP_NWE_simsol_SNE.treemix.frq.gz -bootstrap -m 0 -root Solidissima -se -o out_solSNEm0rb
treemix -i testin.gz -bootstrap -m 0 ise -o intest_m0


grep  -c -v "^#" SNP_NWE_simsol_SNE.vcf 
grep  -c -v "^#" ../filterDoc_b0119/SNP.DP3g95p5maf05.HWE.recode.vcf
grep  -c -v "^#" ../filterDoc_b1215/sim/SNP_HWE_b1215_sim.vcf 

cp filterDoc_b0119/SNP.DP3g95p5maf05.HWE.recode.vcf treemix
mv SNP.DP3g95p5maf05.HWE.recode.vcf SNP.HWE.simandsol.vcf
bash vcf2treemix_2.sh SNP.HWE.simandsol.vcf popmapcluster_simGASCCCCB_ShelfCoast.txt
#332 sites
treemix -i SNP.HWE.simandsol.treemix.frq.gz -m 0 -se -o out_stem_simsol
#Very few sites

#Try 7k sites (fewer solidissima)
SNP_HWE_simsol7k.vcf
cp SNP_HWE_simsol7k.vcf SNP_HWE_simsol_SNE_7k.vcf
bash vcf2treemix_2.sh SNP_HWE_simsol_SNE_7k.vcf treeclust_simsol_SNE.txt
cp SNP_HWE_simsol7k.vcf SNP_HWE_shelfcoast_SNE_7k.vcf
bash vcf2treemix_2.sh SNP_HWE_shelfcoast_SNE_7k.vcf treeclust_shelfcoastSNE.txt


treemix -i SNP_HWE_simsol_SNE_7k.treemix.frq.gz -bootstrap -m 0 -se -o out_solSNE_7km0b
treemix -i SNP_HWE_simsol_SNE_7k.treemix.frq.gz -bootstrap -m 0 -k 30 -root Solidissima -se -o out_solSNE_7k3m0rb
treemix -i SNP_HWE_simsol_SNE_7k.treemix.frq.gz -bootstrap -m 1 -k 30 -root Solidissima -se -o out_solSNE_7k3m1rb
treemix -i SNP_HWE_simsol_SNE_7k.treemix.frq.gz -bootstrap -m 2 -k 30 -root Solidissima -se -o out_solSNE_7k3m2rb

cp SNP_HWE_simsol7k.vcf SNP_HWE_simsol_7k.vcf
bash vcf2treemix_2.sh SNP_HWE_simsol_7k.vcf popmapcluster_simGASCCCCB_ShelfCoast.txt 

treemix -i SNP_HWE_simsol_7k.treemix.frq.gz -bootstrap -m 0 -k 30 -root Solidissima -se -o out_solSNE_7k3m0rb



#Date: Feb 24th
#MEGA

#vcf to fasta with reference
cat ref.fa | vcf-consensus your_file.vcf.gz > out.fa
cat ref.fa | vcf-consensus your_file.vcf.gz > out.fa

SNP_HWE_4consensus_simall.vcf
Could not load .tbi index of SNP_HWE_b1215_sim.vcf

bgzip -c SNP_HWE_4consensus_simall.vcf > SNP_HWE_4consensus_simall.vcf.gz
tabix -p vcf SNP_HWE_4consensus_simall.vcf.gz

cat reference.fasta | vcf-consensus SNP_HWE_4consensus_simall.vcf.gz > out_sim.fa
#Just makes a consensus sequence...
#Make consensus of just each of the populations?

SNP_HWE_4consensus_simall.vcf.gz
cat reference.fasta | vcf-consensus SNP_HWE_4consensus_simall.vcf.gz > out_sim.fa


#Make consensus of just each of the populations?
vcftools --vcf SNP_HWE_4consensus_simall.vcf --recode --out consensus_GA --keep GA_con.txt

cat reference.fasta | vcf-consensus consensus_GA.recode.vcf > consensus_GA.fa



#Date: Mar 8th
#STRUCTURE again
	#i. 	Sol
	#ii.	~30 from all groups
	#iii. 	Redo similis 
	#iv. 	Just SNE
	#v.		Just Shelf
	#vi.	Just Coast
#STRUCT_0208

structure -K 1 -o filter_1012/out_a_K1  >& filter_1012/my_run_a_K1.log -m mainp_1012a

#define INFILE SNP_105_simonly.structure
#define OUTFILE out_105_sim
#define NUMLOCI 1021
#define NUMINDS 130
#define MAXPOPS 7
#define POPFLAG 0
#define PHENOTYPE       0
#define EXTRACOLS       0
#define MISSING -9
#define ONEROWPERIND 0
#define LABEL   1
#define POPDATA 1
#define PLOIDY 2

#First I need to make the right vcf files and then make them STRUCTURE
filterDoc_b1215/sim/SNP.simONLY_sf_1012.recode.vcf #still has multiple per radtag
filterDoc_b1215/sol/SNP_HWEfilterb1215noLD_sol.vcf #some sim (402)
filterDoc_b1215/sol/SNP_HWEfilterb1215noLD_solonly.vcf #391
	#bcftools query -l filterDoc_b1215/sol/SNP_HWEfilterb1215noLD_solonly.vcf | wc -l
filterDoc_b1215/SNP_HWE_b1215_solonly_1fprad.recode.vcf #better
filterDoc_b0119/SNP_HWE_b0119_1fprad.recode.vcf #54 loci from 191 individuals <- that's not all of them...
#dDoc_0118/SNP.DP3g95p5maf05.HWE.recode.vcf #<- also only 191 cause I only ever did similis + 40 solidissima
	#bcftools query -l dDoc_0118/SNP.DP3g95p5maf05.HWE.recode.vcf | wc -l


#move over to organize
cp filterDoc_b1215/sim/SNP.simONLY_sf_1012.recode.vcf STRUCTURE/STRUCT_0208/
cp filterDoc_b1215/SNP_HWE_b1215_solonly_1fprad.recode.vcf STRUCTURE/STRUCT_0208/
cp filterDoc_b0119/SNP_HWE_b0119_1fprad.recode.vcf STRUCTURE/STRUCT_0208/


#Step #8
	#Recode to SNPs only and remove indels
/programs/vcflib-1.0.1/bin/vcfallelicprimitives SNP.simONLY_sf_1012.recode.vcf --keep-info --keep-geno > DP3g95p5maf05.prim.vcf
grep -v "#" DP3g95p5maf05.prim.vcf | wc -l 
vcftools --vcf DP3g95p5maf05.prim.vcf --remove-indels --min-alleles 2 --max-alleles 2 --recode --recode-INFO-all --out SNP.sim
	#Filter to two alleles only

#Step #9
	#HWE
	#curl -L -O https://github.com/jpuritz/dDocent/raw/master/scripts/filter_hwe_by_pop.pl
	#chmod +x filter_hwe_by_pop.pl
	./filter_hwe_by_pop.pl -v SNP.sim.recode.vcf -p popmap_sim -o SNP.simONLY_sf.HWE -h 0.001
	#Separates by popmap so it is okay they they are different species

#Step #10
	#Actual calculation
vcftools --vcf SNP.simONLY_sf.HWE.recode.vcf --freq --out freqlist
		#Open in excel
		#Data by tabs and :
		#Before sorting, measure the minimum between the two (because it isn't minor allele, it's reference vs alternate)
		#Sort by max minor allele frequency then sort by unique contig
			#By removeing duplicates > expand selection > only select column A
		#Export by keeping column A and B without headers
		freq_keep.txt
			#1fprad = 1 (by frequency) per radtag

#Date: Mar 8th
#STRUCTURE again
	#i. 	Sol
	#ii.	~30 from all groups
	#iii. 	Redo similis 
	#iv. 	Just SNE
	#v.		Just Shelf
	#vi.	Just Coast
#STRUCT_0208

structure -K 1 -o ii_both/out_K1 -i SNP_ii_both.structure >& ii_both/my_run_K1.log -m mainp_ii_both
#REMOVE SNP NAME HEADERS
	#NUMBER OF SNPS
	#Coast and Shelf: 2794
	#Both: 54
	#All Sol: 2794
	#Simlis: 1508
	#SNE: 1508
	#NUMBER OF INDS FROM wc -l / 2

#define INFILE SNP_ii_both.structure
#define OUTFILE out_test
#define NUMLOCI 54
#define NUMINDS 191
#define MAXPOPS 7
#define POPFLAG 0
#define PHENOTYPE       0
#define EXTRACOLS       0
#define MISSING -9
#define ONEROWPERIND 0
#define LABEL   1
#define POPDATA 1
#define PLOIDY 2

structure -K 1 -o ii_both/out_K1 -i SNP_ii_both.structure -L 54 -N 191 >& ii_both/my_run_K1.log -m mainp_ii_both

#Make many
for f in {1..10}
do
echo "structure -K $f -o v_Shelf/out_K$f -i SNP_v_solShelf.structure -L 2794 -N 279 >& v_Shelf/my_run_K$f.log -m mainp_ii_both"
echo "structure -K $f -o vi_Coast/out_K$f -i SNP_vi_solCoast.structure -L 2794 -N 112 >& vi_Coast/my_run_K$f.log -m mainp_ii_both"

#echo "structure -K $f -o ii_both/out_K$f -i SNP_ii_both.structure -L 54 -N 191 >& ii_both/my_run_K$f.log -m mainp_ii_both"
#echo "structure -K $f -o filter_1012/out_sim_K$f  >& filter_1012/my_run_sim_K$f.log"
done

sbatch --nodes=1 --ntasks=10 --mem=40000 s_mar8batch.sh --output STRUCT_mar8.out
#Remember to remove header from STRUCTURE file produced by PGDspider

#Date: Mar 14th
export PERL5LIB=/programs/PERL/lib/perl5
perl rad_haplotyper.pl -v SNP_HWE_b1215_solonly_k4.recode.vcf \
-x 15 -r reference.fasta -p popmap_SvC.txt --genepop haps_sol_SvC_0314.gen \
-n -o haps_SvC_0314.vcf --ima haps_sol_SvC_0314.ima &

export PERL5LIB=/programs/PERL/lib/perl5
perl rad_haplotyper.pl -v SNP_HWE_b1215_solonly_k4.recode.vcf \
-x 15 -r reference.fasta -p popmap_region.txt --genepop haps_sol_region_0314.gen \
-n -o haps_0314.vcf --ima haps_sol_region_0314.ima &


#I have to make my own per library
export PERL5LIB=/home/hh693/perl/lib/perl5/


sbatch --nodes=1 --ntasks=15 --mem=30000 s_hap_0314.sh --output haps_log0314.out
sbatch --nodes=1 --ntasks=15 --mem=30000 s_hap_0314b.sh --output haps_log0314b.out


#Date: Mar 17th
#Structure with region for solidissima
for f in {1..10}
do
echo "structure -K $f -o vii_sol_allRegion/out_K$f -i SNP_vii_sol_allRegion.structure -L 2794 -N 391 >& vii_sol_allRegion/my_run_K$f.log -m mainp_ii_both"
done

sbatch --nodes=1 --ntasks=10 --mem=30000 s_mar14batch.sh --output log0317_STRUCT.out


#Date: Mar 23rd
#Tree mix between the two solidissima groups with similis as the outgroup
#Work with SNPs/Haplotype from the original sol_a alignment

#Date: Mar 28th
vcftools --vcf DP3g95p5maf05.FIL.recode.vcf
#After filtering, kept 191 out of 191 Individuals
#After filtering, kept 321 out of a possible 321 Sites

#Date: Mar 30th
#Do a new alignment with all similis and all solidissima
#Do one to each reference (sol reference is better, sim reference to compare to prior)


#The reference in dDoc_0118 is similis
#The reference in dDoc_sol_121 BUT IT's EMPTY? > Cause it is in align121
#So the reference in align121 is solidissima

#Now I need ALL of the trimmed reads and then I need to make them formatted correctly
#Which step in the trimming process do I want? No STACKS?

#For aligning to transcriptome I did:
#take reads from synctrim6
../similis_clean_dups/sync_trim_6/"$p"_R1.trim.sync.fastq ../similis_clean_dups/sync_trim_6/"$p"_R2.trim.sync.fastq >& similis_clean_dups/sync_trim_6/"$p"_resync6.log

#Pretty sure for solidissima
solidis_clean/sync_trim_6
#Nothing saved, must have moved them
#The reads in dDoc_sol_121 are good then


cp /local/storage/Spisula/GBS/similis_clean_dups/sync_trim_6/*.fastq all_trimsync_reads &
		#Also includes the solidissima that were sequenced with them 
		#Remove (see below)
cp /local/storage/Spisula/GBS/dDoc_sol_121/*.fq.gz all_trimsync_reads &

#Return with cp the trimmed reads to 
cp /local/storage/Spisula/GBS/dDoc_sol_121/*R1.fq.gz /local/storage/Spisula/GBS/solidis_clean/sync_trim_6_final/ &
cp /local/storage/Spisula/GBS/dDoc_sol_121/*R2.fq.gz /local/storage/Spisula/GBS/solidis_clean/sync_trim_6_final/ &

#Remove Problem Samples
#BY PUTTING THEM IN A FOLDER, let the idea that this includes all reads stand 
#(including duplicates from the different projects)
	#ELP is similis
	
	#Which CTM and MW are sol?
	MW_001
	MW_002
	MW_016
	MW_033
	CTM_008
	
	#Known Problem Samples to Remove
	MW_055		#Bad sequencing
	MW_056		#Bad sequencing
	NAN_004		#Contaminated
	NAN_005		#Contaminated
	PVT_008		#Same Individual
	PVT_008H07	#Same Individual
	FNJ_028		#Mess with PCA
	FNJ_029 	#Mess with PCA
	#Relatedness filter did not remove any from similis, should do with solidissima later
	
#Fix all the formatting of stuff
parallel -j 2 < scripts/j_gzip
parallel -j 15 < scripts/j_rename

#How do I set up the dDocent run?


#!/bin/bash -l

#SBATCH --partition=regular
#SBATCH --job-name=dDoc0330_sim_th8_mem120
#SBATCH --output=dDoc0330_sim.out
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --mail-user=hh693@cornell.edu
source /programs/miniconda3/bin/activate dDocent-2.8.13
dDocent config_sim0330.txt

#!/bin/bash -l

#SBATCH --partition=regular
#SBATCH --job-name=dDoc0330_sol_th8_mem120
#SBATCH --output=dDoc0330_sol.out
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --mail-user=hh693@cornell.edu
source /programs/miniconda3/bin/activate dDocent-2.8.13
dDocent config_sol0330.txt

cd sim_ref
sbatch --nodes=1 --ntasks=18 --mem=120000 dDoc_sim0330.sh
cd ../sol_ref
sbatch --nodes=1 --ntasks=8 --mem=120000 dDoc_sol0330.sh

Number of Processors
8
Maximum Memory
0
Trimming
no
Assembly?
no
Type_of_Assembly
PE
Clustering_Similarity%
0.85
Minimum within individual coverage level to include a read for assembly (K1)
2
Minimum number of individuals a read must be present in to include for assembly (K2)
2
Mapping_Reads?
yes
Mapping_Match_Value
1
Mapping_MisMatch_Value
4
Mapping_GapOpen_Penalty
6
Calling_SNPs?
yes
Email
hh693@cornell.edu

#For sol it failed when I did solidissima, so now with even more I have to have 12k mem per core
#Try again on 10 cores with 150mem
		#8 mem per core got us to 72%
		#11 mem per core (math guess) to 100?
		#This is 15 mem per core.

#250 mem - 80 mem currently in use
	#170 left
	#Once his is done, I'd have a max of 250/2 = 125 mem per run
	#8 cores times 15 = 120mem


#Similis failed because of PVT_006
#Try again assuming alignment worked and move to call

#!/bin/bash -l

#SBATCH --partition=regular
#SBATCH --job-name=dDoc0330_sol_th8_mem120
#SBATCH --output=dDoc0330_sol.out
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --mail-user=hh693@cornell.edu
source /programs/miniconda3/bin/activate dDocent-2.8.13
dDocent config_sim_call0401.txt 

sbatch --nodes=1 --ntasks=8 --mem=120000 dDoc_sim_call0401.sh 

#Solidissima failed too!!!
#It's the same problem with PVT too

sbatch --nodes=1 --ntasks=8 --mem=120000 dDoc_sol_call0401.sh 


#Date: Apr 6th 2022
#Filtering the dDocent similis alignment

#Step 1
#Basic Filtering
vcftools --vcf TotalRawSNPs.vcf --max-missing 0.5 --mac 3 --minQ 30 --recode --recode-INFO-all --out raw.g5mac3
vcftools --vcf raw.g5mac3.recode.vcf --minDP 3 --recode --recode-INFO-all --out raw.g5mac3dp3 
vcftools --vcf raw.g5mac3dp3.recode.vcf --missing-indv
awk '$5>0.5' out.imiss | cut -f1 > lowDP.indv 
#awk '$5>0.9' out.imiss | cut -f1 > lowDP.indv 
#Set threshold higher for solidissima ref because it's messy - then I'll filter to more loci cause 0.5 might remove all NAN and lots of MW 
vcftools --vcf raw.g5mac3dp3.recode.vcf --remove lowDP.indv --recode --recode-INFO-all --out raw.g5mac3dplm
vcftools --vcf raw.g5mac3dplm.recode.vcf --max-missing 0.90 --min-meanDP 20 --recode --recode-INFO-all --out DP3g95maf05 --min-meanDP 20
#For solidissima, after this step, check the individuals again
#vcftools --vcf DP3g95maf05.recode.vcf --missing-indv --out 2
#awk '$5>0.5' 2.imiss | cut -f1 > lowDP2.indv 
#None have that much missing data - works! Since I have so many sites, I could remove even fewer
#vcftools --vcf raw.g5mac3dplm.recode.vcf --max-missing 0.95 --min-meanDP 20 --recode --recode-INFO-all --out DP3g95maf05 --min-meanDP 20


#Step 2
#Missing by Site/Population/Region/Species
	#First do by Similis sites and solidissima separate
		#If that does not work, then put all similis together
awk '$2 == "GA\r"' popmap_all.txt > 1.keep
awk '$2 == "SCC\r"' popmap_all.txt > 2.keep
awk '$2 == "NLI\r"' popmap_all.txt > 3.keep
awk '$2 == "B\r"' popmap_all.txt > 4.keep
awk '$2 == "A\r"' popmap_all.txt > 5.keep

for f in {1..5}
do
	vcftools --vcf DP3g95maf05.recode.vcf --keep $f.keep --missing-site --out $f
done
cat 1.lmiss 2.lmiss 3.lmiss 4.lmiss 5.lmiss | awk '!/CHR/' | awk '$6 > 0.1' | cut -f1,2 >> badloci
vcftools --vcf DP3g95maf05.recode.vcf --exclude-positions badloci --recode --recode-INFO-all --out DP3g95p5maf05

#Steps 3-7
		#Allele Balance
/programs/vcflib-1.0.1/bin/vcffilter -s -f "AB > 0.2 & AB < 0.8 | AB < 0.01" DP3g95p5maf05.recode.vcf > DP3g95p5maf05.fil1.vcf
		#check how many loci now
awk '!/#/' DP3g95p5maf05.recode.vcf | wc -l && awk '!/#/' DP3g95p5maf05.fil1.vcf | wc -l
	#The next filter we will apply filters out sites that have reads from both strands.
/programs/vcflib-1.0.1/bin/vcffilter -f "SAF / SAR > 100 & SRF / SRR > 100 | SAR / SAF > 100 & SRR / SRF > 100" -s DP3g95p5maf05.fil1.vcf > DP3g95p5maf05.fil2.vcf
awk '!/#/' DP3g95p5maf05.fil2.vcf | wc -l
	#The next filter looks at the ratio of mapping qualities between reference and alternate alleles
	#The rationale here is that, again, because RADseq loci and alleles all should start from the same genomic location there should not be large discrepancy between the mapping qualities of two alleles.
/programs/vcflib-1.0.1/bin/vcffilter -f "MQM / MQMR > 0.9 & MQM / MQMR < 1.05" DP3g95p5maf05.fil2.vcf > DP3g95p5maf05.fil3.vcf
awk '!/#/' DP3g95p5maf05.fil3.vcf | wc -l
	#Yet another filter that can be applied is whether or not their is a discrepancy in the properly paired status of for reads supporting reference or alternate alleles.
/programs/vcflib-1.0.1/bin/vcffilter -f "PAIRED > 0.05 & PAIREDR > 0.05 & PAIREDR / PAIRED < 1.75 & PAIREDR / PAIRED > 0.25 | PAIRED < 0.05 & PAIREDR < 0.05" -s DP3g95p5maf05.fil3.vcf > DP3g95p5maf05.fil4.vcf			
awk '!/#/' DP3g95p5maf05.fil4.vcf | wc -l
	#In short, with whole genome samples, it was found that high coverage can lead to inflated locus quality scores. Heng proposed that for read depths greater than the mean depth plus 2-3 times the square root of mean depth that the quality score will be twice as large as the depth in real variants and below that value for false variants.
	#I actually found that this is a little too conservative for RADseq data, likely because the reads aren’t randomly distributed across contigs. I implement two filters based on this idea. the first is removing any locus that has a quality score below 1/4 of the depth.
		#This part is complicated - do it with just all samples first
/programs/vcflib-1.0.1/bin/vcffilter -f "QUAL / DP > 0.25" DP3g95p5maf05.fil4.vcf > DP3g95p5maf05.fil5.vcf
cut -f8 DP3g95p5maf05.fil5.vcf | grep -oe "DP=[0-9]*" | sed -s 's/DP=//g' > DP3g95p5maf05.fil5.DEPTH
awk '!/#/' DP3g95p5maf05.fil5.vcf | cut -f1,2,6 > DP3g95p5maf05.fil5.vcf.loci.qual
awk '{ sum += $1; n++ } END { if (n > 0) print sum / n; }' DP3g95p5maf05.fil5.DEPTH
		46469.5 #mean depth			#his was 1.9k so this is a lot higher
#Now the the mean plus 3X the square root of the mean - done myself in google/wolfram
		46469.5 + 3*sqrt(46469.5) = 47116.2
		#Insert that value
paste DP3g95p5maf05.fil5.vcf.loci.qual DP3g95p5maf05.fil5.DEPTH | awk -v x=47116 '$4 > x' | awk '$3 < 2 * $4' > DP3g95p5maf05.fil5.lowQDloci
vcftools --vcf DP3g95p5maf05.fil5.vcf --site-depth --exclude-positions DP3g95p5maf05.fil5.lowQDloci --out DP3g95p5maf05.fil5
cut -f3 DP3g95p5maf05.fil5.ldepth > DP3g95p5maf05.fil5.site.depth
	#Now let’s calculate the average depth by dividing the above file by the number of individuals 31
		#I have 446 individuals
		#Insert number
awk '!/D/' DP3g95p5maf05.fil5.site.depth | awk -v x=105 '{print $1/x}' > meandepthpersite
vcftools --vcf  DP3g95p5maf05.fil5.vcf --recode-INFO-all --out DP3g95p5maf05.FIL --max-meanDP 200 --exclude-positions DP3g95p5maf05.fil5.lowQDloci --recode 

#Step #8
	#Recode to SNPs only and remove indels
/programs/vcflib-1.0.1/bin/vcfallelicprimitives DP3g95p5maf05.FIL.recode.vcf --keep-info --keep-geno > DP3g95p5maf05.prim.vcf
grep -v "#" DP3g95p5maf05.prim.vcf | wc -l 
vcftools --vcf DP3g95p5maf05.prim.vcf --remove-indels --min-alleles 2 --max-alleles 2 --recode --recode-INFO-all --out SNP.DP3g95p5maf05
	#Filter to two alleles only

#Step #9
	#HWE
	#curl -L -O https://github.com/jpuritz/dDocent/raw/master/scripts/filter_hwe_by_pop.pl
	#chmod +x filter_hwe_by_pop.pl
	./filter_hwe_by_pop.pl -v SNP.DP3g95p5maf05.recode.vcf -p popmap_all.txt -o SNP.DP3g95p5maf05.HWE -h 0.001
	#Separates by popmap so it is okay they they are different species
cp SNP.DP3g95p5maf05.HWE.recode.vcf SNP_HWE_b0406_basic.recode.vcf

#Step #10
	#Filter to only one per radtag by minor allele frequency
	#Actual calculation
vcftools --vcf SNP.DP3g95p5maf05.HWE.recode.vcf --freq --out freqlist
		#Open freqlist.fq in excel
		#Data by tabs and :
		#Before sorting, measure the minimum between the two (because it isn't minor allele, it's reference vs alternate)
		#Sort by max minor allele frequency then sort by unique contig
			#By removeing duplicates > expand selection > only select column A
		#Export by keeping column A and B without headers
		freq_keep.txt
			#1fprad = 1 (by frequency) per radtag
vcftools --vcf SNP.DP3g95p5maf05.HWE.recode.vcf --positions freq_keep.txt --recode --recode-INFO-all --out SNP_HWE_b0406_1fprad 

#Outputs
SNP_HWE_b0406_basic.recode.vcf
SNP_HWE_b0406_1fprad.recode.vcf


#Step #11
	#Haplotype caller from SNP.DP3g95p5maf05.HWE.vcf
	#https://github.com/chollenbeck/rad_haplotyper
	#Move back into alignment folder
	cp SNP_HWE_b0406_basic.recode.vcf ..
	cd ..

#Can't do in slurm cause it will be in a queue forever but there is space for it just running straight
screen
export PERL5LIB=/programs/PERL/lib/perl5
#Perl library didn't work again! Fix
#Can't find my notes from when I did this previously, check email.
#Does it just not work in screen?
perl rad_haplotyper.pl -v SNP_HWE_b0406_basic.recode.vcf -r reference.fasta -x 10 \
-p popmap_all.txt --genepop haps_simref_all0406.gp \
--ima haps_simref_all0406.ima

mv stats.out haps_stats_simsol2sim0604.out


	#Outputs
	mv stats.out haps_stats_simsol2sim.out
	Writing Genepop file: haps_simsol2sim.gp
	Writing IMa file: haps_simsol2sim.ima
	#Not sure what the file names should end with

#Date: Apr 8th
#Filtering for solidissima reference
#The vcf file is HUGE - more than 60 G
#For similis reference, it was 2G


#Start running haplotype caller for similis reference
#For
SNP_HWE_b0406_basic.recode.vcf
#Do in slurm
source /programs/miniconda3/bin/activate rad_haplotyper
rad_haplotyper.pl -v SNP_HWE_b0406_basic.recode.vcf -r reference.fasta -x 30

#!/bin/bash -l

#SBATCH --partition=regular
#SBATCH --job-name=hapssim_sol_th30_mem80
#SBATCH --output=hapssim_0408.out
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --mail-user=hh693@cornell.edu
source /programs/miniconda3/bin/activate rad_haplotyper
rad_haplotyper.pl -v SNP_HWE_b0406_basic.recode.vcf -r reference.fasta -x 30 \
-p popmap_all.txt --genepop haps_simref_all0406.gp \
--ima haps_simref_all0406.ima

mv stats.out haps_stats_simref_all0406.out

sbatch --nodes=1 --ntasks=30 --mem=80000 s_radhaps.sh


#Gene trees
#Mac Application did not work
#Try command line version
#On personal computer? On server?
#Needs beast first





#Beast needs multi sample fasta
gatk FastaAlternateReferenceMaker \
   -R reference.fasta \
   -O output.fasta \
   -V input.vcf \

#This might work
java -jar /programs/bin/GATK/GenomeAnalysisTK.jar -T FastaAlternateReferenceMaker
#Can I set number of threads?
#GATK4 does parallele-ness so try that and do the above if that doesn't work
/programs/gatk4/gatk HaplotypeCaller 
    -R reference.fasta 
    -I sample1.bam 
    -O variants.g.vcf 
    -ERC GVCF
#Eh, not sure about threading that one either

java -jar /programs/bin/GATK/GenomeAnalysisTK.jar -T FastaAlternateReferenceMaker \
-R refernce.fasta \
-O output_multisample.fasta \
-V input.vcf #Do by contig - then I can parallel myself by doing different jobs

#Maybe do with the haplotype vcf or maybe not
#Output list of contigs:
#Can just do in excel I guess 
	#An adult would do it with grep

#1215 filtering for sol and sim
/local/storage/Spisula/GBS/filterDoc_b1215/sim/SNP.simONLY_sf_1012.HWE.recode.vcf
/local/storage/Spisula/GBS/filterDoc_b1215/sol/SNP_HWE_b1215_solonly.vcf
#Confirm the numbers
#Good

#Pseudocode
While read list of contigs [x] {
vcftools --chr x -input i.vcf -output vcf_contig/x.vcf
samtools faidx ref.fa x -out contig_refs/ref_x.fasta
java -jar /programs/bin/GATK/GenomeAnalysisTK.jar -T FastaAlternateReferenceMaker \
-R contig_refs/ref_x.fasta \
-O contig_fasta/output_multisample_x.fasta \
-V vcf_contig/x.vcf
}

while read p; do
echo "echo $p"
echo ""
samtools flagstat "$p" | grep "primary mapped" 
done <contig_list_simonly.txt  > j_vcf2fasta &

dDocent_Contig_270
vcftools --chr dDocent_Contig_270 --vcf SNP_simonly_b1215_basic.vcf --recode --out contig_vcf/dDocent_Contig_270_simonly
samtools faidx reference.fasta dDocent_Contig_270 -o contig_refs/dDocent_Contig_270_ref.fasta


#Get to VCF Options

#1. vcftools - NO
vcftools vcf-consensus file.vcf.gz > out.fa #Doesn't exist in modern version

#2. Santiago - NO
#Here's another option
https://github.com/santiagosnchez/vcf2fasta

bgzip contig_vcf/dDocent_Contig_270_simonly.recode.vcf
tabix contig_vcf/dDocent_Contig_270_simonly.recode.vcf.gz
#However, I may not need to split it up by contig because it will have the GFF file?

#GFF Format
1	.	gene	10	1000	.	+	.	ID=gene1;Name=g1
dDocent_Contig_270	.	gene	10	1000	.	+	.	ID=dDocent_Contig_270;Name=dDocent_Contig_270
nano gffs/dDocent_Contig_270.gff


perl vcf2fasta_santiago.pl -f reference.fasta -v contig_vcf/dDocent_Contig_270_simonly.recode.vcf.gz -g gffs/dDocent_Contig_270.gff -o contig_fasta -e gene

#Example
perl vcf2fasta_santiago.pl -f reference -v test_file_diploid_unphased.vcf.gz -g gff_file.gff -o contig_fasta -e gene
#Also did not work
#something wrong with the gff file - even in the example one

#3. GATK - talk to BioHPC
#Since things take time in order to do it as jobs I have to do them each as separate while loops so it doesn't overlap itself
java -jar /programs/bin/GATK/GenomeAnalysisTK.jar -T FastaAlternateReferenceMaker \
-R contig_refs/dDocent_Contig_270_ref.fasta \
-O contig_fasta/dDocent_Contig_270_simonly.fasta \
-V contig_vcf/dDocent_Contig_270_simonly.vcf
#Nevermind, I do not think this tool does what because it does one sample at a time - but also I guess I might have to, but also that would be insane
#This does not seem to be a file in the jar
##### ERROR MESSAGE: Invalid command line: Malformed walker argument: Could not find walker with name: FastaAlternateReferenceMaker
#Haplotype Caller did work

#4. Farrel - talk to BioHPC or try again
#Try to run this script?
https://dmnfarrell.github.io/bioinformatics/vcf-sites-fasta2
#Python lack of clarity for how to set input files

#5. Bam Methods
#Should I be using filtered data or unfiltered data?
#Either way I can use the list I have of contigs which have good info

#5.a. Straight to fasta
samtools fasta
#just straight up converts it into a fasta file
#How do I make it into consensus to deal with the fact that I have diploid data?
#Have two sequences for each?

#A recent alignment of all of the similis was dDoc_0118
#It does not matter if the solidissima were also aligned at that time cause I just don't use those
#Therefore I could also use dDoc0330 which is already in my workdir

#Does not look like the contigs are labeled correctly
>A00223:599:H7YTHDRXY:1:2107:3224:33411/2
#as opposed to matching the reference
>dDocent_Contig_1
#Can I also call the bam.bai?
#Do I also need to generate a consensus from my bam file because paired reads?
samtools view GA12_001-RG.bam | awk -v OFS='\t' '{print ">"$3"\n"$10}' > GA_001testb.fasta
#That looks like it worked except it has several reads for each contig....
#And therefore they are not nesecarily aligned?
#So I do need consensus? (But the method I saw previously required a filtered vcf first)

#Do I need to take my reference by the fasta contigs which are nice (2k) and use muscle to re-align my raw reads?
#Seems unnessecary,  instead I still may need MUSCLE to align bam consensus sequences

#The consensus stuff is all weird because that would be from all my samples in the vcf but I suppose I could split my vcf by sample instead of chr

#5b. Consensus bam
#Here's something else which includes the use of a vcf
samtools mpileup -uf reference.fa aligment.bam | bcftools view -cg - | vcfutils vcf2fq  
#What is vcfutils vcf2fq?
	#Part of samtools?
	vcfutils.pl vcf2fq
	#does work but needs the "all-site" vcf meaning polymorphic and non-polymoprhic sites
	#I think it also suppose to work with a single sample
samtools mpileup -uf reference.fasta bams/GA12_001-RG.bam | bcftools call -c | vcfutils.pl vcf2fq > GA12_001_cns.fq
#Convert to fasta
samtools mpileup -uf reference.fasta bams/GA12_001-RG.bam | bcftools call -c | vcfutils.pl vcf2fq | cat | paste - - - - | sed 's/^@/>/g'| cut -f1-2 | tr '\t' '\n' > GA12_001_cns2.fasta
#This leaves a LOT of garbage in it - the consensus sequences are very messy (lots of uncertainties) and they have tons of ! and ~

#Maybe a fasta file is not supposed to have lots of samples in it though and you are supposed to align them each from their own?

#Try just doing it with vcf
vcfutils.pl vcf2fq SNP_simonly_b1215_basic.vcf | > sim_cns_test.fq
#Doesn't save to file and pushes out a bunch of errors
#Probably cause I'm missing the non polymorphic sites?

#6. The Actual Answer ??? - NO - The script doesn't work even on it's toy example
#Don't get too excited but this looks like the holy fucking grail
https://github.com/Bahler-Lab/alignment-from-vcf
#Open python 2.x only
#Requires biopython and pysam (probably good on those)
#The script will generate a sequence alignment for multiple individuals using the reference genome and a vcf file. I personally work with S. pombe (which conveniently is haploid) but this script should work for higher ploidy as well. In that case, one sequence per individual and genome copy is generated. This obviously only makes sense if your variants are phased. 
#Usage: The script requires the following 7 arguements (in the given order):
	#path to the reference genome in fasta format
	#path to the vcf. This must be gzipped and indexed. Run 'bgzip -c file.vcf > file.vcf.gz; tabix -p vcf file.vcf.gz' from the samtools/htslib package.
	#name of the contig that contains the region of interest
	#start of the region
	#the start base will be included and this is 1-indexed (i.e. use start=1 to start at the beginning of the contig) end of the region - the end is included in the output
	#ploidy (This script obviously only makes sense if variants are phased. One sequence per genome copy will be generated)
	#path to the output file. This will be in fasta format.
	
python2 align-from-vcf_Bahler.py ./reference.fasta ./SNP_simonly_b1215_basic.vcf dDocent_Contig_270 1 10 2 ./bahtest1.fasta
#Something didn't work even with the toy example
#Try running from command line manually
toy_reference.fa 
toy.vcf  toy.vcf.gz  toy.vcf.gz.tbi #Do I need all of these? Yes
chr1
1
10
2
toy_out.fasta
#Example
#Try earlier python module - not a lot of options other than 2.7
module load python/2.7.15
python2

from Bio import SeqIO
from pysam import VariantFile
from Bio.SeqRecord import SeqRecord
from Bio.Seq import Seq
import sys

helpstr = '''The script requires the following 7 arguements (in the given order):
    path to the reference genome in fasta format
    path to the vcf
    name of the contig that contains the region of interest
    start of the region - the start base will be included and this is 1-indexed (i.e. use start=1 to start at the beginning of the contig)
    end of the region - the end is included in the output
    ploidy (This script obviously only makes sense if variants are phased. One sequence per genome copy will be generated)
    path to the output file. This will be in fasta format.
An 8th arguement can optionally be supplied and should specify the path to a file containing a list of individuals to be included.
If not supplied, all individuals will be used.'''

print 'Alignment-from-vcf, written by Stephan Kamrad (stephan.kamrad@crick.ac.uk)'

if len(sys.argv) not in [8,9]:
    print helpstr
    sys.exit('Invalid arguements supplied')

ref_path = sys.argv[1]
print 'Reading the reference genome: '+ref_path
ref = SeqIO.index(ref_path, "fasta")

vcf_path = sys.argv[2]
print 'Reading the vcf: '+vcf_path
vcffile = VariantFile(vcf_path, 'r')     #Error here, it cannot detect vcf or bcf eventhough it says it will automatically

contig = sys.argv[3]
start = int(sys.argv[4])
end = int(sys.argv[5])
print 'Getting variants in region %s:%i-%u'%(contig, start, end)
variants = list(vcffile.fetch(contig, start-1, end))
ref_seq = ref[contig].seq[start-1:end]

if len(variants) == 0:
    raise Exception('No variants in specified region. Terminating.')

ploidy = int(sys.argv[6])
print 'Ploidy is %i.'%ploidy

if len(sys.argv) == 9:
    samples_path = sys.argv[8]
    with open(samples_path, 'r') as sfile:
        samples = [l.strip() for l in sfile if l.strip()!='']
    print 'Generating alignment for the following individuals: '+str(samples)
else:
    samples = list(variants[0].samples.keys())
    print 'Generating alignment for the all individuals: '+str(samples)

def get_sequence(ref_seq, variants, s, p):
    seq = list(str(ref_seq))
    for v in variants:
        #In order to create a correct alignement, we need to determine the length of the longest allele if the variant is an indel
        max_indel_length = max(map(len,v.alleles))
        
        #Now we determine the allele of the individual at the specified phase
        allele = v.samples[s].alleles[p]
        allele = allele if allele is not None else 'N'

        #Adding the appropriate number of - so alignment stays intact
        if len(allele) != max_indel_length:
            allele += '-'*(max_indel_length-len(allele))
        seq[v.pos-start] =  allele 
    return ''.join(seq)

#Getting sequences for all individuals and haplotypes
records = [SeqRecord(Seq(get_sequence(ref_seq, variants, s, p)), id='%s-%i-%s:%i-%i'%(s, p, contig, start, end)) for p in range(ploidy) for s in samples]

out_path = sys.argv[7]
print 'Writing alignement to: '+out_path
SeqIO.write(records, out_path, "fasta")

print 'Done.'


#Other Ideas
#Can I google vcf 2 fastq instead of fasta??
#Try googling vcf to alignment file or NEXUS file
	#One option to make it into a tabular file without the non-polymorphic sites: https://code.google.com/archive/p/vcf-tab-to-fasta/
		#not ideal
#WORKS START HERE
#Convert to a PHYLIP relatedness matrix which can make trees
		#Or NEXUS
		https://github.com/edgardomortiz/vcf2phylip
		python vcf2phylip.py -i SNP_simonly_b1215_basic.vcf -n --output-prefix mortiztest1
		#Another script yeah
		#So this puts together all of the loci
	
		#So could I do it per contig?
		#Still is going to be missing all the good stuff in the middle
		python vcf2phylip.py -i contig_vcf/dDocent_Contig_270_simonly.recode.vcf.gz -n --output-prefix mortiztestdDoc_270
		#Grep from it any individual who has NN
	
		#Only do for contigs with several SNPs?
	
	#Phylogenetics from SNP data
		https://bmcgenomics.biomedcentral.com/articles/10.1186/1471-2164-15-162
		#They do it by concatenating
	
#So Matt wants to view genetrees
#My best option for that right now is to create a NEXUS file for each "good" contig 
#and then run them through PHYILP or something I can do in command line 
#and then take all of those files and put them into DensiTree

#However, producing those is not as relevant to the rest of my work
#Instead, I need trees to estimate migration
	#Does not have to be gene by gene
	#What sort of data do folks use? 
		#Looks like maybe just SNPS
		https://www.annualreviews.org/doi/full/10.1146/annurev-ecolsys-110617-062431?casa_token=Zft-KJBjo6IAAAAA:jyJ5M1SLIBZ2RD2uLb3IL1mWRm0W5sI3dcDHmZlIqhVzhXb7jHky0jNKFhhmIfau64RmW-N78lah
		#Good review for me
	#Can I exclude non polymorphic sites? My outgroup is sol for sim and sim for sol so I'm not comparing to another alignment
		#A cheating idea I have if I need to have the monomorphic sites is to add the reference sequence on to the end of each... so the ratio of poly to monomorphic is about right
	

#Date: Apr 9th
#Running radhaplotyper on the server for solidissima
#Already did for similis
#Have filtered vcf in the folder with the bam files for each

#Do in slurm
#copy script from sim
#Add
mv stats.out haps_stats_solref_all0408.out
#Run
sbatch --nodes=1 --ntasks=30 --mem=80000 s_radhaps.sh

#Need to do FST from genepop in Arlequin on PC
#Change genepop to have only 3 populations - just A, B, Similis

#From yesterday
/programs/gatk4/gatk FastaAlternateReferenceMaker -h
#Should work to pull it up, so I could try doing that over individuals (or by location/population level vcfs)
	#By Individual
	#By GA/NLI/SCC
#Qi said that including the invariant sites could make it too long to make a tree
	#You can either use the software TASSEL which makes neighbor joining tree from vcf file.
	#Or, to make a maximum likelihood tree, you can follow this procedure to run raxml
		https://rdtarvin.github.io/IBS2019_Genomics-of-Biodiversity/main/2019/08/05/09-raxml-epi.html 
	#For the alignment from vcf script check that my vcf is well formatted
		bcftools view myvcffile.vcf 
		#to verify your vcf file format
		
#Date: Apr 10th
#Something went wrong with haplotyper
#Run outside of slurm
source /programs/miniconda3/bin/activate rad_haplotyper
rad_haplotyper.pl -v SNP_HWE_b0408_solrefall_basic.vcf -r reference.fasta -x 20 \
-p popmap_all.txt --genepop haps_simref_all0408.gp \
--ima haps_solref_all0408.ima &

mv stats.out haps_stats_simsol2sim0408.out

#Running into an issue I had before: phenotypes have different number of loci - how did I fix before?
#I ran a version of it with fewer loci?
#And also it was SNP data instead of haplotype data
#Is it the ? for missing data? Is it number of loci?
	#I tried changing pdg spider to read it as microsats from genepop with arbitrary number
	#Then read as SNP data for arlequin
	
#Effective Population
#From Nick
#Demographic History
	#contemporary effective population sizes based on linkage disequilibrium occurring in the microsatellite data using NeEstimator ver. 2 (Do et al 2014), ignoring alleles with frequency < 0.02 to minimize bias
	#demographic expansion using Tajima’s D, and Fu’s FS using ARLEQUIN 3.5 (. Tajima’s D and Fu’s FS statistics were tested against a neutral model using 1000 simulated samples. )
#NeEstimator ver. 2 (Do et al 2014), ignoring alleles with frequency < 0.02 to minimize bias
#I have version 2.1
	#Uses genpop file

#Which files should I use?
	#Haplotypes? SNPs?
	#Sol only?
	#Sim only?
		#PGDSpider vcf to genepop
	#Do I have to use independent loci only? 1fp? Or LD prune?
		#Most of them use LD to help estimate - even though I can't
		#So therefore I do not need to filter to only independent loci
		#From other reference: (https://cdnsciencepub.com/doi/pdf/10.1139/cjfas-2020-0069?casa_token=gIuvKVKBV_0AAAAA:TAcNh9LvGehKkfEmIjrYKKzMgsfMogpQriW1m6yQc6oNt6Yd9sHydU3sNyvtCsUVmXqNgunqWG-85w)  using the linkage disequilibrium method with a random mating model as implemented in program NeEstimator (version 2.1; Do et al. 2014). Assuming a frequency of 0.05 for rare alleles, we obtained negative NE estimates with infinite confidence intervals (CIs) for some clusters. Negative values are potentially the result of small sample sizes relative to the NE (Do et al. 2014; Jensen et al. 2018; Waples and Do 2010).
			#They have reference
		#From others: We also estimated effective population size using the Linkage Disequilibrium method implemented in NeEstimatorV2.1 (the lowest allele frequency = 0.05) [12].
			#Report Ne and 95% confident intervals
		#(https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/ece3.6093)?
		#linkage disequilibrium and the molecular co-ancestry method of Nomura [55], implemented in NeEstimator version 2.1
#Filtered SNPs (but many per radtag) and haplotypes
	#Both all
	#Each Independently
	#Populations to Try With: 
		#A, B, Sim
		#GA vs North
		#Sim vs 6 Locations for Sol
		#A vs B at Locations
		#A vs B
#Check if there are any other ways to estimate Ne without a reference genome?
	#https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/eva.13128
	#Effective population size was estimated using the linkage disequilibrium (LD) estimator implemented in NeEstimator 2.01 (Do et al., 2014). This method is based on the assumption that LD at independently segregating loci in a finite population is a function of genetic drift and performs particularly well with a large number of loci and where population sizes are expected to be small (Waples & Do, 2010). 
	#NeEstimator was run assuming random mating and using a Pcrit value of 0.075 following guidelines for small sample sizes suggested by Waples and Do (2010).
	
	#https://www.nature.com/articles/hdy201660
	#Fast SIMCOAL2 recommended
		#Asks about approximate sex ratio (not 1:1 in oysters? What about clams)
	#Ne was estimated from the mean amount of LD present in the genome remaining after accounting for sampling-induced LD. We used PLINK ver. 1.07 (Purcell et al., 2007) to calculate Pearson’s squared correlation coefficient (r2) of the genotype vectors for each pair of loci as a measure of LD.
	#Genotypes were coded as the number of non-reference alleles, with possible values in [0, 1, 2]. With genotypes coded in this way, r2 is identical to Burrows’ composite measure of linkage disequilibrium (rΔ) (Weir, 1996; Zaykin, 2004).
		#So SNP data...
		#They have a nice, very long explanation of a way to do it with lots of math

#Haplotypes
../align121_sol/haps_region/haps_sol_region_0314.gen
../align121_sol/haps_region/codes.haps_sol_region_0314.gen
#It would be good for me to also have the haplotype codes for easier reference
#0118 is the sim reference alignment with the solidissima subset
#Where is my haplotype stuff for similis? - the filtering I liked was 1012
	#/local/storage/Spisula/GBS/dDoc_928_b/undedup/codes.haps_simONLY_0126.gen
	#*928_b*
/local/storage/Spisula/GBS/dDoc_928_b/undedup/haps_simONLY_0126.gen
/local/storage/Spisula/GBS/dDoc_928_b/undedup/codes.haps_simONLY_0126.gen 

#Probably not needed but IBD calculations (for Ne):
	#First https://github.com/browning-lab/hap-ibd
	#Then https://faculty.washington.edu/browning/ibdne.html
	
#Date: Apr 22nd

#From ealier
/programs/gatk4/gatk FastaAlternateReferenceMaker -h
#Should work to pull it up, so I could try doing that over individuals (or by location/population level vcfs)
	#By Individual
	#By GA/NLI/SCC
#Qi said that including the invariant sites could make it too long to make a tree
	#You can either use the software TASSEL which makes neighbor joining tree from vcf file.
	#Or, to make a maximum likelihood tree, you can follow this procedure to run raxml
		https://rdtarvin.github.io/IBS2019_Genomics-of-Biodiversity/main/2019/08/05/09-raxml-epi.html 
	#For the alignment from vcf script check that my vcf is well formatted
		bcftools view myvcffile.vcf 
		#to verify your vcf file format

#Steps

#Similis Reference
#SNP data
#Split alignment vcf of all samples to this into A vs B vs Similis by sample name (from popmap)
dDocent/dDoc_0330/sim_ref/SNP_HWE_b0406_basic.recode.vcf

#For loop along list of contigs
#Split reference fasta from dDocent catalogs - separate: align 121 and 1010
#Make fasta reference based on each of the three vcfs A/B/Similis
#Concatenate them
#End for loop

#Make into job list

#Try first with one contig
#Check that this makes them aligned


#Set up file with all the ones I was to for loop over
awk '{print $1}' highFSTradtagpos_bysubsetSNPs.txt > highFSTradtags_bysubsetSNPs.txt
cat highFSTradtags_bysubsetSNPs.txt highFSTradtags_byhaps.txt | awk '{print $1}' | sort | uniq -u > highFSTradtags_0422.txt
	#Did not fully get rid of uniqueness issue - not sure why manual fix
#Set up separate vcfs for each
vcftools --vcf SNP_simrefall_b0406_basic.vcf --keep samples_A.txt --out A --recode
vcftools --vcf SNP_simrefall_b0406_basic.vcf --keep samples_B.txt --out B --recode
vcftools --vcf SNP_simrefall_b0406_basic.vcf --keep samples_Sim.txt --out Sim --recode
#Index vcfs
/programs/gatk4/gatk IndexFeatureFile -I A.recode.vcf
/programs/gatk4/gatk IndexFeatureFile -I B.recode.vcf
/programs/gatk4/gatk IndexFeatureFile -I Sim.recode.vcf

#Split one reference fasta
samtools faidx sim_reference.fasta dDocent_Contig_2084 > contig_fasta/dDocent_Contig_2084.fasta
#Index reference (in multiple ways)
samtools faidx contig_fasta/dDocent_Contig_2084.fasta
/programs/gatk4/gatk CreateSequenceDictionary -R contig_fasta/dDocent_Contig_2084.fasta

#Try to use Reference Maker 3x
/programs/gatk4/gatk FastaAlternateReferenceMaker -O contig_out/dDocent_Contig_2084_A.fasta -R contig_fasta/dDocent_Contig_2084.fasta -V A.recode.vcf
/programs/gatk4/gatk FastaAlternateReferenceMaker -O contig_out/dDocent_Contig_2084_B.fasta -R contig_fasta/dDocent_Contig_2084.fasta -V B.recode.vcf
/programs/gatk4/gatk FastaAlternateReferenceMaker -O contig_out/dDocent_Contig_2084_Sim.fasta -R contig_fasta/dDocent_Contig_2084.fasta -V Sim.recode.vcf

#Add headers
sed -i 's/>1/>A/g' contig_out/dDocent_Contig_2084_A.fasta
sed -i 's/>1/>B/g' contig_out/dDocent_Contig_2084_B.fasta
sed -i 's/>1/>Sim/g' contig_out/dDocent_Contig_2084_Sim.fasta

#Concatentate
cat contig_out/dDocent_Contig_2084_A.fasta contig_out/dDocent_Contig_2084_B.fasta contig_out/dDocent_Contig_2084_Sim.fasta > contig_out2/dDocent_Contig_2084.fasta

#These three are all the same - what the hell??
#Just try across all first and see what I can get

#Rather than run slurm just do:


while read p; do
echo "Working on $p"
samtools faidx sim_reference.fasta $p > contig_fasta/$p.fasta
#Index reference (in multiple ways)
samtools faidx contig_fasta/$p.fasta
/programs/gatk4/gatk CreateSequenceDictionary -R contig_fasta/$p.fasta
#Try to use Reference Maker 3x
/programs/gatk4/gatk FastaAlternateReferenceMaker -O contig_out/$p.A.fasta -R contig_fasta/$p.fasta -V A.recode.vcf
/programs/gatk4/gatk FastaAlternateReferenceMaker -O contig_out/$p.B.fasta -R contig_fasta/$p.fasta -V B.recode.vcf
/programs/gatk4/gatk FastaAlternateReferenceMaker -O contig_out/$p.Sim.fasta -R contig_fasta/$p.fasta -V Sim.recode.vcf
#Add headers
sed -i 's/>1/>A/g' contig_out/$p.A.fasta
sed -i 's/>1/>B/g' contig_out/$p.B.fasta
sed -i 's/>1/>Sim/g' contig_out/$p.Sim.fasta
#Concatentate
cat contig_out/$p.A.fasta contig_out/$p.B.fasta contig_out/$p.Sim.fasta > contig_out2/$p.fasta
done <highFSTradtags_0422.txt

#Reminder that this is only working for the haplotype (non subset data) - I could repeat with the subset data or even better with totally separate solidissima



#A USER ERROR has occurred: Input files reference and features have incompatible contigs: No overlapping contigs found.
#  reference contigs = [dDocent_Contig_2084]
#  features contigs = [dDocent_Contig_4042]
#Maybe also do vcf by contig too even though that seems silly
vcftools --chr dDocent_Contig_2084 --vcf SNP_simrefall_b0406_basic.vcf --keep samples_A.txt --out contig_vcf/dDocent_Contig_2084_A --recode
#Run
/programs/gatk4/gatk FastaAlternateReferenceMaker -O contig_out/dDocent_Contig_2084_A -R contig_fasta/dDocent_Contig_2084.fasta -V contig_vcf/dDocent_Contig_2084_A.recode.vcf
#Same exact error with the contig_4042 and everything...?
#There's a note in the top of the vcf that lists it as contig 4042. Why?
	#Remove from original
#Fixed

#Now
#A USER ERROR has occurred: Input A.recode.vcf must support random access to enable queries by interval. If its a file, please index it using the bundled tool IndexFeatureFile
/programs/gatk4/gatk IndexFeatureFile -h
/programs/gatk4/gatk IndexFeatureFile -I A.recode.vcf




#Okay so the code ran but it is not working because there is no difference in the fasta files
vcftools --vcf SNP_simrefall_b0406_basic.vcf --chr dDocent_Contig_2323 --recode --out contig_vcf/dDocent_Contig_2323
NTGCAGTTTTTTCTCCGTTAAGGTAATTTTCCATATCTTTAAATGTTCCATTATGCAGAA
CGCGTCTACACTAATGCGACAGAGATTTTAGATTTTTCAACCAAGGAATCACAACAGTGC
GCATGACAAACAAGCACCCGACCTGTAGAATGNNNNNNNNNNACCAAGAAAATCTAATAT
TCTAACCAAGTTCTTTAACTTTATTAAAATGGCTTTAATGTTGTCTATACAAAACCGTGC
GACAGAAAATGTTTCTACAGCGGTCCGATTCAAAAACTCAATGTGAGGAGTTTCGGAGTT
TTTGGAAGGACCGN
#Is the reference already showing the alt allele?
#Otherwise it is clear that solidissima are all 1/1 and sim is all 0/0

#Try another
>dDocent_Contig_1159
NCAGTGCAGGTCATGGCCATCCCTATCTTGGACCGTGGGTGCAAACGTTCTCTTGTTACT
TTTCACCAACGGACCAATCGACGTACATGAATAAAGCAATATAGTTGACCATTAGTCTGT
GTAGAGATTTCATTCATTTGTTAAAGGTTGCTNNNNNNNNNNTGATTCGAGGGTTGGTGT
ATTGAGGTCCTACCATTATTGATAATGCCGCTGTTTCCAGTTGTAATAGTCAGGTCATAA
AGTGAAGCTCGCCATGCTCCTCCACATATTTGGGCTGTGTTGTTTGCACAATTATGGTTG
CACTCTGATTCCGN

vcftools --vcf SNP_simrefall_b0406_basic.vcf --chr dDocent_Contig_1159 --recode --out contig_vcf/dDocent_Contig_1159
#This one has 10 positions
#First one is C, alt T which looks good in the reference - does not have high species differentiation low across population FST (maybe highish within)
#Next is T alt C which also looks good - has some species difference but not diagnostic
#Some of them are "phased" - urg
#The second to last  one which I suspect has the highest FST says it is supposed to be G alt T at base 256 but that DOESN'T WORK
#Maybe it doesn't count the NNNNNNs in the middle? Why wouldn't it?
#Nope, cause base 79 is also wrong
#This doesn't make any sense

#Does it have something to do with the fai file used by dDocent?
#That now the index is different or something?

#How to read a fai
#       NAME        Name of this reference sequence
#       LENGTH      Total length of this reference sequence, in bases
#       OFFSET      Offset within the FASTA file of this sequence's first base
#       LINEBASES   The number of bases on each line
#       LINEWIDTH   The number of bytes in each line, including the newline

#But the offset just ticks up throughout the reference, but why is the first one 18?
dDocent_Contig_1159	314	94531	314	315

#Try just doing a reference of each of the three from the full reference and then comparing contig by contig
#I did do samtools faidx sim_reference.fasta in this folder - if it still looks wrong trying pulling the fai from the original dDocent run
	#Actually just do that cause it is easy
	#Next question, did it keep non-variant sites when making A.recode etc? I assume it does

/programs/gatk4/gatk CreateSequenceDictionary -R sim_reference.fasta

/programs/gatk4/gatk FastaAlternateReferenceMaker -O speciesRef.A.fasta -R sim_reference.fasta -V A.recode.vcf 
/programs/gatk4/gatk FastaAlternateReferenceMaker -O speciesRef.B.fasta -R sim_reference.fasta -V B.recode.vcf
/programs/gatk4/gatk FastaAlternateReferenceMaker -O speciesRef.Sim.fasta -R sim_reference.fasta -V Sim.recode.vcf
#Still resulted in no difference
#Confirm with 
diff speciesRef.A.fasta speciesRef.Sim.fasta
#Yep no difference


#Make a test reference and vcf
samtools faidx test_ref.fasta
/programs/gatk4/gatk CreateSequenceDictionary -R test_ref.fasta

NCAGTGCAGGTCATGGCCATCCCTATCTTGGATCGTGGGCGCAAACGTCCTCTAGGTACTTTTCACCAACGGACCAATGGACGTACATGAATAAAGCAATATAGTTGACCATTAGTCTGTGTAGAGATTTCATTCATTTGTTAAAGGTTGCTNNNNNNNNNNTGATTCGAGGGGTGGTGTATTGAGGTCCTACCATTATTGATAATGCCGCTGTTTCCAGTAGTAATAGTCAGGTCATAAAGTGAAGCTCGCCATTCTCCTCCACATATTTGGGCTGTGTTGTCTGCACAATTATGGTTGCACTCTGATTCCGN



/programs/gatk4/gatk FastaAlternateReferenceMaker -O testout.fasta -R test_ref.fasta -V test.vcf
#Not enough collumn malformed
#Disagree - I have the exact number of collumns for my data
	#sapces instead of tabs
/programs/gatk4/gatk IndexFeatureFile -I test.vcf
#It worked! It switched the things to be different from the reference at those positions

#So now just figure out what's wrong with the other one
#For one, it does seem to have some difficulty perhaps with the ># confusing what contig it is supposed to be
#Confirm no differences between reference and solidissima

diff sim_reference.fasta speciesRef.A.fasta
#Oh! There are many but it may just be whether there are enters within the line or not...
diff speciesRef.Sim.fasta speciesRef.A.fasta
#Did the test I had above replace the locations I thought it would?
#AHHHHH NO IT FUCKING DIDNT
#IT REPLACED location 3 fine but then it replaced a C with G but G was the alt allele so why was it not in the spot I thought it would be?
#Okay, thats because the line breaks count when I'm trying to count bases but is not an issue for the actual code so then the only problem is why are there no dang differences?

#Okay so now compare single contigs directly

#I could do something stupid
sed -e ':a' -e 'N;$!ba' -e 's/\nA/A/g' testspeciesoneline.B.fasta |sed -e ':a' -e 'N;$!ba' -e 's/\nG/G/g'|sed -e ':a' -e 'N;$!ba' -e 's/\nC/C/g'|sed -e ':a' -e 'N;$!ba' -e 's/\nT/T/g' > tester.B.fasta
diff sim_reference.fasta tester.B.fasta
#Still run into issues because contig naming is different
#It does flag some of them as different!
sed -e ':a' -e 'N;$!ba' -e 's/\nA/A/g' speciesRef.A.fasta | sed -e ':a' -e 'N;$!ba' -e 's/\nG/G/g'|sed -e ':a' -e 'N;$!ba' -e 's/\nC/C/g'|sed -e ':a' -e 'N;$!ba' -e 's/\nT/T/g' > tester.A.fasta
sed -e ':a' -e 'N;$!ba' -e 's/\nA/A/g' speciesRef.Sim.fasta | sed -e ':a' -e 'N;$!ba' -e 's/\nG/G/g'|sed -e ':a' -e 'N;$!ba' -e 's/\nC/C/g'|sed -e ':a' -e 'N;$!ba' -e 's/\nT/T/g' > tester.Sim.fasta
diff sim_reference.fasta tester.Sim.fasta
#Unfortunately it flags the same ones as being different when comparing to similis
dDocent_Contig_1035
dDocent_Contig_1159
dDocent_Contig_1161
dDocent_Contig_1181
dDocent_Contig_1183
dDocent_Contig_1186
dDocent_Contig_1218
dDocent_Contig_1235
dDocent_Contig_1268
dDocent_Contig_1296
dDocent_Contig_1312
dDocent_Contig_1328
dDocent_Contig_1345
dDocent_Contig_1351
dDocent_Contig_1401
dDocent_Contig_1470
dDocent_Contig_1611
dDocent_Contig_1960
dDocent_Contig_2068
dDocent_Contig_2084
dDocent_Contig_2323
dDocent_Contig_2325
dDocent_Contig_2527
dDocent_Contig_2940
dDocent_Contig_2980
dDocent_Contig_3247
dDocent_Contig_3572
dDocent_Contig_3906
#There is a difference! Two alleles, not sure if it's 100% right but it seems to be doing something
NGAATCGATGCAGAACATAACAGAACAGAACAGGACCATAAATGTTAAATACCAGTTGTAACATCTACCGAACAACTCCATGCACTTGTTTTGTTATTTTAATTTATAGGTGGATTTTACAGTAAGTTTGTAATATAGCAGAATCAGTGTTTNNNNNNNNNNTGATGTTTGTGGATATGATGTTGAAACTTTATTATTGTTAAGGGAATTCCTGCTCAATTTTCTAATTTAGTAATTTGTCAAGAAAACACACTTGGTTGCAACTGAGTGTTGTTTATAGTTTCATTTAATTGCCATGTGGAATATTTTTCCGN
NGAATCGATGCAGAACATAACAGAACAGAACAGGACCATAAATGTTAAATACCAGTTGTAACATCTACCGAACAACTCCATGCACTTGTTTTGTTATTTTAATTTATAGGTGGATTTTACAGTAAGTTTGTAATATAGCAGAATCAGTGTTTNNNNNNNNNNTGATGTTTGTGAATGTGATGTTGAAACTTTATTATTGTTAAGGGAATTCCTGCTCAATTTTCTAATTTAGTAATTTGTCAAGAAAACACACTTGGTTGCAACTGAGTGTTGTTTATAGTTTCATTTAATTGCCATGTGGAATATTTTTCCGN

dDocent_Contig_3928
NATGCAGTGAAGATAACCAGTGAATATATGAGATTGTATTTCACTGATAGAACTCAGTATCTCACCAGTTAGCATTAAACAAATCTGATATCTAAGTGCTTATATGCTGTATATCATTCGGGCCAAAAATGAAATAATTGTATATTGCTCTTNNNNNNNNNNATTGCTCTTTACATTGTAAACCTGTGTGTTACATATGCCCTATTTGTTTGCATTTCTATGATAGCTGCGGAAGACAGTACACAAGTGACGATTAAATATGCCTCCAACAGTGGGGTCAGTGTCAGGTACGATGGAACAACATACGGACCCGN
NATGCAGTGAAGATAACCAGTGAATATATGAGATTGTATTTCACTGATAGAACTCAGTATCTCACCAGTTAGCATTAAACAAATCTGATATCTAAGTGCTTATATGCTGTATATCATTCGGGCCAAAAATGAAATAATTGTATATTGCTCTTNNNNNNNNNNATTGCTCTTTACATTGTAAACCTGTGTGTTACGTATGCCCTATTTGTTTGCATTTCTATGATAGCTGCGGAAGACAGTACACAAGTGACAATTAAATATGCCTCCAACAGTGGGGTCAGTGTCAGATACAATGGAACAACATACGGACCCGN

dDocent_Contig_936
dDocent_Contig_982
#So are they all different from the reference in the same way as though I had not subdivided their vcfs?
#Or representing even a single alternative allele?

#So then do I need a consensus vcf? And or is the allele change when comparing similis to the reference what it is supposed to be for solidissima?
cat reference.fa | bcftools consensus calls.vcf.gz > consensus.fa
#Try a totally different tool
bgzip -c A.recode.vcf > A.recode.vcf.gz
tabix -p vcf A.recode.vcf.gz
cat sim_reference.fasta | bcftools consensus A.recode.vcf.gz > A.consensus.fasta

bgzip -c B.recode.vcf > B.recode.vcf.gz
tabix -p vcf B.recode.vcf.gz
cat sim_reference.fasta | bcftools consensus B.recode.vcf.gz > B.consensus.fasta

bgzip -c Sim.recode.vcf > Sim.recode.vcf.gz
tabix -p vcf Sim.recode.vcf.gz
cat sim_reference.fasta | bcftools consensus Sim.recode.vcf.gz > Sim.consensus.fasta

#No differences!!!!
#Seriously???
>Sim dDocent_Contig_1161
NACAGATGCAGGCTTAGGATTGCTACGTACGTTCTCCAAAGGTGTGATTTCGTGACCTTTGGTCGCTTTGAATGACATGTGACATTTGGCGCATTCTTCACACATTGCTTCGTTACATTGCTGACACCACGAACGCGCAATCTTTGATTCTTNNNNNNNNNNACAGGAACAATATCGATCAGTGTTCCAACTAGATGATTAACTGGAAAATGCTCCGCCCACTGAGCCACGGGAAGCGCAGGTTTTGGTGAAAAGGTCACGAGCTTGCACGTGGGGCACGGAATTCCGTCTTTTGTCGCGTTCTTCTTTTCCGN
>A dDocent_Contig_1161
NACAGATGCAGGCTTAGGATTGCTACGTACGTTCTCCAAAGGTGTGATTTCGTGACCTTTGGTCGCTTTGAATGACATGTGACATTTGGCGCATTCTTCACACATTGCTTCGTTACATTGCTGACACCACGAACGCGCAATCTTTGATTCTTNNNNNNNNNNACAGGAACAATATCGATCAGTGTTCCAACTAGATGATTAACTGGAAAATGCTCCGCCCACTGAGCCACGGGAAGCGCAGGTTTTGGTGAAAAGGTCACGAGCTTGCACGTGGGGCACGGAATTCCGTCTTTTGTCGCGTTCTTCTTTTCCGN
>B dDocent_Contig_1161
NACAGATGCAGGCTTAGGATTGCTACGTACGTTCTCCAAAGGTGTGATTTCGTGACCTTTGGTCGCTTTGAATGACATGTGACATTTGGCGCATTCTTCACACATTGCTTCGTTACATTGCTGACACCACGAACGCGCAATCTTTGATTCTTNNNNNNNNNNACAGGAACAATATCGATCAGTGTTCCAACTAGATGATTAACTGGAAAATGCTCCGCCCACTGAGCCACGGGAAGCGCAGGTTTTGGTGAAAAGGTCACGAGCTTGCACGTGGGGCACGGAATTCCGTCTTTTGTCGCGTTCTTCTTTTCCGN
>Ref dDocent_Contig_1161
NACAGATGCAGGCTTAGGATTGCTACGTACGTTCTCCAAAGGTGTGATTTCGTGACCTTTGGTCGCTTTGAATGACATGTGACATTTGGCGCATTCTTCACACATTGCCTCGTTACATTGCTGACACCACGAACGCGCAATCTTTGATTCTTNNNNNNNNNNACAGGAACAATATCGATCAGTGTTCCAACTAGATGATTAACTGGAAAATGCTCCGCCCACTGAGCCACGGGAAGCGCAGGTTTTGGTGAAAAGGTCACGAGCTTGCACGTGGGGCACGGAATTCCGTCTTTTGTCGCGTTCTTCTTTTCCGN
#There is a one nucleotide change from the reference a C to a T in the others but why all three the same?

#This just applies variants if they exist at all?
#If more more advanced logic is required, the VCF has to be preprocessed with other tools, such as the +setGT plugin (see an example here).
#Doesn't work

#Could try to filter vcf to sites where alt allele is >0.5
bcftools plugins can be used to get the AF tag and value to your vcf file
export BCFTOOLS_PLUGINS="~/bcftools/bcftools-1.3.1/plugins/"
bcftools plugin fill-tags -Oz -o A.recode.vcf -- -t AF > aftestA.txt

vcftools --vcf A.recode.vcf --freq --out aftestA
vcftools --vcf B.recode.vcf --freq --out aftestB
vcftools --vcf Sim.recode.vcf --freq --out aftestSim

A
dDocent_Contig_1181     181     2       94      G:0     A:1
B
dDocent_Contig_1181     181     2       520     G:0.00576923    A:0.994231
Sim
dDocent_Contig_1181     181     2       250     G:0.964 A:0.036
#Nice
#So then I could do a filter of 0.5 and a filter of 0.9 or more
#Unfortunately it's not going to get me what I wanted where it shows variant alleles when those are possible.
#Instead, since it takes very little time to do an individual sequence for a whole reference I could do it for all 500 individuals and then make consensus sequences by population but that's a LOT of files
	#Would be better - could do later
	#Could do a comparison of AF > 0.5 and AF > 0.1 for possibly common trouble spots
		#Otherwise at some point it just has to be a consensus sequence

#Date: Apr 23rd
#Okay so within each species vcf output allele freq
	#Sed : with /t to save time
	sed 's/:/\t/g' aftestA.frq > afrq_A_simSNP.txt
	sed 's/:/\t/g' aftestB.frq > afrq_B_simSNP.txt
	sed 's/:/\t/g' aftestSim.frq > afrq_Sim_simSNP.txt
#Download
#Open in excel
	#Sort by alt allele
	#Paste just the chr and pos into a new file
#vcftools only those locations in the vcf

vcftools --vcf A.recode.vcf --positions frqpos_A_05_simSNP.txt --recode --out vcf_byAF/A_05
vcftools --vcf A.recode.vcf --positions frqpos_A_09_simSNP.txt --recode --out vcf_byAF/A_09
vcftools --vcf B.recode.vcf --positions frqpos_B_05_simSNP.txt --recode --out vcf_byAF/B_05
vcftools --vcf B.recode.vcf --positions frqpos_B_09_simSNP.txt --recode --out vcf_byAF/B_09
vcftools --vcf Sim.recode.vcf --positions frqpos_Sim_05_simSNP.txt --recode --out vcf_byAF/Sim_05
#There are no Sim sites > 0.9 (as it probably should be because it should match the reference allele)

#Now make references from them
#Then concat
#Set up vcf reference
bgzip -k vcf_byAF/*.recode.vcf
#Do over all of them (-k is to keep the original but using the * interrupts after one)
ls vcf_byAF/*recode.vcf | xargs -n1 bgzip -k

tabix -p vcf vcf_byAF/*.recode.vcf.gz
#This one also doesn't work over all at once...
ls vcf_byAF/*recode.vcf.gz | xargs -n1 tabix -p vcf
#Love it, using less to pipe with xargs is wonderful

cat sim_reference.fasta | bcftools consensus vcf_byAF/A_05.recode.vcf.gz > full_byAF/con_A_05.fasta
cat sim_reference.fasta | bcftools consensus vcf_byAF/A_09.recode.vcf.gz > full_byAF/con_A_09.fasta
cat sim_reference.fasta | bcftools consensus vcf_byAF/B_05.recode.vcf.gz > full_byAF/con_B_05.fasta
cat sim_reference.fasta | bcftools consensus vcf_byAF/B_09.recode.vcf.gz > full_byAF/con_B_09.fasta
cat sim_reference.fasta | bcftools consensus vcf_byAF/Sim_05.recode.vcf.gz > full_byAF/con_Sim_05.fasta

#Woohoo there are differences between the two

#Save a similis reference in the same format 
cat sim_reference.fasta | bcftools consensus > contig_byAF/con_ref.fasta
#Eh nah, remove the line breaks from the other ones
sed -e ':a' -e 'N;$!ba' -e 's/\nA/A/g' full_byAF/con_A_05.fasta | sed -e ':a' -e 'N;$!ba' -e 's/\nG/G/g'|sed -e ':a' -e 'N;$!ba' -e 's/\nC/C/g'|sed -e ':a' -e 'N;$!ba' -e 's/\nT/T/g' > full_byAF/con_A_05_1l.fasta
sed -e ':a' -e 'N;$!ba' -e 's/\nA/A/g' full_byAF/con_A_09.fasta | sed -e ':a' -e 'N;$!ba' -e 's/\nG/G/g'|sed -e ':a' -e 'N;$!ba' -e 's/\nC/C/g'|sed -e ':a' -e 'N;$!ba' -e 's/\nT/T/g' > full_byAF/con_A_09_1l.fasta
sed -e ':a' -e 'N;$!ba' -e 's/\nA/A/g' full_byAF/con_B_05.fasta | sed -e ':a' -e 'N;$!ba' -e 's/\nG/G/g'|sed -e ':a' -e 'N;$!ba' -e 's/\nC/C/g'|sed -e ':a' -e 'N;$!ba' -e 's/\nT/T/g' > full_byAF/con_B_05_1l.fasta
sed -e ':a' -e 'N;$!ba' -e 's/\nA/A/g' full_byAF/con_B_09.fasta | sed -e ':a' -e 'N;$!ba' -e 's/\nG/G/g'|sed -e ':a' -e 'N;$!ba' -e 's/\nC/C/g'|sed -e ':a' -e 'N;$!ba' -e 's/\nT/T/g' > full_byAF/con_B_09_1l.fasta
sed -e ':a' -e 'N;$!ba' -e 's/\nA/A/g' full_byAF/con_Sim_05.fasta | sed -e ':a' -e 'N;$!ba' -e 's/\nG/G/g'|sed -e ':a' -e 'N;$!ba' -e 's/\nC/C/g'|sed -e ':a' -e 'N;$!ba' -e 's/\nT/T/g' > full_byAF/con_Sim_05_1l.fasta
cp sim_reference.fasta full_byAF/con_Sim_ref_1l.fasta

#Now while loop over the contigs
while read p; do
echo "Working on $p"
samtools faidx full_byAF/con_Sim_ref_1l.fasta $p > contig_byAF/$p.simref.fasta
samtools faidx full_byAF/con_A_05_1l.fasta $p > contig_byAF/$p.A5.fasta
samtools faidx full_byAF/con_A_09_1l.fasta $p > contig_byAF/$p.A9.fasta
samtools faidx full_byAF/con_B_05_1l.fasta $p > contig_byAF/$p.B5.fasta
samtools faidx full_byAF/con_B_09_1l.fasta $p > contig_byAF/$p.B9.fasta
samtools faidx full_byAF/con_Sim_05_1l.fasta $p > contig_byAF/$p.Sim5.fasta
#add headers and concatenate
sed -i 's/>d/>A d/g' contig_byAF/$p.A5.fasta
sed -i 's/>d/>B d/g' contig_byAF/$p.B5.fasta
sed -i 's/>d/>A_90p d/g' contig_byAF/$p.A9.fasta
sed -i 's/>d/>B_90p d/g' contig_byAF/$p.B9.fasta
sed -i 's/>d/>Similis d/g' contig_byAF/$p.Sim5.fasta
sed -i 's/>d/>Similis_Refernce d/g' contig_byAF/$p.simref.fasta
cat contig_byAF/$p.A5.fasta contig_byAF/$p.B5.fasta contig_byAF/$p.Sim5.fasta contig_byAF/$p.simref.fasta contig_byAF/$p.A9.fasta contig_byAF/$p.B9.fasta > contig_byAF2/$p.fasta
done <highFSTradtags_0422.txt

#1159 has interesting SNPs but not a high FST haplotype
#2084 has high haplotype but only one SNP difference - sim

#Align them here
https://www.ebi.ac.uk/Tools/msa/mview/

#Date: Apr 23rd
#More data sets for high FST testing
#Similis SNPs (full data)
#Sol SNPs and Haplotypes

#Output list of contigs and positions in order to align with the locus thing from arlequin
awk '{print $1, $2}' SNP_solrefall_b0408_basic.vcf | grep -v "##" > ../solrefSNP_0408_loci.txt
#But it did make there be only a space instead of a tab between them, probably in how I set up the {}

#Try re-running treemix with the full data set?
#Did I previously run it with haplotypes or SNPs?
#Used ANGSD to call genotypes
/programs/angsd0.937/angsd/angsd -doGeno 2 -P for number of thread
#What input files does it take?
#bam files instead of vcf -> All it is doing it creating the vcf to avoid freebayes I think - don't need this


#Date: Apr 29th
#Full multisample fasta
#I need to refilter the fasta to not worry as much about minor allele freq?
#

#Not at all filtered version
#5b. Consensus bam
#Here's something else which includes the use of a vcf
samtools mpileup -uf reference.fa aligment.bam | bcftools view -cg - | vcfutils vcf2fq  
#What is vcfutils vcf2fq?
	#Part of samtools?
	vcfutils.pl vcf2fq
	#does work but needs the "all-site" vcf meaning polymorphic and non-polymoprhic sites
	#I think it also suppose to work with a single sample
samtools mpileup -uf reference.fasta bams/GA12_001-RG.bam | bcftools call -c | vcfutils.pl vcf2fq > GA12_001_cns.fq
#Convert to fasta
samtools mpileup -uf reference.fasta bams/GA12_001-RG.bam | bcftools call -c | vcfutils.pl vcf2fq | cat | paste - - - - | sed 's/^@/>/g'| cut -f1-2 | tr '\t' '\n' > GA12_001_cns2.fasta
#This leaves a LOT of garbage in it - the consensus sequences are very messy (lots of uncertainties) and they have tons of ! and ~

samtools mpileup -uf reference.fasta bams/GA12_001-RG.bam | bcftools call -c | vcfutils.pl vcf2fq | cat | paste - - - - | sed 's/^@/>/g'| cut -f1-2 | tr '\t' '\n' > GA12_001_cns2.fasta


samtools mpileup -uf ../dDocent/dDoc_0330/sim_ref/reference.fasta ../dDocent/dDoc_0330/sim_ref/GA12_001-RG.bam | bcftools call -c | vcfutils.pl vcf2fq | cat | paste - - - - | sed 's/^@/>/g'| cut -f1-2 | tr '\t' '\n' > bysample/GA12_001_cns2.fasta

samtools mpileup -uf ../dDocent/dDoc_0330/sim_ref/reference.fasta ../dDocent/dDoc_0330/sim_ref/GA12_001-RG.bam | bcftools call -c | vcfutils.pl vcf2fq | cat | paste - - - - | sed 's/^@/>/g'| cut -f1-2 | tr '\t' '\n' > bysample/GA12_001_c2.fasta
samtools mpileup -uf ../dDocent/dDoc_0330/sim_ref/reference.fasta ../dDocent/dDoc_0330/sim_ref/GA12_001-RG.bam | bcftools call -c | vcfutils.pl vcf2fq > GA12_001.fq
cat GA12_001.fq | paste - - - - | sed 's/^@/>/g'| cut -f1-2 | tr '\t' '\n' > GA12_001t.fasta



while read p; do
echo "echo "Working on $p""
echo "samtools mpileup -uf ../dDocent/dDoc_0330/sim_ref/reference.fasta ../dDocent/dDoc_0330/sim_ref/"$p" | bcftools call -c | vcfutils.pl vcf2fq | cat | paste - - - - | sed 's/^@/>/g'| cut -f1-2 | tr '\t' '\n' > bysample/"$p"cns2.fasta"
done <highFSTradtags_0422.txt > j_mpilesample.txt


vcfutils.pl vcf2fq | cat | paste - - - - | sed 's/^@/>/g'| cut -f1-2 | tr '\t' '\n' > bysample/"$p"cns2.fasta
#Just try vcfutils on a vcf?

vcfutils.pl vcf2fq simrefSNPs/SNP_simrefall_b0406_basic.vcf > temp #nope
vcf2fq 
#OH, does fq have quality but fasta does not so it I convert from that to fasta?
#Yeah, but those last bits after the cat are supposed to remove it
sed -n '1~4s/^@/>/p;2~4p' GA12_001.fq > GA12_001t2.fasta
cat GA12_001.fq | awk '{if(NR%4==1) {printf(">%s\n",substr($0,2));} else if(NR%4==2) print;}' > GA12_001t3.fa
#this just doesn't work but it also seems like even if it did the Ns in the seuqence would be in a weird spot....?


#Date: Apr 29th

#FINE! I have to do it the even stupider way
bcftools query -l ../simrefSNPs/SNP_simrefall_b0406_basic.vcf > samplelist.txt

while read p; do
echo $p
vcftools --vcf ../simrefSNPs/SNP_simrefall_b0406_basic.vcf --indv $p --out $p.sim --recode
done <samplelist.txt

while read p; do
echo $p
mv $p.sim.recode.vcf $p.recode.vcf
bgzip -c $p.recode.vcf > $p.recode.vcf.gz
tabix -p vcf $p.recode.vcf.gz
cat ../simrefSNPs/sim_reference.fasta | bcftools consensus $p.recode.vcf.gz > $p.consensus.fasta
done <samplelist.txt


#Can I use this one to just pick one at a time rather than making their own files
bcftools consensus --sample
#Yup, I think this would have worked.

#So now I have to do the double loop
#So I need the list of loci of interest for both sim SNPs and sim haps
#Convert SNPs to haplotypes with awk print 1
sort simhaps_highFSTloci.txt simSNP_highFSTloci.txt | awk '{print $1}'| uniq > sim_highFST.txt


#Confirm there are differences
diff GA12_002.consensus.fasta PT_018.consensus.fasta
#FUCK
#So it is just taking the alt allele by default

bgzip -c ../simrefSNPs/SNP_simrefall_b0406_basic.vcf> ../simrefSNPs/SNP_simrefall_b0406_basic.vcf.gz
tabix -p vcf ../simrefSNPs/SNP_simrefall_b0406_basic.vcf.gz
while read p; do
echo $p
bcftools consensus ../simrefSNPs/SNP_simrefall_b0406_basic.vcf.gz --sample $p > $p.indv.fasta
done <samplelist.txt

bcftools consensus ../simrefSNPs/SNP_simrefall_b0406_basic.vcf.gz --sample 413_001 > 413_001.indv.fasta
#Examples:
   # Get the consensus for one region. The fasta header lines are then expected
   # in the form ">chr:from-to".
#   samtools faidx ref.fa 8:11870-11890 | bcftools consensus in.vcf.gz > out.fa

samtools faidx ../simrefSNPs/sim_reference.fasta | bcftools consensus ../simrefSNPs/SNP_simrefall_b0406_basic.vcf.gz --sample 413_001 > 413_001.indv.fasta
#Applied 0 variants
cat ../simrefSNPs/sim_reference.fasta | bcftools consensus ../simrefSNPs/SNP_simrefall_b0406_basic.vcf.gz --sample 413_001 > 413_001.indv.fasta
#Applied 632 variants
#That is NOT a lot - there's only 2992 sites I guess?
samtools faidx ../simrefSNPs/sim_reference.fasta dDocent_Contig_1312 | bcftools consensus ../simrefSNPs/SNP_simrefall_b0406_basic.vcf.gz --sample 413_001 > 413_001_c3.indv.fasta
#Applied 1 variants
#A'ight


while read p; do
echo "Working on $p"
while read q; do
samtools faidx ../simrefSNPs/sim_reference.fasta $p | bcftools consensus ../simrefSNPs/SNP_simrefall_b0406_basic.vcf.gz --sample $q > bycontig/$q.$p.fasta
#add header
sed -i "s/>d/>$q d/g" bycontig/$q.$p.fasta #using double quotes for sed takes it less literally than single quotes?
done <samplelist.txt
cat bycontig/*.$p.fasta > finishedcontig/$p.all.fasta
echo "Done with $p"
done <sim_highFST.txt &> log_0429.log



#For solidissima reference
bgzip -c ../solrefSNPs/SNP_solrefall_b0408_basic.vcf> ../solrefSNPs/SNP_solrefall_b0408_basic.vcf.gz
tabix -p vcf ../solrefSNPs/SNP_solrefall_b0408_basic.vcf.gz
bcftools query -l ../solrefSNPs/SNP_solrefall_b0408_basic.vcf > samplelist.txt
sort solhaps_highFSTloci.txt solSNP_highFSTloci.txt | awk '{print $1}'| uniq > sol_highFST.txt


while read p; do
echo "Working on $p"
while read q; do
samtools faidx ../solrefSNPs/sol_reference.fasta $p | bcftools consensus ../solrefSNPs/SNP_solrefall_b0408_basic.vcf.gz --sample $q > bycontig/$q.$p.fasta
#add header - slightly different for sol
sed -i "s/>d/>$q solref_d/g" bycontig/$q.$p.fasta #using double quotes for sed takes it less literally than single quotes?
done <samplelist.txt
cat bycontig/*.$p.fasta > finishedcontig/$p.all.fasta
echo "Done with $p"
done <sol_highFST2.txt &> log_0429_sol2.log

#SsLI
#My new basics have it
SNP_solrefall_0528_basic.vcf
bgzip
#index
#Except the sim ref all doesn't have it...
while read p; do
echo "Working on $p"
while read q; do
samtools faidx ../solrefSNPs/sol_reference.fasta $p | bcftools consensus ../solrefSNPs/SNP_solrefall_0528_basic.vcf.gz --sample $q > bycontig/$q.$p.fasta
#add header - slightly different for sol
sed -i "s/>d/>$q solref_d/g" bycontig/$q.$p.fasta #using double quotes for sed takes it less literally than single quotes?
done <samplist.SsLI.txt
cat bycontig/*.$p.fasta > finishedcontig/$p.all.fasta
echo "Done with $p"
done <sol_primer_highFST.txt &> log_0920_sol2.log

dDocent_Contig_3976
dDocent_Contig_2651
dDocent_Contig_2788

#Now I need to find a way to do it with similis primer pairs (especially because they are the majority)


while read p; do
echo "Working on $p"
while read q; do
samtools faidx ../solrefSNPs/sol_reference.fasta $p | bcftools consensus ../solrefSNPs/SNP_solrefall_b0408_basic.vcf.gz --sample $q > bycontig/$q.$p.fasta
#add header - slightly different for sol
sed -i "s/>d/>$q solref_d/g" bycontig/$q.$p.fasta #using double quotes for sed takes it less literally than single quotes?
done <samplelist.txt
cat bycontig/*.$p.fasta > finishedcontig/$p.all.fasta
echo "Done with $p"
done <sol_highFST2.txt &> log_0429_sol2.log

sol_highFST2.txt
dDocent_Contig_17072
sed -e '1,200d' < sol_highFST.txt > sol_highFST2.txt

#!/bin/bash -l

#SBATCH --partition=regular
#SBATCH --job-name=temp
#SBATCH --output=temp.out
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --mail-user=hh693@cornell.edu

sbatch --nodes=1 --ntasks=1 --mem=10000 -o outextra_sol.out -J FST_th1_10MB extra_sol_highFST.sh


#Test if there are any overlapped names in the sol vs sim ref contigs
sort bysample_simref/sim_highFST.txt bysample_solref/sol_highFST.txt | uniq | wc -l
cat bysample_simref/sim_highFST.txt bysample_solref/sol_highFST.txt | wc -l
#Yes, there are several with the same name
cp bysample_simref/finishedcontig/* multisampleFastas
rename .all. .simref. multisampleFastas/*all.fasta
cp bysample_solref/finishedcontig/* multisampleFastas
rename .all. .solref. multisampleFastas/*all.fasta

cp multisampleFastas/* /local/storage/Spisula/GBS/highFST/multisampleFastas

#Date: Apr 29th
#Try again with FEEMS
source /programs/miniconda3/bin/activate feems
jupyter notebook --ip=0.0.0.0 --port=8016 --no-browser
#Thanks Qi! For helping me figure out what's what

#Convert file to PLINK format
To use version 1.07, plink is in your path. You can directly type the command "plink".
#I think I can start from a vcf / pop map
start_HERE/SNP_simonly_b1215_1fp.vcf

#Or plink2
/programs/plink2_linux_avx2_20220129/plink2

plink2 --vcf example.vcf.gz --make-bed --out ex

/programs/plink2_linux_avx2_20220129/plink2 --vcf start_HERE/SNP_simonly_b1215_1fp.vcf --make-bed --allow-extra-chr \ 
--out simo

/programs/plink2_linux_avx2_20220129/plink2 \
--vcf start_HERE/SNP_simonly_b1215_1fp.vcf --make-bed --out simo \
--allow-extra-chr --threads 24 --vcf-half-call m #Call them as missing

/programs/plink2_linux_avx2_20220129/plink2 \
--vcf start_HERE/SNP_solonly_b1215_1fp.vcf --make-bed --out solo \
--allow-extra-chr --threads 24 --vcf-half-call m #Call them as missing

#Put data here
/home/hh693/.local/lib/python3.8/site-packages/feems/data/
#n_samples=125, n_snps=1508
#Heck yeah!

#Coordinates -70 40 for each individual
#Check the order of individuals in the bed file
#Assume it's the same as the samples in the vcf
bcftools query -l start_HERE/SNP_simonly_b1215_1fp.vcf > simonly_samplelist.txt
#Then pull into excel to manually add the right coordinates for each sample
#Still not sure what .outer is - try just using theirs

#Great, my input is working but I definitely need better estimates of outer cause my sample sites are outside of it
#outer is just the outline of the polygon?

#Try the lambda estimate or the workshop example

#Originally I tried it with Outer set to just ocean but I'll try including some land maybe
#I could also try to do the more detailed one

#Date: May 6th
#Make new FEEMS

#So lets start by just doing a nice solidissima base
/programs/plink2_linux_avx2_20220129/plink2 \
--vcf start_HERE/SNP_solonly_b1215_1fp.vcf --make-bed --out solo \
--allow-extra-chr --threads 24 --vcf-half-call m
#Already done, data also stored in FEEMS/data 

#Before moving, put data here
/home/hh693/.local/lib/python3.8/site-packages/feems/data/

#So now I just need solo.outer and solo.coord

#My fam file for similis looks a little, uh, mellow. So maybe that's the problem
#My fam file for similis would be more like the the wolf one if the first column was identical to the 2nd column
#Pull the sample order from the .fam file though

#The data includes PVT_008H07 and it should NOT
#Just remove that one
vcftools --remove-indv PVT_008H07 --vcf start_HERE/SNP_solonly_b1215_basic.vcf --recode --out start_HERE/SNP_solonly_b1215_basic2
vcftools --remove-indv PVT_008H07 --vcf start_HERE/SNP_solonly_b1215_1fp.vcf --recode --out start_HERE/SNP_solonly_b1215_1fp2

#So uh since the server is completely full up and feems is installed everywhere so I can do it on another server
#Mounting start_HERE and feems
/programs/plink2_linux_avx2_20220129/plink2 \
--vcf start_HERE/SNP_solonly_b1215_1fp2.recode.vcf --make-bed --out solo2 \
--allow-extra-chr --threads 24 --vcf-half-call m
#Have to run this version on lamer machines
/programs/plink2_linux_general_20220129/plink2 \
--vcf start_HERE/SNP_solonly_b1215_1fp2.recode.vcf --make-bed --out solo2 \
--allow-extra-chr --threads 22 --vcf-half-call m &
#Runs almost instantly
#Put all the data in the right spot
cp solo2* /home/hh693/.local/lib/python3.8/site-packages/feems/data/

#Start FEEMS
source /programs/miniconda3/bin/activate feems
jupyter notebook --ip=0.0.0.0 --port=8016 --no-browser
#Goes REAL slow on the alternate servers instead of hare
#Probably would be worth it to get a beefier computer

#Something is wrong with my solo2.outer
#But it did converge!
#Noice, try remaking solo2.outer starting with simo.outer

-76.120793	36.275964
-76.345136	37.709371
-75.223419	40.630624
-69.928912	44.591133
-66.698366	45.478918
-62.884527	45.478918
-66.967578	36.420518
#Lat/Lon are switched, negatives on the left

#List of GA
awk '{print $1}' simo.fam | tail -106 | head -23 > GA_list.txt
vcftools --remove GA_list.txt --vcf start_HERE/SNP_simonly_b1215_1fp.vcf --recode --out SNP_simonly_SNE_1fp

/programs/plink2_linux_general_20220129/plink2 \
--vcf SNP_simonly_SNE_1fp.recode.vcf --make-bed --out simoSNE \
--allow-extra-chr --threads 22 --vcf-half-call m &
#Outer for SNE
40.739386, -74.803468
39.926546, -74.333030
40.136668, -69.801143
41.554149, -68.585845
42.439830, -69.605127
42.091692, -72.365030
41.858532, -74.874033

cp simoSNE* /home/hh693/.local/lib/python3.8/site-packages/feems/data/

#Higher lambda decreases the maximum value?
#Does not affect convergence

#Date: May 6th
#Running more stuff in SpaceMix
#Notes in R

#Date: May 13th
#More FEEMS
#Separate A/B in vcf and then do the conversion
vcftools --keep As.txt --vcf ../start_HERE/SNP_solonly_b1215_1fp2.recode.vcf --recode --out AB_vcfs_0513/SNP_Aonly_b1215_1fp2
vcftools --keep Bs.txt --vcf ../start_HERE/SNP_solonly_b1215_1fp2.recode.vcf --recode --out AB_vcfs_0513/SNP_Bonly_b1215_1fp2

#105 A, 270 B

cd AB_vcfs_0513/
/programs/plink2_linux_avx2_20220129/plink2 \
--vcf SNP_Bonly_b1215_1fp2.recode.vcf --make-bed --out Bonly \
--allow-extra-chr --threads 24 --vcf-half-call m

#Move into proper folder
cp Aonly* /home/hh693/.local/lib/python3.8/site-packages/feems/data/
cp Bonly* /home/hh693/.local/lib/python3.8/site-packages/feems/data/

#Start jupyter notebook
source /programs/miniconda3/bin/activate feems
jupyter notebook --ip=0.0.0.0 --port=8016 --no-browser
cd ../feems
#Copy over files from when I was doing this on another server
#Error: AssertionError: genotypes and sample positions must be the same size

#Maybe the vcfs need to be filtered to only variant sites?
	#If so, maybe I need to do this from the main one and then filter one to 1fp
vcftools --keep As.txt --maf 0.05 --vcf ../start_HERE/SNP_solonly_b1215_1fp2.recode.vcf --recode --out AB_vcfs_0513/SNP_Aonly_b1215_1fp2
vcftools --keep Bs.txt --maf 0.05 --vcf ../start_HERE/SNP_solonly_b1215_1fp2.recode.vcf --recode --out AB_vcfs_0513/SNP_Bonly_b1215_1fp2
/programs/plink2_linux_avx2_20220129/plink2 \
--vcf SNP_Bonly_b1215_1fp2.recode.vcf --make-bed --out Bonly \
--allow-extra-chr --threads 24 --vcf-half-call m
cp Aonly* /home/hh693/.local/lib/python3.8/site-packages/feems/data/
cp Bonly* /home/hh693/.local/lib/python3.8/site-packages/feems/data/
#Need new coordinates file for the just A and just B samples
#Butts
#Get the sample list in order
bcftools query -l SNP_Aonly_b1215_1fp2.recode.vcf

mv /home/hh693/.local/lib/python3.8/site-packages/feems/data/Aonly.coords /home/hh693/.local/lib/python3.8/site-packages/feems/data/Aonly.coord
mv /home/hh693/.local/lib/python3.8/site-packages/feems/data/Bonly.coords /home/hh693/.local/lib/python3.8/site-packages/feems/data/Bonly.coord

#Date: May 13th
#Now try EEMS
/programs/eems/bed2diffs/src/bed2diffs_v1
#Get the right input
/programs/eems/runeems_snps/src/runeems_snps
#Run on sol B to start
#Move files to EEMS folder

#Try running
/programs/eems/bed2diffs/src/bed2diffs_v1 --bfile Bonly --nthreads 2
#Error
#[Data::getsize] Error opening plink files Bonly.[bed/bim/fam] 
#This error means that libplinkio (https://github.com/mfranberg/libplinkio) cannot read the dataset correctly.

#For example, check that `plink --bfile datapath --freq` runs without errors.
plink --bfile Bonly --freq
#Edit plink path?

#See if I can just install this part myself
git clone https://github.com/mfranberg/libplinkio
cd libplinkio
git checkout 781e9ee37076
mkdir build
cd build
../configure --prefix=/path/to/plinkio
make && make check && make install
#Not working, there's something that can also do it in R?
#Done one version of it
	#Assuming the output need to be tab separated
	#Delete header and collumn name
mv test_Bonly.tsv Bonly.diffs
mkdir tempB

/programs/eems/runeems_snps/src/runeems_snps --help
#8 was how many there were before
  --nDemes arg                          Number of demes, roughly. EEMS 
                                        constructs a regular triangular grid 
                                        with ~nDemes vertices.
#I don't have
  --gridpath arg                        Full path to a set of three files: 
                                        gridpath.demes, gridpath.edges and 
                                        gridpath.ipmap.

/programs/eems/runeems_snps/src/runeems_snps --datapath Bonly --mcmcpath tempB \
--nDemes 8 \
--nIndiv 270 --nSites 1990 \
--numMCMCIter 100 --numBurnIter 10 --numThinIter 10

#Test run with very few MCMC
#Error: The population grid is not connected.
#Change/reduce the number of demes
/programs/eems/runeems_snps/src/runeems_snps --datapath Bonly --mcmcpath tempB \
--nDemes 2 \
--nIndiv 270 --nSites 1990 \
--numMCMCIter 100 --numBurnIter 10 --numThinIter 10
#Demes is not the number of dots, it's the number of total triangular intersections

#Noice, this worked:
/programs/eems/runeems_snps/src/runeems_snps --datapath Bonly --mcmcpath tempB \
--nDemes 2000 \
--nIndiv 270 --nSites 1990 \
--numMCMCIter 100 --numBurnIter 10 --numThinIter 10 > tempB/test1.log
#Now how do I plot?
#In R?
#Not compatible with current version of R
#Try instead there's a python script
git clone https://github.com/dipetkov/eems
source /programs/miniconda3/bin/activate feems
jupyter notebook --ip=0.0.0.0 --port=8016 --no-browser

#Disconnected
sudo apt-get install python-rpy2
#We trust you have received the usual lecture from the local System Administrator.
#I don't wanna to scary
#install python-rpy2 .

#Date: May 15th
#Redo Fst with no hybrids
#Start with...
SNP_solonly_b1215_basic2.recode.vcf
#Popmap A vs B

#Redo haployper
#Nope! Don't, just refilter the genepop file by population definitions for FST
#But I do need to change the pops

#Grep:
#First line (head -1)
# Grep dDoc +
# Cat(POP) + 
# Grep While reading the list of As +
# Cat(POP) + 
# Grep While reading the list of Bs +

head -1 haps_sol_region_0314.gen >> haps_sol_AvsBnoH_0515.gp
grep "dDoc" haps_sol_region_0314.gen >> haps_sol_AvsBnoH_0515.gp
echo "POP" >> haps_sol_AvsBnoH_0515.gp
while read p; do
grep "$p" haps_sol_region_0314.gen >> haps_sol_AvsBnoH_0515.gp
done <AsnoH.txt
echo "POP" >> haps_sol_AvsBnoH_0515.gp
while read p; do
grep "$p" haps_sol_region_0314.gen >> haps_sol_AvsBnoH_0515.gp
done <BsnoH.txt

#Noice
#Repeat but instead do for sets with similis too
grep "Sim" popmaps/popmap_all_ABsim.txt | awk '{print $1}' > start_HERE/haplotypes/Sims.txt

head -1 haps_solref_all0408.gp >> haps_solref_AvsBnoHsim_0515.gp
grep "dDoc" haps_solref_all0408.gp >> haps_solref_AvsBnoHsim_0515.gp
echo "POP" >> haps_solref_AvsBnoHsim_0515.gp
while read p; do
grep "$p" haps_solref_all0408.gp >> haps_solref_AvsBnoHsim_0515.gp
done <AsnoH.txt
echo "POP" >> haps_solref_AvsBnoHsim_0515.gp
while read p; do
grep "$p" haps_solref_all0408.gp >> haps_solref_AvsBnoHsim_0515.gp
done <BsnoH.txt
echo "POP" >> haps_solref_AvsBnoHsim_0515.gp
while read p; do
grep "$p" haps_solref_all0408.gp >> haps_solref_AvsBnoHsim_0515.gp
done <Sims.txt

#NOTE: DID NOT PRODUCE WHAT I WAS LOOKING FOR, what's up?
#Maybe take a look at just the situation in the genepop file first
#Why are there no SsLI?
#Also no WVs?
#What the heck?
#Check the dDoc0330 files

grep -v "dDoc" haps_solref_all0408.gp | awk '{print $1}' | grep -v "POP"

#Somewhere along the line with the vcf filtering must have gone wrong
#So I will have to rerun the haplotyper
Final.recode.vcf  SNP_HWE_b0408_solrefall_basic.vcf  TotalRawSNPs.vcf

#List of samples
SNP_HWE_b0408_solrefall_basic.vcf
bcftools query -l SNP_HWE_b0408_solrefall_basic.vcf | wc -l
bcftools query -l TotalRawSNPs.vcf | wc -l
#Missing <20 samples

#Ooph, I have two PEC013s
#But SsLI is not present
#They were not aligned to the reference! Crap! Why?

#When I redo this in the future, I should have exactly 500 samples
#For today then, I can work with what I have and theoretically, my above code does work to resample populations

awk '{print $1, $2}' SNP_simrefall_b0406_basic.vcf > ../sim_loci.txt
awk '{print $1, $2}' SNP_solrefall_b0408_basic.vcf > ../sol_loci.txt


#Date: May 17th
#EEMS for similis
#Demes is not the number of dots, it's the number of total triangular intersections

#Noice, this worked:
/programs/eems/runeems_snps/src/runeems_snps --datapath Bonly --mcmcpath tempB \
--nDemes 2000 \
--nIndiv 270 --nSites 1990 \
--numMCMCIter 100 --numBurnIter 10 --numThinIter 10 > tempB/test1.log
#Just copy the data from before?
cp /home/hh693/.local/lib/python3.8/site-packages/feems/data/simo* .
#I can run both SNE and all simo
#Need to create the .diff files from vcf
	#Done in the EEMS r script - function does not work, do it step by step
	#Exports as .tsv
	#Then #Delete header and collumn name and upload to server then change to .diff
	

#SNE vcf
vcftools --vcf start_HERE/SNP_simonly_b1215_1fp.vcf --remove temp --recode

#Run
/programs/eems/runeems_snps/src/runeems_snps --datapath simo --mcmcpath tempB \
--nDemes 2000 \
--nIndiv 270 --nSites 1990 \
--numMCMCIter 100 --numBurnIter 10 --numThinIter 10 > tempsimo/test1.log

#102, 1507
#125, 1508

/programs/eems/runeems_snps/src/runeems_snps --datapath simo --mcmcpath tempsimo \
--nDemes 2000 \
--nIndiv 125 --nSites 1508 \
--numMCMCIter 100 --numBurnIter 10 --numThinIter 10 > tempsimo/test1.log

/programs/eems/runeems_snps/src/runeems_snps --datapath simoSNE --mcmcpath tempSNE \
--nDemes 2000 \
--nIndiv 102 --nSites 1507 \
--numMCMCIter 100 --numBurnIter 10 --numThinIter 10 > tempSNE/test1.log

#To plot, what do I import into R?
#Just grab the whole "temp" mcmcpath folder

#The computational cost of EEMS is cubic in the size of the population grid, and the current implementation does not scale well beyond 1,000 demes
#Their example
#numMCMCIter = 2000000
#numBurnIter = 1000000
#numThinIter = 9999

/programs/eems/runeems_snps/src/runeems_snps --datapath Bonly --mcmcpath tempB \
--nDemes 1000 \
--nIndiv 270 --nSites 1990 \
--numMCMCIter 1000000 --numBurnIter 500000 --numThinIter 9999 > tempB/test2.log

/programs/eems/runeems_snps/src/runeems_snps --datapath simo --mcmcpath tempsimo \
--nDemes 1000 \
--nIndiv 125 --nSites 1508 \
--numMCMCIter 1000000 --numBurnIter 500000 --numThinIter 9999 > tempsimo/test2.log

/programs/eems/runeems_snps/src/runeems_snps --datapath simoSNE --mcmcpath tempSNE \
--nDemes 1000 \
--nIndiv 102 --nSites 1507 \
--numMCMCIter 1000000 --numBurnIter 500000 --numThinIter 9999 > tempSNE/test2.log


#!/bin/bash -l

#SBATCH --partition=regular
#SBATCH --job-name=temp
#SBATCH --output=temp.out
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --mail-user=hh693@cornell.edu

sbatch --nodes=1 --ntasks=2 --mem=50000 -o B1.out -J B_EEMS s_B1.sh
sbatch --nodes=1 --ntasks=2 --mem=50000 -o sim1.out -J sim_EEMS s_sim1.sh
sbatch --nodes=1 --ntasks=1 --mem=20000 -o SNE1test.out -J SNE_EEMS s_SNE1.sh

#Time how long
#in 10 minutes it goes from 20000 > 39000, 19k per 10 min
#I could do this with fewer than 10000 demes and it would go faster...
#So about 9 hours

#Date: May 19th
#Get the genotype A ready
	#Get .bed, bim and fam
		#In AB_vcfs_0513/
	#Make diffs file in R
	#105, 1928 individuals and sites
rename _outer.txt .outer *_outer.txt

#A
/programs/eems/runeems_snps/src/runeems_snps --datapath Aonly --mcmcpath A_0519 \
--nDemes 1000 \
--nIndiv 105 --nSites 1928 \
--numMCMCIter 2000000 --numBurnIter 1000000 --numThinIter 999 > A_0519/A2.log
#B
/programs/eems/runeems_snps/src/runeems_snps --datapath Bonly --mcmcpath B_0519 \
--nDemes 1000 \
--nIndiv 270 --nSites 1990 \
--numMCMCIter 2000000 --numBurnIter 1000000 --numThinIter 999 > B_0519/B2.log
#simo
/programs/eems/runeems_snps/src/runeems_snps --datapath simo --mcmcpath simo_0519 \
--nDemes 1000 \
--nIndiv 125 --nSites 1508 \
--numMCMCIter 2000000 --numBurnIter 1000000 --numThinIter 999 > simo_0519/simo2.log
#simoSNE
/programs/eems/runeems_snps/src/runeems_snps --datapath simoSNE --mcmcpath simoSNE_0519 \
--nDemes 1000 \
--nIndiv 102 --nSites 1507 \
--numMCMCIter 2000000 --numBurnIter 1000000 --numThinIter 999 > simoSNE_0519/simoSNE2.log

sbatch --nodes=1 --ntasks=1 --mem=20000 -o simoSNE_server/SNE2.out -J SNE_EEMS simoSNE2.sh
sbatch --nodes=1 --ntasks=1 --mem=20000 -o simo_server/simo2.out -J simo_EEMS simo2.sh
sbatch --nodes=1 --ntasks=1 --mem=20000 -o A_server/A2.out -J A_EEMS A2.sh
sbatch --nodes=1 --ntasks=1 --mem=20000 -o B_server/SNE2.out -J B_EEMS B2.sh

#Get another server
/programs/bin/labutils/mount_server cbsuhare /storage
cp /fs/cbsuhare/storage/Spisula/Mounting/EEMS/ . -r

sed -i "s/_0519/_server/g" *.sh

manage_slurm new cbsumm12
#Takes about 3 minutes to set up

#So now I've get two sets running

#Date: May 20th
#Some finished, some haven't
	#sol B and sim SNE finished
#Server all still running


#Date: May 23rd
#Primer 3 and fasta maker

#Continue with fasta marker
nano sol_highFST3.txt
dDocent_Contig_2788
dDocent_Contig_3439
dDocent_Contig_3976

while read p; do
echo "Working on $p"
while read q; do
samtools faidx ../solrefSNPs/sol_reference.fasta $p | bcftools consensus ../solrefSNPs/SNP_solrefall_b0408_basic.vcf.gz --sample $q > bycontig/$q.$p.fasta
#add header - slightly different for sol
sed -i "s/>d/>$q solref_d/g" bycontig/$q.$p.fasta #using double quotes for sed takes it less literally than single quotes?
done <samplelist.txt
cat bycontig/*.$p.fasta > finishedcontig/$p.all.fasta
echo "Done with $p"
done <sol_highFST3.txt &> log_0523_sol3.log &

nano sim_highFST3.txt
dDocent_Contig_1028
dDocent_Contig_1268
dDocent_Contig_1296
dDocent_Contig_1965
dDocent_Contig_2642
dDocent_Contig_1036
dDocent_Contig_1708
dDocent_Contig_1960
dDocent_Contig_2364
dDocent_Contig_2651
dDocent_Contig_2752
dDocent_Contig_2773
dDocent_Contig_2956

while read p; do
echo "Working on $p"
while read q; do
samtools faidx ../simrefSNPs/sim_reference.fasta $p | bcftools consensus ../simrefSNPs/SNP_simrefall_b0406_basic.vcf.gz --sample $q > bycontig/$q.$p.fasta
#add header - slightly different for sol
sed -i "s/>d/>$q simref_d/g" bycontig/$q.$p.fasta #using double quotes for sed takes it less literally than single quotes?
done <samplelist.txt
cat bycontig/*.$p.fasta > finishedcontig/$p.all.fasta
echo "Done with $p"
done <sim_highFST3.txt &> log_0523_sim3.log &

#Yes, there are several with the same name
rename .all. .simref. bysample_simref/finishedcontig/*all.fasta
cp bysample_simref/finishedcontig/* multisampleFastas

rename .all. .solref. bysample_solref/finishedcontig/*all.fasta
cp bysample_solref/finishedcontig/* multisampleFastas

ls /local/storage/Spisula/GBS/highFST/multisampleFastas | wc -l
cp multisampleFastas/* /local/storage/Spisula/GBS/highFST/multisampleFastas

#Date: May 23rd
#Do Primer 3 on high FST contigs
	#Back to the world of primer 3
	#Does primer 3 use vcf, then I just filter to that region, aka the contig
#Options for getting it
	#NOPE Install from github <- Nope, requires sudo so I'm nervous
	#NOPE Look for old version
	#NOPE Try Web Applet version <- requires primer 3
	#NOPEInstall with sudo <- sudo missing a command
		#Try on another server cause I'm nervous
		cbsum1c1b007
	#Ask BioHPC to install
	
#While waiting, try the alternate available program: ecoPrimers
	#version 0.5
	#https://pythonhosted.org/OBITools/scripts/ecoPrimers.html
	#Have to format database
		#Fasta format works so I think I can use what I just made
			#Example
			ecoPrimers -d mydatabase -e 3 -l 50 \
			   -L 800 -r 2759 -3 2 > mybarcodes.ecoprimers

			#Try once with my data
			/programs/ecoprimers-0.5/bin/ecoPrimers -d dDocent_Contig_2956.simref.fasta \
			-e 0 -l 250 -L 350 \
			-3 5 > test_dDocent_Contig_2956.ecoprimers
			#Reading taxonomy database ...Error 1 in file ecoIOUtils.c line 101 : Cannot open file
				#Maybe I do need to convert the database?
				#Oh yeah, it can take inputs that are fasta but needs to output them
				
			#e = number of mismatches allowed
			#l vs L are minimum versus maximum 
				#I'm trying to make sure most of it is captured from 250-350
			#Defines the number of nucleotides on the 3’ end of the primers that must have a strict match with their target sequences.
				#Why would this only be 2 in the example, I'm trying 5	
			#-q <FLOAT>
				#Defines the strict matching quorum, i.e. the proportion of the sequence records in which a strict match between the primers and their targets occurs (default: 0.7)
				#I probably want this to be 0.9 or higher on a future run

			#Convert database
			source /programs/Anaconda2/bin/activate obitools
			#obiannotate -h
			obiconvert -h

			obiconvert --ecopcrdb --fasta-output --raw-fasta -t temp \
			  'dDocent_Contig_2956.simref.fasta' > dDocent_Contig_2956.db.fasta
  
			/programs/ecoprimers-0.5/bin/ecoPrimers -d dDocent_Contig_2956.db.fasta \
			-e 0 -l 250 -L 350 \
			-3 5 > test_dDocent_Contig_2956.ecoprimers

			obiconvert --ecopcrdb --fasta-output --raw-fasta --ecopcrdb-output dDocent_Contig_2956.2db \
			  'dDocent_Contig_2956.simref.fasta'
			# raise Exception("Taxonomy error for %s: %s"%(seq.id, "taxonomy is missing" if self._taxonomy is None else "sequence has no taxid" if 'taxid' not in seq else "wrong taxid"))
			#Do I need taxid for the fasta?

			#But not that other type, I need straight up taxonomic ids from genbank. WHY? Also, how do I deal with that if they are all the same taxon?
#Instead, try other primer designing
#Like the one on NCBI

while read q; do
cp ../multisampleFastas/$q* ./priority_0523
done <simcontigs_0523.txt
while read q; do
cp ../multisampleFastas/$q* ./priority_0523
done <solcontigs_0523.txt

#Settings
#Min
250
#Max
300
# of primers to return
10
#Min 5' match Min 3' match Max 3' match
7				4				8
#Turn off Repeat Filter and Low Complexity Filter under advanced

#Try removing the Ns internally because it's still spitting out too many Ns
#Still did not work
#Try copy and pasting a couple samples
#That worked!
#Try pasting the whole thing
	#Also worked
	#Try pasting with the original Ns

#Date: May 24th
#Primer3 installed on server

# set environment
module load gcc/10.2.0
#run command
/programs/primer3-2.6/primer3_core --output out.out < my_input_file
#OR
/programs/primer3-2.6/primer3_core --output out.out  my_input_file

#Look through my notes from the previous time I ran it
for chr in ${CONTIGS[@]}
do 
echo "$chr"
tmpseq=$(samtools faidx Sol_ref.fa $chr | grep -v ">" |tr -d "\n" | tr '[A-Z]' '[a-z]')
#it likes lowercase input? Seriously?
done

for chr in ${CONTIGS[@]}
do 
echo "$chr"
samtools faidx Sol_ref.fa $chr > ref_chrs/ref_$chr.fa
tmpseq=$(samtools faidx Sol_ref.fa $chr | grep -v ">" |tr -d "\n" | tr '[A-Z]' '[a-z]')
echo \
"SEQUENCE_ID=$chr
SEQUENCE_TEMPLATE=$tmpseq
PRIMER_TASK=generic
PRIMER_PICK_LEFT_PRIMER=1
PRIMER_PICK_INTERNAL_OLIGO=0
PRIMER_PICK_RIGHT_PRIMER=1
PRIMER_OPT_SIZE=20
PRIMER_MIN_SIZE=18
PRIMER_MAX_SIZE=22
PRIMER_PRODUCT_SIZE_RANGE=200-300
PRIMER_EXPLAIN_FLAG=1
PRIMER_NUM_RETURN=10
PRIMER_MIN_GC=45
PRIMER_MAX_GC=60
PRIMER_MAX_TM=62
PRIMER_MAX_SELF_ANY=0.0
PRIMER_MAX_SELF_END=0.0
PRIMER_THERMODYNAMIC_OLIGO_ALIGNMENT=1
PRIMER_MAX_HAIRPIN_TH=30.0
PRIMER_WT_HAIRPIN_TH=1
=" > primer_attempts/primer_$chr.input
./primer3/src/primer3_core primer_attempts/primer_$chr.input > primer_attempts/primer_$chr.output
for NUM in 0 1 2 3 4 5 6 7 8 9
do
pleft=$(grep "PRIMER_LEFT_$NUM=" primer_attempts/primer_$chr.output | sed 's/^.\{14\}//' | sed 's/.\{3\}$//')
pright=$(grep "PRIMER_RIGHT_$NUM=" primer_attempts/primer_$chr.output | sed 's/^.\{15\}//' | sed 's/.\{3\}$//')
bcftools view o_total_filter.vcf.gz -r $chr:$pleft-$pright -o primer_attempts/f_$chr.$NUM.vcf
usable=$(grep -v "#" primer_attempts/f_$chr.$NUM.vcf | wc -l)
if [ $usable -ge 1 ]; then
echo -e "$chr\t$NUM\t$pleft\t$pright\t$usable" >> list_of_sites.txt
echo "found $usable SNPS"
#SAVE THE FASTA FILE OF THAT PRIMER PRODUCT
samtools faidx Sol_ref.fa $chr:$pleft-$pright -o products/p_sol_$chr.$NUM.fasta
#Grab the fasta of any one similis from because they should all have the same snp at these sites (but what about added snps that it might pick up on)
#So maybe instead I could use the reference with the vcf (keep only the similis in the vcf)
bgzip -c primer_attempts/f_$chr.$NUM.vcf > primer_attempts/f_$chr.$NUM.vcf.gz
tabix primer_attempts/f_$chr.$NUM.vcf.gz
bcftools view primer_attempts/f_$chr.$NUM.vcf.gz -s Sim4 -O z > primer_attempts/f_sim_$chr.$NUM.vcf.gz
tabix primer_attempts/f_sim_$chr.$NUM.vcf.gz
samtools faidx ref_chrs/ref_$chr.fa $chr:$pleft-$pright  | vcf-consensus primer_attempts/f_sim_$chr.$NUM.vcf.gz > products/sim_$chr.$NUM.fasta
#Add species name to header
sed -i 's/>/>Sim_/' products/sim_$chr.$NUM.fasta
sed -i 's/>/>Sol_/' products/p_sol_$chr.$NUM.fasta
#Make into single file for SNP2CAPS
cat products/*_$chr.$NUM.fasta >> CAPS_input/input_SNP2CAP_$chr.$NUM.fasta
fi
done
done


#Modify for new
while read chr; do
echo \
"SEQUENCE_ID=1028.simref
SEQUENCE_TEMPLATE=
PRIMER_TASK=generic
PRIMER_PICK_LEFT_PRIMER=1
PRIMER_PICK_INTERNAL_OLIGO=0
PRIMER_PICK_RIGHT_PRIMER=1
PRIMER_OPT_SIZE=20
PRIMER_MIN_SIZE=18
PRIMER_MAX_SIZE=22
PRIMER_PRODUCT_SIZE_RANGE=200-300
PRIMER_EXPLAIN_FLAG=1
PRIMER_NUM_RETURN=10
PRIMER_MIN_GC=45
PRIMER_MAX_GC=60
PRIMER_MAX_TM=62
PRIMER_MAX_SELF_ANY=0.0
PRIMER_MAX_SELF_END=0.0
PRIMER_THERMODYNAMIC_OLIGO_ALIGNMENT=1
PRIMER_MAX_HAIRPIN_TH=30.0
PRIMER_WT_HAIRPIN_TH=1
=" > primer_attempts/primer_$chr.input
/programs/primer3-2.6/primer3_core primer_attempts/primer_$chr.input --output primer_attempts/primer_$chr.output
done <solcontigs_0523.txt

/programs/primer3-2.6/primer3_core < primer_attempts/primer_dDocent_Contig_3976.input
#I can have multiple samples as separate seq id and seq template entries
	#NOPE
#Nope, maybe not, that's to make primers for each one separately
#Instead, I could use the vcf to make sure there is no SNP at that site I guess

echo \
"SEQUENCE_ID=1028.simref
SEQUENCE_TEMPLATE=NGCTATGCAGCATTAGGTTGTCATCTGTTATTTGTTGATATATTTTGATGAAATGTACTCAATATGTAGGTAATGTACTAATGAAATGTAATAAAATCATAATTCATCATTTAAATTCTTTATTTCCGAATATTTATGTTTCCTAAATTGTTNNNNNNNNNNTACATATAAAGTTACCAAAAATTATAATATACTCTTACAATATTTTTATGTGGGCACTGACGGTGTTTTCTAGAGCTTCGTTTACATAACAGTCCACAATGTATACCCGATCTTTTGATTCAATCAGATTTATGTTGTCGTCGGTCGTCCGN
SEQUENCE_ID=1028.simref.P
SEQUENCE_TEMPLATE=NGCTATGCAGCATTAGGTTGTCATCTGTTATTTGTTGATATATTTTGATGAAATGTACTCAATATGTAGGTAATGTACTAATGAAATGTAATAAAATCATAATTCATCATTTAAATTCTTTATTTCCGAATATTTATGTTTCCTAAATTGTTNNNNNNNNNNTACATATAAAGTTACCAAAAATTATAATATACTCTTACAATATTTTTATGTGGGCACTGACGGTGTTTTCTAGAGCTTCGTTTACATAACAGTCCACAATGTATACCCGATCTTTTGATTCAATCAGATTTATGTTGTCGTCGGTCGTCCGN
SEQUENCE_ID=1028.simref.A
SEQUENCE_TEMPLATE=NGCTATGCAGCATTAGGTTGTCATCTGTTATTTGTTGATATATTTTGATGAAATGTACTCAATATGTAGGTAATGTACTAATGAAATGTAATAAAATCATAATTCATCATTTAAATTCTTTATTTCCGAATATTTATGTTTCCTAAATTGTTNNNNNNNNNNTACATATAAAGTTACCAAAAATTATAATATACTCTTACAATATTTTTATGTGGGCATTGACGGCGTTTTCTAGAGCTTCGTTTACATAACAGTCCACAATGTATACCCGATCTTTTGATTCAATCAGATTTATGTTGTCGTCGGTCGTCCGN
SEQUENCE_ID=1028.simref.B
SEQUENCE_TEMPLATE=NGCTATGCAGCATTAGGTTGTCATCTGTTATTTGTTGATATATTTTGATGAAATGTACTCAATATGTAGGTAATGTACTAATGAAATGTAATAAAATCATAATTCATCATTTAAATTCTTTATTTCCGAATATTTATGTTTCCTAAATTGTTNNNNNNNNNNTACATATAAAGTTACCAAAAATTATAATATACTCTTACAATATTTTTATGTGGGCATTGACGGCGTTTTCTAGAGCTTCGTTTACATAACAGTCCACAATGTATACCCGATCTTTTGATTCAATCAGATTTATGTTGTCGTCGGTCGTCCGN
PRIMER_TASK=generic
PRIMER_PICK_LEFT_PRIMER=1
PRIMER_PICK_INTERNAL_OLIGO=0
PRIMER_PICK_RIGHT_PRIMER=1
PRIMER_OPT_SIZE=20
PRIMER_MIN_SIZE=18
PRIMER_MAX_SIZE=22
PRIMER_PRODUCT_SIZE_RANGE=200-300
PRIMER_EXPLAIN_FLAG=1
PRIMER_NUM_RETURN=10
PRIMER_MIN_GC=45
PRIMER_MAX_GC=60
PRIMER_MAX_TM=62
PRIMER_MAX_SELF_ANY=0.0
PRIMER_MAX_SELF_END=0.0
PRIMER_THERMODYNAMIC_OLIGO_ALIGNMENT=1
PRIMER_MAX_HAIRPIN_TH=30.0
PRIMER_WT_HAIRPIN_TH=1
=" > attempt_test.input

/programs/primer3-2.6/primer3_core < attempt_test.input

#Just run on NCBI because it can search for SNPs
#PCR Product Size
#Min	Max
250		350
#Primer Tm
#Min	Opt		Max		Max diff
58		60		62		2
#Disable search for specificity - not relevant
#Primer Size
#Min	Opt		Max
18		20		24
#Primer GC Content
#Min	Max
40		70
#Primer binding site may not contain known SNP (now this may not work cause it is only using databases but I can observe manually)
#Repeat filter
	#None
#Turn off low complexity region avoidance

#For the ones that failed...
#Less stringent
#PCR Product Size
#Min	Max
225		350
#Primer Tm
#Min	Opt		Max		Max diff
57		60		63		3
#Disable search for specificity - not relevant
#Primer Size
#Min	Opt		Max
15		20		25
#Primer GC Content
#Min	Max
30		80
#Primer binding site may not contain known SNP (now this may not work cause it is only using databases but I can observe manually)
#Repeat filter
	#None
#Turn off low complexity region avoidance

#Did not produce viable primers
#Sim
(1296)
1708
2364
2651
2752
2773
(2956)

#Sol
3439

#Date: May 25th
#Lambda cross validation
source /programs/miniconda3/bin/activate feems
jupyter notebook --ip=0.0.0.0 --port=8016 --no-browser

cp EEMS/Bonly2.outer /home/hh693/.local/lib/python3.8/site-packages/feems/data/
cp /workdir/hh

-73	37.5
-76	37

#Date: May 25th
#Exploring SNAPP 2
#Visual thing - run VNC
	#Not working...? Super super slow
#Try without VNC
	#Instead try X-window viewer - like VNC but non-persistent
	#X Quartz

ssh -Y hh693@cbsuhare.biohpc.cornell.edu
#After that, you can directly type in the command. Eg. “gsAssembler”, and the graphical window should pop up.
/programs/beast/bin/beauti

#Installed SNAPP
#SNPAPP will be installed under /home/xxxx/.beast/2.4

#To run beast:
java -jar /programs/beast/lib/beast.jar input.xml

#To prepare a SNAPP input file

less /home/hh693/.local/lib/python3.8/site-packages/feems/data/Bonly2.outer
#10,8,8,10 <- 36, so I should be able to run on server

#Confirm later about how you set the space not being important, for now, just rerun B
sbatch --nodes=1 --ntasks=1 --mem=8000 -o B_0525/B3.out -J B_EEMS B3.sh


#Date: May 27th
#Make the LD data set for the big data sets
#I canceled the VNC, so maybe that'll remove some of my tasks

#My data sets
#Species	#Context	#Basic		#Haplotype		#LD prune
sim			only		done		genpops				
sol			only		done		genpops			
all			simref		done		genpops			
all			solref		done		genpops			

cp settingupvcfs/SNP_simonly_b1215_basic.vcf SNP_simonly_basic.vcf &
cp settingupvcfs/SNP_simrefall_b0406_basic.vcf SNP_allsimref_basic.vcf &
cp settingupvcfs/SNP_solonly_b1215_basic2.recode.vcf SNP_solonly_basic.vcf &
cp settingupvcfs/SNP_solrefall_b0408_basic.vcf SNP_allsolref_basic.vcf &

vcftools --vcf SNP_simonly_basic.vcf --geno-r2 --ld-window-bp 600 --out LD_prune/simonly_LD &
vcftools --vcf SNP_solonly_basic.vcf --geno-r2 --ld-window-bp 600 --out LD_prune/solonly_LD &
vcftools --vcf SNP_simrefall_basic.vcf --geno-r2 --ld-window-bp 600 --out LD_prune/simrefall_LD &
vcftools --vcf SNP_solrefall_basic.vcf --geno-r2 --ld-window-bp 600 --out LD_prune/solrefall_LD &
#Looks like it is able to calculate it from MOST sites
2992 out of a possible 2992 Sites
12375 out of a possible 12751 Sites
#However, many of them were nan
	#So instead perhaps keep just the ones with LD r2 < 0.8 rather than the rest
#Download and open in excel
	#Filter to a list of r < 0.8
	#Check after I do this that I also remove to only have one per contig?
	#Ahh.... actually... these are PAIRS of things within 600 bp of eachother
	#So... then they are not pairing anything OTHER than stuff on the same contig...
	#So... then I do actually need to have the r2 > 0.8 and to remove those (and maybe also remove nans)
#So... what do other projects do for LD stuff?
	#Check the frog paper / other papers that reference dDocent
		#Frogs
			#"These three analyses used genotype likelihoods (GL) as input, which were estimated in Angsd v.0.9.18 (Korneliussen, Albrechtsen, & Nielsen, 2014) at sites covered by at least one read in at least 50% of the samples 
			#without filtering for linkage disequilibrium."
#So then, I need to repeat what will be an incredibly long run of testing everything against eachother with no ld window
vcftools --vcf SNP_simonly_basic.vcf --geno-r2 --out LD_prune/simonly_LDfull
vcftools --vcf SNP_solonly_basic.vcf --geno-r2 --out LD_prune/solonly_LDfull
vcftools --vcf SNP_simrefall_basic.vcf --geno-r2 --out LD_prune/simrefall_LDfull
vcftools --vcf SNP_solrefall_basic.vcf --geno-r2 --out LD_prune/solrefall_LDfull

#!/bin/bash -l

#SBATCH --partition=regular
#SBATCH --job-name=temp
#SBATCH --output=temp.out
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --mail-user=hh693@cornell.edu

sbatch --nodes=1 --ntasks=1 --mem=6000 -o LD1.out -J LD_1th_6kmem s_LD1.sh

#Ahhh, I think it did not measure any of the comparisons outside of the chromosome because it assumes that it does not have any issues if it did
	#Can I measure LD across chromosomes?
	#Plink may be able to? (https://zzz.bwh.harvard.edu/plink/ld.shtml)
		#To use version 1.07, plink is in your path. You can directly type the command "plink".
		--inter-chr
		#This will take an insane amount of time to run I suppose but here we go, that's what I was suspecting
		--ld-window-r2 0.8
		#Therefore will only save the ones with high enough to be filtered
		#However, it should be noted that reducing the number saved will likely not impact the run time that much

#Need to convert to a plink file first
	#First make chromosome map
bcftools view -H SNP_simonly_basic.vcf | cut -f 1 | uniq | awk '{print $0"\t"$0}' > simonly.chrom-map.txt
	#Convert
vcftools --vcf SNP_simonly_basic.vcf --chrom-map simonly.chrom-map.txt --out SNP_simonly_basic --plink

#Do it again and again
bcftools view -H SNP_solonly_basic.vcf | cut -f 1 | uniq | awk '{print $0"\t"$0}' > solonly.chrom-map.txt
vcftools --vcf SNP_solonly_basic.vcf --chrom-map solonly.chrom-map.txt --out SNP_solonly_basic --plink




plink --file mydata --r2 --ld-window-r2 0.80
		#r2 is faster to calculate than r
plink --file SNP_simonly_basic --r2 --ld-window-r2 0.80 --no-sex --allow-no-sex --out simonly_pLD
plink --file SNP_solonly_basic --r2 --ld-window-r2 0.80 --out solonly_pLD

plink --file SNP_simrefall_basic.vcf --r2 --ld-window-r2 0.80 --out simrefall_pLD
plink --file SNP_solrefall_basic.vcf --r2 --ld-window-r2 0.80 --out solrefall_pLD

sbatch --nodes=1 --ntasks=1 --mem=6000 -o LD4.out -J LD_1th_6kmem s_LD4.sh

Expecting 5 + 2 * 12375 = 24755 columns, but found more
#How many does it have?
awk '{print NF}' SNP_simonly_basic.ped | sort -nu | tail -n 1
24756
#Just one extra column, there are two columns with the specimen name
#Can I remove that one?
#Ped format? I think so
#Remove first column
awk '{$1=""}1' SNP_simonly_basic.ped > SNP_simonly_basic2.ped	
#Works
#Now try running again
#I think it works but it spits out SUPER few options, not even one per chromosome, but a guess the default is 0.20, so try one with 0
	#For sanity check
	#Yeah okay, that works there just aren't very many > 0.8
	90464 simonly_pLD.ld #Also seems like too few in general but don't look a gift horse in the mouth I guess
	#Just eliminate those loci and otherwise use the 1fp

bcftools view -H SNP_solonly_basic.vcf | cut -f 1 | uniq | awk '{print $0"\t"$0}' > solonly.chrom-map.txt
vcftools --vcf SNP_solonly_basic.vcf --chrom-map solonly.chrom-map.txt --out SNP_solonly_basic --plink
awk '{$1=""}1' SNP_solonly_basic.ped > SNP_solonly_basic2.ped
mv SNP_solonly_basic2.ped SNP_solonly_basic.ped
plink --file SNP_solonly_basic --r2 --ld-window-r2 0.80 --no-sex --allow-no-sex --out solonly_pLD


bcftools view -H SNP_solrefall_basic.vcf | cut -f 1 | uniq | awk '{print $0"\t"$0}' > solrefall.chrom-map.txt
vcftools --vcf SNP_solrefall_basic.vcf --chrom-map solrefall.chrom-map.txt --out SNP_solrefall_basic --plink
awk '{$1=""}1' SNP_solrefall_basic.ped > SNP_solrefall_basic2.ped
mv SNP_solrefall_basic2.ped SNP_solrefall_basic.ped
plink --file SNP_solrefall_basic --r2 --ld-window-r2 0.80 --no-sex --allow-no-sex --out solrefall_pLD

#Oh, but if I'm just going to filter to 1fp anyway, then I should check that if even with those ones filtered, there is still LD to perserve the maximum number of loci.
#I don't have a solref all 1p?
#Guess not

#Step #10
	#Filter to only one per radtag by minor allele frequency
	#Actual calculation
vcftools --vcf SNP_solrefall_basic.vcf --freq --out freqlist
		#Open freqlist.fq in excel
		#Data by tabs and :
		#Before sorting, measure the minimum between the two (because it isn't minor allele, it's reference vs alternate)
		#Sort by max minor allele frequency then sort by unique contig
			#By removeing duplicates > expand selection > only select column A
		#Export by keeping column A and B without headers
		freq_keep.txt
			#1fprad = 1 (by frequency) per radtag
vcftools --vcf SNP_solrefall_basic.vcf --positions freq_keep.txt --recode --recode-INFO-all --out SNP_solrefall_1fprad 

PROJECTS=(
simonly
simrefall
solonly
solrefall
)

for p in ${PROJECTS[@]}
do
echo "$p"
bcftools view -H SNP_"$p"_1fprad.vcf | cut -f 1 | uniq | awk '{print $0"\t"$0}' > "$p"_1fprad.chrom-map.txt
vcftools --vcf SNP_"$p"_1fprad.vcf --chrom-map "$p"_1fprad.chrom-map.txt --out SNP_"$p"_1fprad --plink
awk '{$1=""}1' SNP_"$p"_1fprad.ped > SNP_"$p"_1fprad2.ped
mv SNP_"$p"_1fprad2.ped SNP_"$p"_1fprad.ped
plink --file SNP_"$p"_1fprad --r2 --ld-window-r2 0.80 --noweb --no-sex --allow-no-sex --out "$p"_1fp_pLD
done

for p in ${PROJECTS[@]}
do
#Adjust the below to be about sites so it can more easily be put into vcftools and --exclude-positions
#Pull out the column with one of the two contigs with high LD					#And remove header
awk '{ print $6 }' "$p"_1fp_pLD.ld | awk 'BEGIN { FS = ":" } ; { print $1,$2 }' | tail -n+2 > "$p"_1fp_pLD_chr.txt
vcftools --vcf SNP_"$p"_1fprad.vcf --exclude-positions "$p"_1fp_pLD_chr.txt --recode --out SNP_"$p"_LDprune
done

rename .recode.vcf .vcf *LDprune*


#Number of SNPs per set

				#Basic		#LDprune	#N		#Haplotypes
simonly			12751		1505		125			
simrefall		2992		442			444			
solonly			49438		2761		390			
solrefall		19345		3335		446			

#For analyses with all stuff, use the solrefall stuff probably
#Are all the samples in that one?
less haplotypes/haps_solref_all0408.gp 
grep -v "dD" haplotypes/haps_solref_all0408.gp | grep -v "POP" | grep -v "hap" | awk '{ print $1 }' | less
432 #So what's missing?
#Ssli and others...
#Confirm that solrefall has them... if so, then how could they be missing the bams?
	#It does not.
	#Also true for simrefall but I only need one
	
#Date: May 28th
#Rerun dDocent sol ref ALL 0528
	#Get all samples into a folder
	cp -n myoldfile.txt mycopiedfile.txt
		#-n is Copy without overwriting, there are all sol in the align sol 121
	#Should be at least 500 samples so 2000 F/R/R1/R2
		#Format with F and R1 <- already done
	#Remove known problem samples
		#Why are most of the CTMs in the unused files folder?
		#Remove NAN_004, PVT_006
	#Known Problem Samples to Remove
	MW_055		#Bad sequencing
	MW_056		#Bad sequencing
	NAN_004		#Contaminated
	NAN_005		#Contaminated
	PVT_008		#Same Individual
	PVT_008H07	#Same Individual
	FNJ_028		#Mess with PCA
	FNJ_029 	#Mess with PCA
	#Relatedness filter did not remove any from similis, should do with solidissima later
	#Check relatedness?

#Now I've got 2156 files so that's several more than the estimate from the popmap all
	#Scroll through to see differences
	#popmap is missing
		BAR_001
		BLP_233
		CGC_006
		CGC_007
		CGC_008
		GBE_524
		MCX_038
		MW_017	#But it has MW_01, just have both
		MW_012
		MW_013
		MW_020
		MW_021
		MW_033 #Was that the one closest to being a sim/sol hybrid - include for sure
		NAN_006
		NAN_011
		NAN_013
		#Several NANs - they had trouble sequencing - see the unused ones from the previous runs and follow that again 
		PLY_001
		PLY_002
		PLY_010
		PT_002
		PT_016
		PVT_001
		PVT_005
		SsLI1112_1
		SsLI1113_3
		SsLI1113_4
		SsLI4106_3
		
	#Samples are missing (but popmap has)
		FNJ_028		#Mess with PCA
		FNJ_029 	#Mess with PCA
	
	#Problem
	#I have samples for both PEC_013 and PEC0819_013 keep only one
		#Check sizes
		#Keep PEC0819_013
	
	#Well I'll just try to run it with these, and filter out later
		#Remove PVT_006 though cause it caused problems before

#Run dDocent
	#Find scripts and input files
	ls -d */ #List directories
	config_sol0330.txt
	config_sol_call0401.txt

Number of Processors
24
Maximum Memory
0
Trimming
no
Assembly?
no
Type_of_Assembly
PE
Clustering_Similarity%
0.85
Minimum within individual coverage level to include a read for assembly (K1)
2
Minimum number of individuals a read must be present in to include for assembly (K2)
2
Mapping_Reads?
yes
Mapping_Match_Value
1
Mapping_MisMatch_Value
4
Mapping_GapOpen_Penalty
6
Calling_SNPs?
yes
Email
hh693@cornell.edu

#!/bin/bash -l

#SBATCH --partition=regular
#SBATCH --job-name=temp
#SBATCH --output=temp.out
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --mail-user=hh693@cornell.edu
source /programs/miniconda3/bin/activate dDocent-2.8.13
dDocent config_solrefALL_0528.txt

sbatch --nodes=1 --ntasks=24 --mem=125000 -o dDoc_0528.out -J dDoc_24th_125M s_dDoc_0528.sh

#Date: May 28th
#Run EEMS with altered outers
sbatch --nodes=1 --ntasks=1 --mem=4000 -o B_0528/B4.out -J B_EEMS B4.sh
sbatch --nodes=1 --ntasks=1 --mem=4000 -o A_0528/A4.out -J A_EEMS A4.sh

#Date: May 30th
#Run similis EEMS with longer num iterations
sbatch --nodes=1 --ntasks=1 --mem=4000 -o simo_0530/simo4.out -J simo_EEMS simo4.sh
sbatch --nodes=1 --ntasks=1 --mem=4000 -o simoSNE_0530/smoSNE4.out -J SNE_EEMS simoSNE4.sh

#Date: May 30th
#Try the SNAPP estimation
#In brief, the dataset has been filtered to include only bi-allelic SNPs with a low proportion of missing data, for the 26 individuals from 13 cichlid species listed in the table below.

#So should I use fewer individuals?
#Probably
#Use the LD pruned set

#Install tutorial and get all the supplies
#This tutorial requires BEAST2, Tracer, and FigTree to be installed. Details about the installation of these tools can be found in tutorial Bayesian Phylogenetic Inference.
/programs/beast-2.6.7/bin/beast

#To modify the memory
#The java memory allocation is hard coded in the beast scripts. To use bigger memory allocation, copy the software to your home directory, and modify the script file with a text editor, e.g. bin/treeannotator. You need to replace "-Xmx8g" with a bigger number.
#copy the code
cp -r /programs/beast-2.6.7/ /home/$USER
#modify the script file
#run command
/home/$USER/beast-2.6.7/bin/beast
#So you have to modify it to let it use more memory

#Tracer does not exist on the server
#FigTree also does not

#Can I install myself?
#FigTree is for local computer to visualize


#Following this steps to install SNAPP to you home directory. This procedure is described on the SNAPP web site.
#1. Open BEAST2 package manager BEAUti. It has a graphic user interface. If you do not know how, read https://cbsu.tc.cornell.edu/lab/userguide.aspx?a=access under How to run graphical applications. 
#Run this command to start BEAUti: 
/programs/beast/bin/beauti
#2.  Select the File/Manage packages menu. 
#A list appears from which you select SNAPP — then hit the install button. 
#SNAPP will be installed under /home/xxxx/.beast/2.4

#I have to use the VCN
#I already did it!
/home/hh693/.beast/2.5/SNAPP

#Will BEAST be able to call it directly? I dunno. Is it okay if I run beast 2.6.7, or should I run beast 2.5

#Run prep
/programs/ruby/bin/ruby snapp_prep.rb -h
/programs/ruby/bin/ruby snapp_prep.rb -v 

#Date: June 2nd
#Finished dDocent rerun solrefall
#In the background rerun simrefall too (eventually)

#
#Step 1
#Basic Filtering
vcftools --vcf TotalRawSNPs.vcf --max-missing 0.5 --mac 3 --minQ 30 --recode --recode-INFO-all --out raw.g5mac3
#Run Time = 1556.00 seconds
vcftools --vcf raw.g5mac3.recode.vcf --minDP 3 --recode --recode-INFO-all --out raw.g5mac3dp3 
vcftools --vcf raw.g5mac3dp3.recode.vcf --missing-indv
awk '$5>0.5' out.imiss | cut -f1 > lowDP.indv 
#awk '$5>0.9' out.imiss | cut -f1 > lowDP.indv 
#Set threshold higher for solidissima ref because it's messy - then I'll filter to more loci cause 0.5 might remove all NAN and lots of MW 
vcftools --vcf raw.g5mac3dp3.recode.vcf --remove lowDP.indv --recode --recode-INFO-all --out raw.g5mac3dplm
vcftools --vcf raw.g5mac3dplm.recode.vcf --max-missing 0.90 --min-meanDP 20 --recode --recode-INFO-all --out DP3g95maf05 --min-meanDP 20
#For solidissima, after this step, check the individuals again
#vcftools --vcf DP3g95maf05.recode.vcf --missing-indv --out 2
#awk '$5>0.5' 2.imiss | cut -f1 > lowDP2.indv 
#None have that much missing data - works! Since I have so many sites, I could remove even fewer
#vcftools --vcf raw.g5mac3dplm.recode.vcf --max-missing 0.95 --min-meanDP 20 --recode --recode-INFO-all --out DP3g95maf05 --min-meanDP 20


#Step 2
#Missing by Site/Population/Region/Species
	#First do by Similis sites and solidissima separate
		#If that does not work, then put all similis together
awk '$2 == "GA\r"' samplist.txt > 1.keep
awk '$2 == "SCC\r"' samplist.txt > 2.keep
awk '$2 == "NLI\r"' samplist.txt > 3.keep
awk '$2 == "Bin\r"' samplist.txt > 4.keep
awk '$2 == "Boff\r"' samplist.txt > 6.keep
awk '$2 == "A\r"' samplist.txt > 5.keep

for f in {1..6}
do
	vcftools --vcf DP3g95maf05.recode.vcf --keep $f.keep --missing-site --out $f
done
cat 1.lmiss 2.lmiss 3.lmiss 4.lmiss 5.lmiss 6.lmiss | awk '!/CHR/' | awk '$6 > 0.1' | cut -f1,2 >> badloci
vcftools --vcf DP3g95maf05.recode.vcf --exclude-positions badloci --recode --recode-INFO-all --out DP3g95p5maf05

#Steps 3-7
		#Allele Balance
/programs/vcflib-1.0.1/bin/vcffilter -s -f "AB > 0.2 & AB < 0.8 | AB < 0.01" DP3g95p5maf05.recode.vcf > DP3g95p5maf05.fil1.vcf
		#check how many loci now
awk '!/#/' DP3g95p5maf05.recode.vcf | wc -l && awk '!/#/' DP3g95p5maf05.fil1.vcf | wc -l
	#The next filter we will apply filters out sites that have reads from both strands.
/programs/vcflib-1.0.1/bin/vcffilter -f "SAF / SAR > 100 & SRF / SRR > 100 | SAR / SAF > 100 & SRR / SRF > 100" -s DP3g95p5maf05.fil1.vcf > DP3g95p5maf05.fil2.vcf
awk '!/#/' DP3g95p5maf05.fil2.vcf | wc -l
	#The next filter looks at the ratio of mapping qualities between reference and alternate alleles
	#The rationale here is that, again, because RADseq loci and alleles all should start from the same genomic location there should not be large discrepancy between the mapping qualities of two alleles.
/programs/vcflib-1.0.1/bin/vcffilter -f "MQM / MQMR > 0.9 & MQM / MQMR < 1.05" DP3g95p5maf05.fil2.vcf > DP3g95p5maf05.fil3.vcf
awk '!/#/' DP3g95p5maf05.fil3.vcf | wc -l &
	#Yet another filter that can be applied is whether or not their is a discrepancy in the properly paired status of for reads supporting reference or alternate alleles.
/programs/vcflib-1.0.1/bin/vcffilter -f "PAIRED > 0.05 & PAIREDR > 0.05 & PAIREDR / PAIRED < 1.75 & PAIREDR / PAIRED > 0.25 | PAIRED < 0.05 & PAIREDR < 0.05" -s DP3g95p5maf05.fil3.vcf > DP3g95p5maf05.fil4.vcf			
awk '!/#/' DP3g95p5maf05.fil4.vcf | wc -l &
	#In short, with whole genome samples, it was found that high coverage can lead to inflated locus quality scores. Heng proposed that for read depths greater than the mean depth plus 2-3 times the square root of mean depth that the quality score will be twice as large as the depth in real variants and below that value for false variants.
	#I actually found that this is a little too conservative for RADseq data, likely because the reads aren’t randomly distributed across contigs. I implement two filters based on this idea. the first is removing any locus that has a quality score below 1/4 of the depth.
		#This part is complicated - do it with just all samples first
/programs/vcflib-1.0.1/bin/vcffilter -f "QUAL / DP > 0.25" DP3g95p5maf05.fil4.vcf > DP3g95p5maf05.fil5.vcf
cut -f8 DP3g95p5maf05.fil5.vcf | grep -oe "DP=[0-9]*" | sed -s 's/DP=//g' > DP3g95p5maf05.fil5.DEPTH
awk '!/#/' DP3g95p5maf05.fil5.vcf | cut -f1,2,6 > DP3g95p5maf05.fil5.vcf.loci.qual
awk '{ sum += $1; n++ } END { if (n > 0) print sum / n; }' DP3g95p5maf05.fil5.DEPTH
		54235.7 #mean depth			#his was 1.9k so this is a lot higher
#Now the the mean plus 3X the square root of the mean - done myself in google/wolfram
		54235.7 + 3*sqrt(54235.7) =54934.4
		#Insert that value
paste DP3g95p5maf05.fil5.vcf.loci.qual DP3g95p5maf05.fil5.DEPTH | awk -v x=454934 '$4 > x' | awk '$3 < 2 * $4' > DP3g95p5maf05.fil5.lowQDloci
vcftools --vcf DP3g95p5maf05.fil5.vcf --site-depth --exclude-positions DP3g95p5maf05.fil5.lowQDloci --out DP3g95p5maf05.fil5
cut -f3 DP3g95p5maf05.fil5.ldepth > DP3g95p5maf05.fil5.site.depth
	#Now let’s calculate the average depth by dividing the above file by the number of individuals 31
		#I have 484 individuals
		#Insert number
awk '!/D/' DP3g95p5maf05.fil5.site.depth | awk -v x=484 '{print $1/x}' > meandepthpersite
vcftools --vcf  DP3g95p5maf05.fil5.vcf --recode-INFO-all --out DP3g95p5maf05.FIL --max-meanDP 200 --exclude-positions DP3g95p5maf05.fil5.lowQDloci --recode 

#Step #8
	#Recode to SNPs only and remove indels
/programs/vcflib-1.0.1/bin/vcfallelicprimitives DP3g95p5maf05.FIL.recode.vcf --keep-info --keep-geno > DP3g95p5maf05.prim.vcf
grep -v "#" DP3g95p5maf05.prim.vcf | wc -l 
vcftools --vcf DP3g95p5maf05.prim.vcf --remove-indels --min-alleles 2 --max-alleles 2 --recode --recode-INFO-all --out SNP.DP3g95p5maf05
	#Filter to two alleles only

#Step #9
	#HWE
	#curl -L -O https://github.com/jpuritz/dDocent/raw/master/scripts/filter_hwe_by_pop.pl
	#chmod +x filter_hwe_by_pop.pl
	./filter_hwe_by_pop.pl -v SNP.DP3g95p5maf05.recode.vcf -p samplist.txt -o SNP.DP3g95p5maf05.HWE -h 0.001
	#Separates by popmap so it is okay they they are different species
cp SNP.DP3g95p5maf05.HWE.recode.vcf SNP_HWE_solrefall_0528_basic.vcf

#Step #10
	#Filter to only one per radtag by minor allele frequency
	#Actual calculation
vcftools --vcf SNP.DP3g95p5maf05.HWE.recode.vcf --freq --out freqlist
		#Open freqlist.fq in excel
		#Data by tabs and :
		#Before sorting, measure the minimum between the two (because it isn't minor allele, it's reference vs alternate)
		#Sort by max minor allele frequency then sort by unique contig
			#By removeing duplicates > expand selection > only select column A
		#Export by keeping column A and B without headers
		freq_keep.txt
			#1fprad = 1 (by frequency) per radtag
vcftools --vcf SNP.DP3g95p5maf05.HWE.recode.vcf --positions freq_keep.txt --recode --recode-INFO-all --out SNP_HWE_solrefall_0528_1fprad 

#Step #10.a
	#Filter for LD before 1fp
PROJECTS=(
solrefall_0528
)
for p in ${PROJECTS[@]}
do
echo "$p"
bcftools view -H SNP_"$p"_1fprad.vcf | cut -f 1 | uniq | awk '{print $0"\t"$0}' > "$p"_1fprad.chrom-map.txt
vcftools --vcf SNP_"$p"_1fprad.vcf --chrom-map "$p"_1fprad.chrom-map.txt --out SNP_"$p"_1fprad --plink
awk '{$1=""}1' SNP_"$p"_1fprad.ped > SNP_"$p"_1fprad2.ped
mv SNP_"$p"_1fprad2.ped SNP_"$p"_1fprad.ped
plink --file SNP_"$p"_1fprad --r2 0.80 --noweb --no-sex --allow-no-sex --out "$p"_1fp_pLD
done

for p in ${PROJECTS[@]}
do
#Adjust the below to be about sites so it can more easily be put into vcftools and --exclude-positions
#Pull out the column with one of the two contigs with high LD					#And remove header
awk '{ print $6 }' "$p"_1fp_pLD.ld | awk 'BEGIN { FS = ":" } ; { print $1,$2 }' | tail -n+2 > "$p"_1fp_pLD_chr.txt
vcftools --vcf SNP_"$p"_1fprad.vcf --exclude-positions "$p"_1fp_pLD_chr.txt --recode --out SNP_"$p"_LDprune
done
rename .recode.vcf .vcf *LDprune*


#Step #11
	#Haplotype caller
sed -i "s/Sim/MA/g" samplist.txt

#Put into script
source /programs/miniconda3/bin/activate rad_haplotyper
rad_haplotyper.pl -v SNP_solrefall_0528_basic.vcf -r reference.fasta -x 16 \
-p samplist.txt --genepop haps_solrefall_0528.gp
mv stats.out haps_stats_solrefall_0528.out

sbatch --nodes=1 --ntasks=8 --mem=20000 -o radhaps.out -J haplotype s_radhaps.sh

#Date: Jun 3rd 2022
sbatch --nodes=1 --ntasks=1 --mem=4000 -o B_0603/B5.out -J B_EEMS B5.sh
sbatch --nodes=1 --ntasks=1 --mem=4000 -o A_0603/A5.out -J A_EEMS A5.sh
sbatch --nodes=1 --ntasks=1 --mem=4000 -o simo_0603/simo5.out -J B_EEMS simo5.sh
sbatch --nodes=1 --ntasks=1 --mem=4000 -o simoSNE_0603/simoSNE5.out -J simoSNE_EEMS simoSNE5.sh

#Date: Jun 6th 2022
#For haplotype number, count the number of the commas in the codes
#Sol only
bgzip -c SNP_solonly_basic.vcf > SNP_solonly_basic.vcf.gz
tabix -p vcf SNP_solonly_basic.vcf.gz
bcftools index -s SNP_solonly_basic.vcf.gz

#Date: Jun 7th 2022
#Call monomorphic sites from bam files with freebayes (not ddocent - but do in ddoc folders)
#Do for simonly, solonly and solrefALL
align121_sol
dDoc_0528
simONLYsf1012 > /local/storage/Spisula/GBS/dDoc_928_b/undedup/ #From here

#Date: Jun 8th 2022
#Before calling Freebayes change all the names
	#Remove sneaky solidissima
	#Change MW_017
	rename MW_01. MW_017. *MW_01.*
	rename MW_01- MW_017- *MW_01-RG*
	#Shorten GLD and PEC
	rename PEC0819 PEC PEC0819*
	rename GLD0819 GLD GLD0819*
	#NOTE - these where changed in the file names only, not in the vcf or any lists that dDocent might use so that could confuse it but freebayes I'll do so it might work.
#How to use freebayes to call vcf monomorphic

# set environment
export PATH=/programs/freebayes-1.3.5/bin:$PATH
export PATH=/programs/freebayes-1.3.5/scripts:$PATH
export PATH=/programs/vcflib-1.0.1/bin:$PATH
export PATH=/programs/vcflib-1.0.1/scripts:$PATH
# To run freebayes-parallel (use 24 cores in this example, ref.fa.fai can be generated with command "samtools faidx ref.fa"):
freebayes-parallel <(fasta_generate_regions.py ref.fa.fai 100000) 24 -f ref.fa aln.bam >out.vcf
## The script freebayes-parallel call freebayes with default setting. 
#If you need to change the parameter to run freebayes, you can copy the freebayes-parallel to your workding directory:  
cp /programs/freebayes-1.3.5/scripts/freebayes-parallel ./  .
#Then you can modify the line command=("freebayes" "$@") to add other parameters.  Testing on a small file before you run on the real job.

#Or run regular without paralele
freebayes

#Multiple BAM files may be given for joint calling.
#So I just put them all in a line? How do I do R1 and R2? <- not a problem - all in one bam

#The fasta file output will still have to be concatenated...?

--report-monomorphic
#This works and this WOULD let me just combine them later since all should have all the same sites
freebayes -f reference.fasta GA12_017-RG.bam --report-monomorphic > testmono_GA12_017.vcf


#Not relevant right now but interesting possible command to get haplotypes/fasta from vcf?
    # generate long haplotype calls over known variants
    freebayes -f ref.fa --haplotype-basis-alleles in.vcf.gz \ 
                        --haplotype-length 50 aln.bam
                        
#Maybe parallele over the contigs but call all at once so I can just concatennate the bottoms. Sure
grep ">" reference.fasta > listofcontigs_simref.txt
sed -i "s/>//g" listofcontigs_simref.txt
#shorter test list: head listofcontigs_simref.txt > headcontigs.txt
mkdir bayescalls_bycontig
#make a list of all bam files
#make a list all on one line
#Make a list on one list divided by spaces
ls *_*.bam > tmpbam.txt # _ is to remove any ones not specific to samples
tr '\n' ' ' < tmpbam.txt > bamlist.txt #replace new line with space
b=$(cat bamlist.txt)

while read p; do
echo "echo "$p""
echo "freebayes -f reference.fasta $b -r $p --report-monomorphic > bayescalls_bycontig/mono_$p.vcf"
done < listofcontigs_simref.txt > j_free.txt

#This takes freaking forever. Even to just call one contig. Also, already with some of the default parameter settings, its not doing every base, I think because of missing data. I guess I can still filter by missing data though.

#Remove the header as part of it and just do one later with the header
#Freebayes takes a LOT of memory, I'm not going to be able to do this on the server with everything hongang is doing

#Get another server
manage_slurm new cbsumm12
#Takes about 3 minutes to set up
/programs/bin/labutils/mount_server cbsuhare /storage
cp /fs/cbsuhare/storage/Spisula/Mounting/EEMS/ . -r

#Set up script
#Freebayes memory keeps growing as it builds the thing. 15% of 250G by base 194
#This is how long the contig is...
echo "NTGTGCAGATGCAGAATCATATGCAGTGTATATATTCTCCATACTCAGACAGGCGGTTAATAACTTCTGATTACAAATGCCACTGATTTTGCCAAATTTCCAGATTTCTCTCACAAAACTCGTAGGTCCAGTCCCATTTAACCCATGGTATCAAGCAGGCNNNNNNNNNNCTACCACTCTCACCATACTCTCACATCATAATCAATCAGTATACCATGTTTCATTGATGTACCTTGGAACGGTTTTTGAGAAATTCTCTAAAATAGAAACCTGCCAAAAACTTTAACGTAAAAACCAATGCAATTGCCAACACCGACACCCN" > n
#323 bases (assume 330)
#37.5 G / 194 bases = x G / 330 bases = 64 G for 330 bases - this server only has 126G memory, so I can only run two at once?
#What happens when freebayes runs out of memory?
#dDocent restarts it better, how do I do that?
#17.6%*250 / 204 bases = x G / 330 bases = 71 G for 330 bases - oooph, it gets better and better eh?

#Okay, try running just the first ten with some of the recommendations
while read p; do
echo "echo "$p""
echo "freebayes -f reference.fasta -r $p -g 20000 $b --report-monomorphic --use-best-n-alleles 2 > bayescalls_bycontig/mono_$p.vcf"
done < headcontigs.txt > j_freehead.txt
#Make separate script so it finishes first
echo "echo Removing Headers" > j_removeheaders.txt
while read p; do
echo "grep -v "##" bayescalls_bycontig/mono_$p.vcf > bayescalls_bycontig/mononh_$p.vcf"
done < headcontigs.txt >> j_removeheaders.txt
while read p; do
echo "rm bayescalls_bycontig/mono_$p.vcf"
done < headcontigs.txt >> j_removeheaders.txt

#Memory recommendations
--use-best-n-alleles 4 #for higher ploidy but I could use --use-best-n-alleles 2 to just use the best two options rather than a multimorphic single nucleotide
#Remove --genotype-qualities: calculating genotype qualities requires O(samples*genotypes) memory.
	#Is this the Q that I'm filtering by later? If so I would not want to remove.
		#No, but I'm also already not including it
#Set higher input thresholds. Require that N reads in one sample support an allele in order to consider it: --min-alternate-count N, or that the allele fraction in one sample is M: --min-alternate-fraction M. This will filter noisy alleles. The defaults, --min-alternate-count 2 --min-alternate-fraction 0.2, are most-suitable for diploid, moderate-to-high depth samples, and should be changed when working with different ploidy samples.
--min-alternate-fraction 0.05 #default is 0.05 - I'm not ready to sacrifice this cause I'm trying to detect diversity
#Higher depth areas will take more time and memory
#Set maximum depth?
#Skip over regions of high depth by discarding alignments overlapping positions where total read depth is greater than 200:
	freebayes -f ref.fa -g 2000

#HOLY MOLY! the -g 200 has a huge impact
#How much am I missing? get an estimation of average read depth in the bams?
samtools depth GLD_*.bam > temp.txt
	#Yeah... most of my depth is incredibly high - from several hundred to 30k
	#Plot
	#I can cut mine off around 1,000, lets do 2,000 to be safe
	#Many of the -g 200 files were empty cause no loci had that I guess but also had greater depth than 0
	#Even with the 2000, it still goes much faster and with SO MUCH less memory
		#I could run 50 in paralelle easy. <- EH, maybe not, some of the jobs will take more. (Above 5% memory)
		#But now they are all coming out to be relatively similar in file size across vcfs
		#Wait a second, removing the header got rid of everything still... 
		#What's going on on some of these contigs?
		#Well... contig 1 is either basically zero or >16k
		#What about 20000k?
		#Still runs decently fast

while read p; do
echo "echo "$p""
echo "freebayes -f reference.fasta -r $p -g 20000 $b --report-monomorphic --use-best-n-alleles 4 > bayescalls_bycontig/mono_$p.vcf"
done < listofcontigs_simref.txt > j_free.txt
#Make separate script so it finishes first
echo "echo Removing Headers" > j_removeheaders.txt
while read p; do
echo "grep -v "##" bayescalls_bycontig/mono_$p.vcf > bayescalls_bycontig/mononh_$p.vcf"
done < listofcontigs_simref.txt >> j_removeheaders.txt
while read p; do
echo "rm bayescalls_bycontig/mono_$p.vcf"
done < listofcontigs_simref.txt >> j_removeheaders.txt


#!/bin/bash -l

#SBATCH --partition=regular
#SBATCH --job-name=temp
#SBATCH --output=temp.out
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --mail-user=hh693@cornell.edu
export PATH=/programs/freebayes-1.3.5/bin:$PATH
export PATH=/programs/freebayes-1.3.5/scripts:$PATH
export PATH=/programs/vcflib-1.0.1/bin:$PATH
export PATH=/programs/vcflib-1.0.1/scripts:$PATH

parallel -j 5 < j_free.txt
parallel -j 20 < j_removeheaders.txt

sbatch --nodes=1 --ntasks=22 --mem=125000 -o fullrun1.out -J fullfree s_free1.sh

freebayes -f reference.fasta -r dDocent_Contig_1 -g 200 $b --report-monomorphic --use-best-n-alleles 4 > bayescalls_bycontig/headeronly.vcf

#Note, changing the names of the bam files did not fix how they were IDed in the vcf so that must be something internal
#I worry rad_haplotyper won't be able to recognize them...

sed -i "s/0819//g" bayescalls_bycontig/headeronly.vcf
#BUT MW is 017... what??? Maybe because when I messed it up, I only messed up the file name?
grep "#" bayescalls_bycontig/headeronly.vcf > bayescalls_bycontig/headeronly1.vcf

cp bayescalls_bycontig/headeronly1.vcf ./headerhapstest.vcf
#Try running radhaplotyper on a subset
ls bayescalls_bycontig/mono_* | head -143 > temp.txt
while read t; do
grep -v "#" $t >> headerhapstest.vcf
done < temp.txt
#Oh actually I should filter it first


#Start deciding on filtering
#Step 1
#Augmented filtering for monomorphic sites too!
vcftools --vcf headerhapstest.vcf --max-missing 0.5 --mac 3 --minQ 30 --recode --recode-INFO-all --out raw.g5mac3
#After filtering, kept 6 out of a possible 183 Sites
#Did this on a subset of the data
#Check, are any of those sites monomorphic?
#No:
grep -v "#" raw.g5mac3.recode.vcf | grep "0/0" | wc -l
6
grep -v "#" raw.g5mac3.recode.vcf | grep "0/1" | wc -l
6
#All sites have atleast one heterozygote
#Try with an expanded set
#After filtering, kept 30 out of a possible 1015 Sites
grep -v "#" raw.g5mac3.recode.vcf | grep "0/0" | wc -l
22
grep -v "#" raw.g5mac3.recode.vcf | grep "0/1" | wc -l
27
#Uh... that's not good either... What?
#There are several sites with only 1/2s which could have to do with the --use-best-n-alleles because there's something more common than the reference
grep -v "#" raw.g5mac3.recode.vcf | grep "0/0" | grep -v "0/1" | wc -l
1
#OOOOH
#Double check:
grep -v "#" raw.g5mac3.recode.vcf | grep "0/0" | grep -v "0/1" | grep -v "1/1" | wc -l
0
#Nope. No monomorphic sites remain

#Try another option
#Generating AllSites VCFs using BCFtools (mpileup/call)
#BCFtools mpileup can be used to produce genotype likelihoods, and this operation can be followed by bcftools call to call SNPs/INDELS. BCFtools offers a number of flexible options documented here: https://samtools.github.io/bcftools/bcftools-man.html#call
#In this example, we call mpileup and pipe the output to call variants and generate and AllSites VCF:

bcftools mpileup -f <reference.fa> -b <bamlist.txt> -r <X> | bcftools call -m -Oz -f GQ -o <output>
#Notes on the options selected here:
#b points mpileup to a list of BAM files contained in a text file.
#r specifies the genomic region. In this example, we specify the X chromosome, but mpileup provides a variety of options for specifying the region (CHR|CHR:POS|CHR:FROM-TO|CHR:FROM-[,…]). Alternatively, -R <file> can be used to read regions from a provided file.
#Oz specifies compressed VCF output
#f GQ indicates that the FORMAT field GQ should be output for each sample

bcftools mpileup -f reference.fasta -b bamlonglist.txt -r dDocent_Contig_1 | bcftools call -m -Oz -f GQ -o temp_bcfcall.vcf

#All interesting notes:
#[mpileup] 150 samples in 131 input files
#[mpileup] maximum number of reads per input file set to -d 250
#Note: The maximum per-sample depth with -d 250 is 218.3x
#Also when the site is 0/0, doesn't output anything else...
grep -v "#" temp_bcfcall.vcf | wc -l
333
grep -v "#" bayescalls_bycontig/mono_dDocent_Contig_1.vcf | wc -l
21
#Also interesting, clearly keeps all of the sites (or at least more of them)
#Already better. I think the -d means that it subsets rather than ignoring which is also nice

#Test in vcftools
vcftools --vcf temp_bcfcall.vcf --max-missing 0.5 --minQ 30 --recode --recode-INFO-all --out raw.g5mac3
#After filtering, kept 22 out of a possible 333 Sites
#Should NOT be filtering for mac? Cause I want ones with zero mac
grep -v "#" raw.g5mac3.recode.vcf | grep "0/0" | wc -l
21
grep -v "#" raw.g5mac3.recode.vcf | grep "0/0" | grep -v "0/1" | grep -v "1/1" | wc -l
5
#WOOT! They in there!
#Noice!


#Set up to run on bcf
mkdir bcf_bycontig

while read p; do
echo "echo "$p""
echo "bcftools mpileup -f reference.fasta -b bamlonglist.txt -d 20000 -r $p | bcftools call -m -Oz -f GQ -o bcf_bycontig/bcf_$p.vcf"
done < listofcontigs_simref.txt > j_bcf.txt
#Make separate script so it finishes first
echo "echo Removing Headers" > j_removeheaders.txt
while read p; do
echo "grep -v "##" bcf_bycontig/bcf_$p.vcf > bcf_bycontig/bcfnh_$p.vcf"
done < listofcontigs_simref.txt >> j_removeheaders.txt
while read p; do
echo "rm bcf_bycontig/bcf_$p.vcf"
done < listofcontigs_simref.txt >> j_removeheaders.txt

#!/bin/bash -l

#SBATCH --partition=regular
#SBATCH --job-name=temp
#SBATCH --output=temp.out
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --mail-user=hh693@cornell.edu
parallel -j 20 < j_bcf.txt

echo "Merging vcfs"
bcftools mpileup -f reference.fasta -b bamlonglist.txt -r dDocent_Contig_1 | bcftools call -m -Oz -f GQ -o bcf_bycontig/headeronly.vcf
grep "#" bcf_bycontig/headeronly.vcf > headerhapstest.vcf
while read p; do
grep -v "#" bcf_bycontig/bcf_$p.vcf >> headerhapstest.vcf
done < listofcontigs_simref.txt

sbatch --nodes=1 --ntasks=22 --mem=125000 -o bcfrun2.out -J bcf s_bcf2.sh

#That is darn near aggressively fast
bcftools mpileup -f reference.fasta -b bamlonglist.txt -r dDocent_Contig_1 | bcftools call -m -Oz -f GQ -o bcf_bycontig/headeronly.vcf

#Could increase -d?
#After ~3 min 30 sec, there were 319 in the folder
#Then remove them and try with -d 20000
#Seems like it is still going very fast
#Hhhhummmm, it warns about memory hogs though in the .out file (but it didn't give that for all)
	#And it doesn't look like any one job had taken up more than 2%
#After 4 min, there with 79 in the folder. It's slower but still FINE and so much better than freebayes
wc -l listofcontigs_simref.txt
3054
#Means it will finish in 172 minutes
#I can then repeat with solidissima only and solrefALL


#Date: June 9th 2022
#Continue monomorphic sites creation for other datasets
cp /fs/cbsuhare/storage/Spisula/Mounting/freebayes_0608/dDoc_0528/*_*.bam* solrefall &
cp /fs/cbsuhare/storage/Spisula/Mounting/freebayes_0608/align121_sol/*_*.bam* solonly &
cp /fs/cbsuhare/storage/Spisula/Mounting/freebayes_0608/dDoc_0528/reference* solrefall &
cp /fs/cbsuhare/storage/Spisula/Mounting/freebayes_0608/align121_sol/reference* solonly &

#Prep list of contigs
grep ">" reference.fasta > listofcontigs_solref.txt
sed -i "s/>//g" listofcontigs_solref.txt
#Shorten GLD and PEC
rename PEC0819 PEC sol*/PEC0819*
rename GLD0819 GLD sol*/GLD0819*
rename SsLI SL SsLI*
#Remove 
	#sneaky similis from solonly
	#rm NAN_004*
	#rm NAN_005*
	#rm PVT_008H07*
#List of bams
ls *.bam > bamlonglist.txt

#Set up to run on bcf
mkdir bcf_bycontig

while read p; do
echo "echo "$p""
echo "bcftools mpileup -f reference.fasta -b bamlonglist.txt -d 20000 -r $p | bcftools call -m -Oz -f GQ -o bcf_bycontig/bcf_$p.vcf"
done < listofcontigs_solref.txt > j_bcf.txt
#Make separate script so it finishes first
echo "echo Removing Headers" > j_removeheaders.txt
while read p; do
echo "grep -v "##" bcf_bycontig/bcf_$p.vcf > bcf_bycontig/bcfnh_$p.vcf"
done < listofcontigs_solref.txt >> j_removeheaders.txt
while read p; do
echo "rm bcf_bycontig/bcf_$p.vcf"
done < listofcontigs_solref.txt >> j_removeheaders.txt

#Get another server
#Make slurm cluster
manage_slurm new cbsumm12
#Takes about 3 minutes to set up
/programs/bin/labutils/mount_server cbsuhare /storage
cp /fs/cbsuhare/storage/Spisula/Mounting/EEMS/ . -r

sed -i "s/0819//g" bayescalls_bycontig/headeronly.vcf

#!/bin/bash -l

#SBATCH --partition=regular
#SBATCH --job-name=temp
#SBATCH --output=temp.out
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --mail-user=hh693@cornell.edu
parallel -j 11 < j_bcf.txt

echo "Merging vcfs"
bcftools mpileup -f reference.fasta -b bamlonglist.txt -r dDocent_Contig_5 | bcftools call -m -Oz -f GQ -o bcf_bycontig/headeronly.vcf
grep "#" bcf_bycontig/headeronly.vcf > headerhapstest.vcf
while read p; do
grep -v "#" bcf_bycontig/bcf_$p.vcf >> headerhapstest.vcf
done < listofcontigs_solref.txt

sbatch --nodes=1 --ntasks=11 --mem=60000 -o bcfrun_only.out -J bcf_only s_bcf_only.sh

#While that is running...
#Start setting up the filtering protocol

#May not be working because it doesn't output enough information for filtering... Instead I could try GATK

#Start filtering from original vcfs
#Just start from my original alignment vcfs but alter the filtering for not limiting maf

#View samples
grep "#CHROM" #422_014 <- this has 422 in it...
awk -v OFS="\t" '$1=$1' samplist.txt > samplist1.txt
mv samplist1.txt samplist.txt

#But my stuff is only 413...? Right?
sed -i "s/0819//g" simonlyRawSNPs.vcf
sed -i "s/422/413/g" simonlyRawSNPs.vcf
sed -i "s/SsLI/SL/g" simonlyRawSNPs.vcf

#Remove sneaky solidissima
awk '$2 == "GA"' samplist.txt | awk '{ print $1 }' > 1.keep
awk '$2 == "MA"' samplist.txt | awk '{ print $1 }' > 2.keep
awk '$2 == "NLI"' samplist.txt | awk '{ print $1 }' > 3.keep
awk '$2 == "Bin"' samplist.txt | awk '{ print $1 }' > 4.keep
awk '$2 == "Boff"' samplist.txt | awk '{ print $1 }' > 6.keep
awk '$2 == "A"' samplist.txt | awk '{ print $1 }' > 5.keep
#Used to be awk '$2 == "A\r"' samplist.txt > 5.keep - but doesn't work
vcftools --vcf simonlyRawSNPs.vcf --remove 4.keep --remove 5.keep --remove 6.keep --recode --recode-INFO-all --out simonly_f1

#Start filtering
#Augmented filtering for monomorphic sites
#Step 1
vcftools --vcf simonly_f1.recode.vcf --max-missing 0.5 --minQ 30 --recode --recode-INFO-all --out raw.g5mac3
vcftools --vcf raw.g5mac3.recode.vcf --minDP 3 --recode --recode-INFO-all --out raw.g5mac3dp3 
vcftools --vcf raw.g5mac3dp3.recode.vcf --missing-indv
awk '$5>0.5' out.imiss | cut -f1 > lowDP.indv
vcftools --vcf raw.g5mac3dp3.recode.vcf --remove lowDP.indv --recode --recode-INFO-all --out raw.g5mac3dplm
#Step 1.5
vcftools --vcf raw.g5mac3dplm.recode.vcf --min-meanDP 20 --recode --recode-INFO-all --out DP3g95maf05
#Step 2
	#Do not do population specific checks?
for f in {1..6}
do
	vcftools --vcf DP3g95maf05.recode.vcf --keep $f.keep --missing-site --out $f
done
cat 1.lmiss 2.lmiss 3.lmiss 4.lmiss 5.lmiss 6.lmiss | awk '!/CHR/' | awk '$6 > 0.1' | cut -f1,2 >> badloci
vcftools --vcf DP3g95maf05.recode.vcf --exclude-positions badloci --recode --recode-INFO-all --out DP3g95p5maf05
	#mv DP3g95maf05.recode.vcf DP3g95p5maf05.recode.vcf
	#UH OH, MAY NOT HAVE BEEN USING p5 AS THE INPUT FOR STEP 3
#Step 3
	#Do not do Allele Balance
	#/programs/vcflib-1.0.1/bin/vcffilter -s -f "AB > 0.2 & AB < 0.8 | AB < 0.01" DP3g95p5maf05.recode.vcf > DP3g95p5maf05.fil1.vcf
	mv DP3g95p5maf05.recode.vcf DP3g95p5maf05.fil1.vcf
#Step 4
/programs/vcflib-1.0.1/bin/vcffilter -f "SAF / SAR > 100 & SRF / SRR > 100 | SAR / SAF > 100 & SRR / SRF > 100" -s DP3g95p5maf05.fil1.vcf > DP3g95p5maf05.fil2.vcf
awk '!/#/' DP3g95p5maf05.fil2.vcf | wc -l
#Step 5
/programs/vcflib-1.0.1/bin/vcffilter -f "MQM / MQMR > 0.9 & MQM / MQMR < 1.05" DP3g95p5maf05.fil2.vcf > DP3g95p5maf05.fil3.vcf &
awk '!/#/' DP3g95p5maf05.fil3.vcf | wc -l &
#Step 6
/programs/vcflib-1.0.1/bin/vcffilter -f "PAIRED > 0.05 & PAIREDR > 0.05 & PAIREDR / PAIRED < 1.75 & PAIREDR / PAIRED > 0.25 | PAIRED < 0.05 & PAIREDR < 0.05" -s DP3g95p5maf05.fil3.vcf > DP3g95p5maf05.fil4.vcf &	
awk '!/#/' DP3g95p5maf05.fil4.vcf | wc -l &
#Step 7
/programs/vcflib-1.0.1/bin/vcffilter -f "QUAL / DP > 0.25" DP3g95p5maf05.fil4.vcf > DP3g95p5maf05.fil5.vcf
cut -f8 DP3g95p5maf05.fil5.vcf | grep -oe "DP=[0-9]*" | sed -s 's/DP=//g' > DP3g95p5maf05.fil5.DEPTH
awk '!/#/' DP3g95p5maf05.fil5.vcf | cut -f1,2,6 > DP3g95p5maf05.fil5.vcf.loci.qual
awk '{ sum += $1; n++ } END { if (n > 0) print sum / n; }' DP3g95p5maf05.fil5.DEPTH
	22926.1 #mean depth
	22926.1 + 3*sqrt(22926.1) = 23380
		#Insert that value
paste DP3g95p5maf05.fil5.vcf.loci.qual DP3g95p5maf05.fil5.DEPTH | awk -v x=23380 '$4 > x' | awk '$3 < 2 * $4' > DP3g95p5maf05.fil5.lowQDloci
vcftools --vcf DP3g95p5maf05.fil5.vcf --site-depth --exclude-positions DP3g95p5maf05.fil5.lowQDloci --out DP3g95p5maf05.fil5
cut -f3 DP3g95p5maf05.fil5.ldepth > DP3g95p5maf05.fil5.site.depth
		#Insert number of individuals
awk '!/D/' DP3g95p5maf05.fil5.site.depth | awk -v x=132 '{print $1/x}' > meandepthpersite
vcftools --vcf  DP3g95p5maf05.fil5.vcf --recode-INFO-all --out DP3g95p5maf05.FIL --max-meanDP 200 --exclude-positions DP3g95p5maf05.fil5.lowQDloci --recode 
#Step 8
	#Do not recode SNPs or remove indels
#Step 9
	#Do not filter for HWE
#Step 10
	#Do not do LD at 1pf
#Step #11
	#Haplotype caller
cp DP3g95p5maf05.FIL.recode.vcf simonly_mono_basic.vcf

mkdir /fs/cbsuhare/storage/Spisula/Mounting/unmount_monomorph
cp simonly_mono_basic.vcf /fs/cbsuhare/storage/Spisula/Mounting/unmount_monomorph

#If I call the FSTs myself:
#Combine this with the output from the bcftools output which calls what is relevant contigs (do not include contigs not included in the bcftools one)
#Try rad_haplotyper and see what ima looks like
	#On main server
source /programs/miniconda3/bin/activate rad_haplotyper
rad_haplotyper.pl -v simonly_mono_basic.vcf -r reference.fasta -x 8 \
-p samplist.txt --a haps_simonly_mono_0609ima.txt --genepop haps_simonly_mono_0609gp.txt
mv stats.out haps_stats_simonly_mono_0609.out
#Looks like it's working, but it does ask for the individuals that were in the vcf but not in the current file

simonly_mono_basic.vcf

#Note: bcf is taking a LOT longer for solidissima ~2.5 hour per 800 contigs with 62.5k total contigs
	#It will take 8 days...
	#Guess it is a good thing I'm not trying to do that then...

#Put into script
source /programs/miniconda3/bin/activate rad_haplotyper
rad_haplotyper.pl -v SNP_solrefall_0528_basic.vcf -r reference.fasta -x 16 \
-p samplist.txt --genepop haps_solrefall_0528.gp
mv stats.out haps_stats_solrefall_0528.out

sbatch --nodes=1 --ntasks=8 --mem=20000 -o radhaps.out -J haplotype s_radhaps.sh

#Shorten GLD and PEC
rename PEC0819 PEC sol*/PEC0819*
rename GLD0819 GLD sol*/GLD0819*
rename SsLI SL SsLI*
#Remove 
	#sneaky similis from solonly
	#rm NAN_004*
	#rm NAN_005*
	#rm PVT_008H07*

#ADD TO README FILE WHICH RUN/FILTERING EACH RENAMMED DATASET IS FROM

#Date: Jun 10th
#Great, server is totally clear
#View samples
grep "#CHROM" #422_014 <- this has 422 in it...
awk -v OFS="\t" '$1=$1' samplist.txt > samplist1.txt
mv samplist1.txt samplist.txt

#But my stuff is only 413...? Right?
sed -i "s/0819//g" Raw_sim.vcf &
sed -i "s/422/413/g" Raw_sim.vcf 
sed -i "s/SsLI/SL/g" Raw_sim.vcf &

sed -i "s/0819//g" samplist.txt &
sed -i "s/422/413/g" samplist.txt &
sed -i "s/SsLI/SL/g" samplist.txt &

#Remove sneaky solidissima
awk '$2 == "GA"' samplist.txt | awk '{ print $1 }' > 1.keep
awk '$2 == "MA"' samplist.txt | awk '{ print $1 }' > 2.keep
awk '$2 == "NLI"' samplist.txt | awk '{ print $1 }' > 3.keep
awk '$2 == "Bin"' samplist.txt | awk '{ print $1 }' > 4.keep
awk '$2 == "Boff"' samplist.txt | awk '{ print $1 }' > 6.keep
awk '$2 == "A"' samplist.txt | awk '{ print $1 }' > 5.keep
#Used to be awk '$2 == "A\r"' samplist.txt > 5.keep - but doesn't work
vcftools --vcf Raw_sim.vcf --remove 4.keep --remove 5.keep --remove 6.keep --recode --recode-INFO-all --out simonly_f1
vcftools --vcf Raw_sol.vcf --remove 1.keep --remove 2.keep --remove 3.keep --recode --recode-INFO-all --out solonly_f1


#Start filtering
#Augmented filtering for monomorphic sites
#Step 1
vcftools --vcf simonly_f1.recode.vcf --max-missing 0.5 --minQ 30 --recode --recode-INFO-all --out raw.g5mac3
vcftools --vcf raw.g5mac3.recode.vcf --minDP 3 --recode --recode-INFO-all --out raw.g5mac3dp3 
vcftools --vcf raw.g5mac3dp3.recode.vcf --missing-indv
awk '$5>0.5' out.imiss | cut -f1 > lowDP.indv
vcftools --vcf raw.g5mac3dp3.recode.vcf --remove lowDP.indv --recode --recode-INFO-all --out raw.g5mac3dplm
#Step 1.5
vcftools --vcf raw.g5mac3dplm.recode.vcf --min-meanDP 20 --recode --recode-INFO-all --out DP3g95maf05
#Step 2
	#Do not do population specific checks?
for f in {1..6}
do
	vcftools --vcf DP3g95maf05.recode.vcf --keep ../$f.keep --missing-site --out $f
done
cat 1.lmiss 2.lmiss 3.lmiss 4.lmiss 5.lmiss 6.lmiss | awk '!/CHR/' | awk '$6 > 0.1' | cut -f1,2 >> badloci
vcftools --vcf DP3g95maf05.recode.vcf --exclude-positions badloci --recode --recode-INFO-all --out DP3g95p5maf05
	#mv DP3g95maf05.recode.vcf DP3g95p5maf05.recode.vcf
	#UH OH, MAY NOT HAVE BEEN USING p5 AS THE INPUT FOR STEP 3
#Step 3
	#Do not do Allele Balance
	#/programs/vcflib-1.0.1/bin/vcffilter -s -f "AB > 0.2 & AB < 0.8 | AB < 0.01" DP3g95p5maf05.recode.vcf > DP3g95p5maf05.fil1.vcf
	mv DP3g95p5maf05.recode.vcf DP3g95p5maf05.fil1.vcf
#Step 4
/programs/vcflib-1.0.1/bin/vcffilter -f "SAF / SAR > 100 & SRF / SRR > 100 | SAR / SAF > 100 & SRR / SRF > 100" -s DP3g95p5maf05.fil1.vcf > DP3g95p5maf05.fil2.vcf
awk '!/#/' DP3g95p5maf05.fil2.vcf | wc -l
#Step 5
/programs/vcflib-1.0.1/bin/vcffilter -f "MQM / MQMR > 0.9 & MQM / MQMR < 1.05" DP3g95p5maf05.fil2.vcf > DP3g95p5maf05.fil3.vcf
awk '!/#/' DP3g95p5maf05.fil3.vcf | wc -l &
#Step 6
/programs/vcflib-1.0.1/bin/vcffilter -f "PAIRED > 0.05 & PAIREDR > 0.05 & PAIREDR / PAIRED < 1.75 & PAIREDR / PAIRED > 0.25 | PAIRED < 0.05 & PAIREDR < 0.05" -s DP3g95p5maf05.fil3.vcf > DP3g95p5maf05.fil4.vcf
awk '!/#/' DP3g95p5maf05.fil4.vcf | wc -l &
#Step 7
/programs/vcflib-1.0.1/bin/vcffilter -f "QUAL / DP > 0.25" DP3g95p5maf05.fil4.vcf > DP3g95p5maf05.fil5.vcf
cut -f8 DP3g95p5maf05.fil5.vcf | grep -oe "DP=[0-9]*" | sed -s 's/DP=//g' > DP3g95p5maf05.fil5.DEPTH
awk '!/#/' DP3g95p5maf05.fil5.vcf | cut -f1,2,6 > DP3g95p5maf05.fil5.vcf.loci.qual
awk '{ sum += $1; n++ } END { if (n > 0) print sum / n; }' DP3g95p5maf05.fil5.DEPTH
	22926.1 #mean depth
	22926.1 + 3*sqrt(22926.1) = 23380
		#Insert that value
paste DP3g95p5maf05.fil5.vcf.loci.qual DP3g95p5maf05.fil5.DEPTH | awk -v x=23380 '$4 > x' | awk '$3 < 2 * $4' > DP3g95p5maf05.fil5.lowQDloci
vcftools --vcf DP3g95p5maf05.fil5.vcf --site-depth --exclude-positions DP3g95p5maf05.fil5.lowQDloci --out DP3g95p5maf05.fil5
cut -f3 DP3g95p5maf05.fil5.ldepth > DP3g95p5maf05.fil5.site.depth
		#Insert number of individuals
awk '!/D/' DP3g95p5maf05.fil5.site.depth | awk -v x=132 '{print $1/x}' > meandepthpersite
vcftools --vcf  DP3g95p5maf05.fil5.vcf --recode-INFO-all --out DP3g95p5maf05.FIL --max-meanDP 200 --exclude-positions DP3g95p5maf05.fil5.lowQDloci --recode 
#Step 8
	#Do not recode SNPs or remove indels
#Step 9
	#Do not filter for HWE
#Step 10
	#Do not do LD at 1pf
#Step #11
	#Haplotype caller
cp DP3g95p5maf05.FIL.recode.vcf ../simonly_mono.vcf


#Turn this into a script for sol and solref so I can set and forget it
#Up through step 7

#Change the names of the bam files ect with rad haplotyper for solidissima
#ls to file
#do in excel
sbatch --nodes=1 --ntasks=1 --mem=10000 -o rename_0612.out -J rename s_renamesol.sh
#sed -i "s/0819//g" newheader.txt
#sed -i "s/422/413/g" newheader.txt
#sed -i "s/SsLI/SL/g" newheader.txt
sbatch --nodes=1 --ntasks=1 --mem=10000 -o rename_all_0612.out -J rename_all s_renameall.sh


#If I call the FASTAs myself:
#Combine this with the output from the bcftools output which calls what is relevant contigs (do not include contigs not included in the bcftools one)
#Try rad_haplotyper and see what ima looks like
	#On main server
source /programs/miniconda3/bin/activate rad_haplotyper
rad_haplotyper.pl -v simonly_mono.vcf -r reference.fasta -x 10 \
-p samplist.txt --a haps_simonly_mono_0610ima.txt --genepop haps_simonly_mono_0610gp.txt
mv stats.out haps_stats_simonly_mono_0610.out
#Looks like it's working, but it does ask for the individuals that were in the vcf but not in the current file

sbatch --nodes=1 --ntasks=10 --mem=20000 -o hapsmono_sim.out -J haps_mono_sim s_haps_mono0610.sh



#Retry with freebayes
#Maybe parallele over the contigs but call all at once so I can just concatennate the bottoms. Sure
grep ">" reference.fasta > listofcontigs_simref.txt
sed -i "s/>//g" listofcontigs_simref.txt
#shorter test list: head listofcontigs_simref.txt > headcontigs.txt
mkdir bayescalls_bycontig
#make a list of all bam files
#make a list all on one line
#Make a list on one list divided by spaces
ls *_*.bam > tmpbam.txt # _ is to remove any ones not specific to samples
	#Manually remove baddies/duplicates
tr '\n' ' ' < tmpbam.txt > bamlist.txt #replace new line with space
b=$(cat bamlist.txt)

export PATH=/programs/freebayes-1.3.5/bin:$PATH
export PATH=/programs/freebayes-1.3.5/scripts:$PATH
export PATH=/programs/vcflib-1.0.1/bin:$PATH
export PATH=/programs/vcflib-1.0.1/scripts:$PATH
while read p; do
echo "echo "$p""
echo "freebayes -r $p -f reference.fasta --min-coverage 20 --limit-coverage 1000 -g 20000 -n 4 -L tmpbam.txt --report-monomorphic > bayescalls_bycontig/mono_$p.vcf"
done < listofcontigs_simref.txt > j_free.txt
while read p; do
echo "grep -v "##" bayescalls_bycontig/mono_$p.vcf > bayescalls_bycontig/mononh_$p.vcf"
done < listofcontigs_simref.txt > j_removeheaders.txt
#  --limit-coverage N    Downsample per-sample coverage to this level if greater than this coverage. default: no limit


#!/bin/bash -l

#SBATCH --partition=regular
#SBATCH --job-name=temp
#SBATCH --output=temp.out
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --mail-user=hh693@cornell.edu
export PATH=/programs/freebayes-1.3.5/bin:$PATH
export PATH=/programs/freebayes-1.3.5/scripts:$PATH
export PATH=/programs/vcflib-1.0.1/bin:$PATH
export PATH=/programs/vcflib-1.0.1/scripts:$PATH

parallel -j 10 < j_free.txt

parallel -j 10 < j_removeheaders.txt
while read p; do
rm bayescalls_bycontig/mono_$p.vcf
done < listofcontigs_simref.txt

#Get header
freebayes -r dDocent_Contig_1 -f reference.fasta --min-coverage 20 --limit-coverage 1000 -g 20000 -n 4 -L tmpbam.txt --report-monomorphic > bayescalls_bycontig/mono_dDocent_Contig_1_4head.vcf
grep "##" bayescalls_bycontig/mono_dDocent_Contig_1_4head.vcf > fullcall_sol.vcf

#Add others to the end
while read p; do
bayescalls_bycontig/mononh_$p.vcf >> fullcall_sol.vcf
done < listofcontigs_simref.txt

sbatch --nodes=1 --ntasks=10 --mem=85000 -o fullrun2.out -J simfree s_free2.sh
sbatch --nodes=1 --ntasks=10 --mem=85000 -o fullrun2_sol.out -J solfree s_free2_sol.sh

#Subset the solidissima based on depth
samtools depth MCX_031-RG.bam CTM_008-RG.bam WV5_009-RG.bam PT_001-RG.bam SL3109_3-RG.bam GBE_521-RG.bam NAN_040-RG.bam 413_027-RG.bam BAR_006-RG.bam > depths.txt
#Very big file
#Filter to one per contig (depths are similar throughout contig)
	#sort -k1,1 --stable --unique depths.txt > tempdepth.txt
	#Sort takes a while, try awk first
#Filter to ones where each has depth >20
awk '$3>20' depths.txt | awk '$4>20' | awk '$5>20' | awk '$6>20' | awk '$7>20' | awk '$8>20' | awk '$11>20' > tempdepth2.txt
#79340
awk '{print $1}' tempdepth2.txt | uniq | wc -l
#9690 not bad
#(with testing all sites 383 #Oh no...)
#as opposed to 62302
#Instead just grab the ones with 

awk '{print $1}' depths.txt | uniq | wc -l #61027
awk '$3>20' depths.txt | awk '$4>20' | awk '$5>20' | awk '$6>20' | awk '$7>20' | awk '$8>20' | awk '$11>20' > tempdepth2.txt
awk '{print $1}' tempdepth2.txt | uniq | wc -l

samtools coverage -b tmpbam_solonly.txt > tempdepth.txt &

cp listofcontigs_simref.txt listofcontigs_solref_true.txt
awk '{print $1}' tempdepth2.txt | uniq > listofcontigs_simref.txt
#Set these 9k gooder contigs as the starting point

while read p; do
echo "echo "$p""
echo "freebayes -r $p -f reference.fasta --min-coverage 20 --limit-coverage 1000 -g 20000 -n 4 -L tmpbam.txt --report-monomorphic > bayescalls_bycontig/mono_$p.vcf"
done < listofcontigs_simref.txt > j_free.txt
while read p; do
echo "grep -v "##" bayescalls_bycontig/mono_$p.vcf > bayescalls_bycontig/mononh_$p.vcf"
done < listofcontigs_simref.txt > j_removeheaders.txt
#  --limit-coverage N    Downsample per-sample coverage to this level if greater than this coverage. default: no limit

sbatch --nodes=1 --ntasks=10 --mem=120000 -o fullrun2_sol.out -J solfree s_free2_sol.sh

#For solonly
#350 in 1:30 = 90 min sol
#750 in 2:22 = 140 min sim
#How many total?
#9600 for solidissima, 3500 for similis
#11 hours for similis
#41 hours for solidissima
#Nasty

#Date: June 13th
#Treemix and Divmigrate with the LD pruned
#Snapper

#Refilter LD for the solrefall
#Done SNP_solrefall_0528_LDprune.vcf
cp /local/storage/Spisula/GBS/treemix/vcf2treemix_2.sh .
#edit to make non-web
#Make cluster file
	#sample_name	sample_name		population
#sed "s/Sim/MA/g" samplist.txt > tmp
#mv tmp samplist.txt
awk '{print $1,$1,$2}' samplist.txt > treeclust_solrefall.txt

bash vcf2treemix_2.sh SNP_solrefall_0528_LDprune.vcf treeclust_solrefall.txt
#vcf2treemix 2 fixed by removing plink --allowchrom and adding "./" before the the python script
#Try letting it do the web thing
#Needed to redownload from the right source

export PATH=/programs/treemix-1.13/bin:$PATH
treemix -i SNP_solrefall_0528_LDprune.treemix.frq.gz -bootstrap -m 0 -se -o out_solrefall/solrefall_bm0
treemix -i SNP_solrefall_0528_LDprune.treemix.frq.gz -bootstrap -global -m 0 -se -o out_solrefall/solrefall_bm0globe
treemix -i SNP_solrefall_0528_LDprune.treemix.frq.gz -bootstrap -m 1 -se -o out_solrefall/solrefall_bm1
treemix -i SNP_solrefall_0528_LDprune.treemix.frq.gz -bootstrap -m 2 -se -o out_solrefall/solrefall_bm2
treemix -i SNP_solrefall_0528_LDprune.treemix.frq.gz -bootstrap -global -m 2 -se -o out_solrefall/solrefall_bm2globe

#Runs but looks weird
#Should I set it to do the two separates?
sed "s/MA/Sim/g" treeclust_solrefall.txt | sed "s/GA/Sim/g" | sed "s/NY/Sim/g" | sed "s/NLI/Sim/g" > treeclust_solmain.txt
sed "s/Boff/Sol/g" treeclust_solrefall.txt | sed "s/Bin/Sol/g" | sed "s/GA/GG/g" | sed "s/MA/MM/g" | sed "s/A/Sol/g" | sed "s/GG/GA/g" | sed "s/MM/MA/g" > treeclust_simmain.txt

bash vcf2treemix_2.sh sra_simmain.vcf treeclust_simmain.txt
bash vcf2treemix_2.sh sra_solmain.vcf treeclust_solmain.txt

treemix -i simmain/sra_simmain.treemix.frq.gz -bootstrap -m 0 -se -root Sol -o out_solrefall/simmain_bm0
treemix -i solmain/sra_solmain.treemix.frq.gz -bootstrap -m 0 -se -root Sim -o out_solrefall/solmain_bm0
treemix -i simmain/sra_simmain.treemix.frq.gz -bootstrap -m 1 -se -root Sol -o out_solrefall/simmain_bm1
treemix -i solmain/sra_solmain.treemix.frq.gz -bootstrap -m 1 -se -root Sim -o out_solrefall/solmain_bm1
treemix -i simmain/sra_simmain.treemix.frq.gz -bootstrap -m 2 -se -root Sol -o out_solrefall/simmain_bm2
treemix -i solmain/sra_solmain.treemix.frq.gz -bootstrap -m 2 -se -root Sim -o out_solrefall/solmain_bm2
treemix -i simmain/sra_simmain.treemix.frq.gz -bootstrap -m 3 -se -root Sol -o out_solrefall/simmain_bm3
treemix -i solmain/sra_solmain.treemix.frq.gz -bootstrap -m 3 -se -root Sim -o out_solrefall/solmain_bm3

treemix -i simmain/sra_simmain.treemix.frq.gz -bootstrap -m 1 -se -root Sol -globe -o out_solrefall/simmain_bm1globe
treemix -i solmain/sra_solmain.treemix.frq.gz -bootstrap -m 1 -se -root Sim -globe -o out_solrefall/solmain_bm1globe
treemix -i simmain/sra_simmain.treemix.frq.gz -bootstrap -m 2 -se -root Sol -globe -o out_solrefall/simmain_bm2globe
treemix -i solmain/sra_solmain.treemix.frq.gz -bootstrap -m 2 -se -root Sim -globe -o out_solrefall/solmain_bm2globe
treemix -i simmain/sra_simmain.treemix.frq.gz -bootstrap -m 3 -se -root Sol -globe -o out_solrefall/simmain_bm3globe
treemix -i solmain/sra_solmain.treemix.frq.gz -bootstrap -m 3 -se -root Sim -globe -o out_solrefall/solmain_bm3globe


#Date: June 16th
#Do not need depth filter with missingness <0
#Do not do the solrefall - but try to make the sim and sol as comparable as possible
	#Since heterozygosity can be sensitive to number of individuals and loci
		#For diversity, down sample each population sample to the same number of individuals
		#Same number of loci
		#Filter to # of individuals first and then by no missing data
		#A vs B in, B off, GA, NLI, MA
			#Subsample all to 13 to match NLI
				#As long as I have several hundred loci
				#Allelic diversity from arl (compare to haplotype richness)
		#Higher missing data within monomorphic sites
		#Subset samples BEFORE calling freebayes - by highest coverage
#Do apply upper bound depth (cause paralogs)

#Haplotype Richness with equalized sample sizes

#Recreate the bcftools with just the minimum number
	#13 for similis and all
	#74 is minimum for solidissima by A/Boff/Bin
		#WAIT BUT TO COMPARE BETWEEN THEM, I MAY ALSO WANT 13 for diversity
	#Also maybe do with solidissima by region too
	
dDoc_928_similisoonly
align121_sol

#Find the highest coverage from bam files
samtools coverage -b depthinfo/GAbamlist.txt > depthinfo/GA_depths.txt &
samtools coverage -b depthinfo/MAbamlist.txt > depthinfo/MA_depths.txt &
#Wait, nope, this gives coverage per contig...
#What gives coverage per sample?
samtools depth  MW_044-RG.bam  |  awk '{sum+=$3} END { print "Average = ",sum/NR}'
#Turn into while loop

while read p; do 
#echo $p
samtools depth $p |  awk -v awkvar="$p" '{sum+=$3} END { print awkvar," ",sum/NR}'
done < depthinfo/GAbamlist.txt > depthinfo/GA_sampdepths.txt &
#In order to use variable inside awk, you have to set it within awk

while read p; do 
#echo $p
samtools depth $p |  awk -v awkvar="$p" '{sum+=$3} END { print awkvar," ",sum/NR}'
done < depthinfo/MAbamlist.txt > depthinfo/MA_sampdepths.txt &

#Also would be good to know for NLI
nano depthinfo/NLIbamlist.txt
while read p; do 
samtools depth $p |  awk -v awkvar="$p" '{sum+=$3} END { print awkvar," ",sum/NR}'
done < depthinfo/NLIbamlist.txt > depthinfo/NLI_sampdepths.txt &

#All NLI samples
GLD_001-RG.bam
GLD_002-RG.bam
GLD_003-RG.bam
GLD_004-RG.bam
GLD_005-RG.bam
GLD_006-RG.bam
GLD_007-RG.bam
GLD_008-RG.bam
PEC_011-RG.bam
PEC_012-RG.bam
PEC_013-RG.bam
RP20_009-RG.bam
RP20_010-RG.bam
#Most coverage 13 samples GA
GA12_008-RG.bam
GA12_002-RG.bam
GA12_019-RG.bam
GA12_020-RG.bam
GA12_009-RG.bam
GA12_025-RG.bam
GA12_001-RG.bam
GA12_018-RG.bam
GA12_010-RG.bam
GA12_014-RG.bam
GA12_015-RG.bam
GA12_004-RG.bam
GA12_017-RG.bam
#Highest coverage 13 samples MA (removed the most highest coverage cause it was very high)
CTM_010-RG.bam
CTM_001-RG.bam
CTM_002-RG.bam
ELP_001-RG.bam
CTM_007-RG.bam
CTM_003-RG.bam
CTM_006-RG.bam
ELP_008-RG.bam
MW_049-RG.bam
ELP_006-RG.bam
ELP_007-RG.bam
ELP_009-RG.bam
ELP_002-RG.bam

#All listed in depthinfo/bamlist_sim13.txt

mkdir bcf_bycontig_13

while read p; do
echo "echo "$p""
echo "bcftools mpileup -f reference.fasta -b depthinfo/bamlist_sim13.txt -d 20000 -r $p | bcftools call -m -Oz -f GQ -o bcf_bycontig_13/bcf_$p.vcf"
done < listofcontigs_simref.txt  > j_bcf.txt
#Make separate script so it finishes first
echo "echo Removing Headers" > j_removeheaders.txt
while read p; do
echo "grep -v "##" bcf_bycontig_13/bcf_$p.vcf > bcf_bycontig_13/bcfnh_$p.vcf"
done < listofcontigs_simref.txt >> j_removeheaders.txt
#while read p; do
#echo "rm bcf_bycontig_13/bcf_$p.vcf"
#done < listofcontigs_simref.txt

nano s_bcfsim_13.sh
#!/bin/bash -l

#SBATCH --partition=regular
#SBATCH --job-name=temp
#SBATCH --output=temp.out
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --mail-user=hh693@cornell.edu

parallel -j 10 < j_bcf.txt
parallel -j 5 < j_removeheaders.txt

#Remove ones with headers
while read p; do
rm bcf_bycontig_13/bcf_$p.vcf
done < listofcontigs_simref.txt

#Merge vcfs
#Get header
freebayes -r dDocent_Contig_1 -f reference.fasta --min-coverage 20 --limit-coverage 1000 -g 20000 -n 4 -L tmpbam.txt --report-monomorphic > bayescalls_bycontig/mono_dDocent_Contig_1_4head.vcf
bcftools mpileup -f reference.fasta -b depthinfo/bamlist_sim13.txt -d 20000 -r dDocent_Contig_1 | bcftools call -m -Oz -f GQ -o bcf_bycontig_13/bcf_dDocent_Contig_1_13head.vcf
grep "#" bcf_bycontig_13/bcf_dDocent_Contig_1_13head.vcf > bcf_bycontig_13/monocall_sim13.vcf
#Add others to the end
while read p; do
bcf_bycontig_13/bcf_$p.vcf >> bcf_bycontig_13/monocall_sim13.vcf
done < listofcontigs_simref.txt

sbatch --nodes=1 --ntasks=10 --mem=40000 -o fullrun13_sim.out -J simbcf_10th_40G s_bcfsim_13.sh

#Repeat for solidissima
mkdir depthinfo
	#Make lists of A/Bin/Boff
	nano depthinfo/A_bamlist.txt
	
while read p; do 
samtools depth $p |  awk -v awkvar="$p" '{sum+=$3} END { print awkvar," ",sum/NR}'
done < depthinfo/Boff_bamlist.txt > depthinfo/Boff_sampdepths.txt &

#13 Lists
#Bin
MW_001-RG.bam
PT_018-RG.bam
PT_009-RG.bam
PT_019-RG.bam
PT_008-RG.bam
MW_002-RG.bam
PT_007-RG.bam
SL4106_4-RG.bam
MW_016-RG.bam
PT_010-RG.bam
PT_013-RG.bam
PT_006-RG.bam
PT_003-RG.bam
#Boff (DEPTH ALSO LOW HERE)
FNJ_014-RG.bam
FNJ_001-RG.bam
FNJ_012-RG.bam
GBE_336-RG.bam
FNJ_002-RG.bam
GBE_384-RG.bam
413_038-RG.bam
FNJ_040-RG.bam
GBE_335-RG.bam
413_046-RG.bam
FNJ_011-RG.bam
413_042-RG.bam
GBE_334-RG.bam
#A (DEPTH WAS MUCH LOWER OVER ALL FOR TYPE A)
BLP_190-RG.bam
BLP_195-RG.bam
WV1_010-RG.bam
WV5_009-RG.bam
BLP_233-RG.bam
BLP_231-RG.bam
WV5_003-RG.bam
CGC_007-RG.bam
WV5_001-RG.bam
MCX_019-RG.bam
WV1_002-RG.bam
SL2108_3-RG.bam
SL2108_2-RG.bam

#Get this again
grep ">" reference.fasta | sed "s/>//g" > listofcontigs_solref_true.txt

#Then also make 30 lists for if I'm doing just solidissimamkdir bcf_bycontig_13
nano depthinfo/bamlist_sol13.txt
mkdir bcf_bycontig_13

while read p; do
echo "echo "$p""
echo "bcftools mpileup -f reference.fasta -b depthinfo/bamlist_sol13.txt -d 20000 -r $p | bcftools call -m -Oz -f GQ -o bcf_bycontig_13/bcf_$p.vcf"
done < listofcontigs_solref_true.txt  > j_bcf.txt
#Make separate script so it finishes first
echo "echo Removing Headers" > j_removeheaders.txt
while read p; do
echo "grep -v "#" bcf_bycontig_13/bcf_$p.vcf > bcf_bycontig_13/bcfnh_$p.vcf"
done < listofcontigs_solref_true.txt >> j_removeheaders.txt

nano s_bcfsol_13.sh
#while read p; do
#rm bcf_bycontig_13/bcf_$p.vcf
#done < listofcontigs_solref_true.txt

sbatch --nodes=1 --ntasks=24 --mem=125000 -o sever13_sol.out -J solbcf s_bcfsol_13.sh

#Next up: filter by no missing data

#Did not make the full file:
#Could fix script in the future
while read p; do
grep -v "#" bcf_bycontig_13/bcf_$p.vcf > bcf_bycontig_13/bcfnh_$p.vcf
done < listofcontigs_simref.txt
grep "#" bcf_bycontig_13/bcf_dDocent_Contig_1_13head.vcf > bcf_bycontig_13/monocall_sim13.vcf
while read p; do
cat bcf_bycontig_13/bcfnh_$p.vcf >> bcf_bycontig_13/monocall_sim13.vcf
done < listofcontigs_simref.txt
cp bcf_bycontig_13/monocall_sim13.vcf ../monomorphic_filtering

#Let's see if it can be filtered
vcftools --vcf monocall_sim13.vcf --max-missing 1.0 --recode --recode-INFO-all --out mono_nomissing_sim
#After filtering, kept 39 out of 39 Individuals
#After filtering, kept 459234 out of a possible 859587 Sites

#Filter to monomorphic only?
vcftools --vcf mono_nomissing_sim.recode.vcf --max-maf 0 --recode --recode-INFO-all --out mono_exclusive_sim
#After filtering, kept 415286 out of a possible 459234 Sites
#Great!

#Now, combine with polymorphic filter
	#Filter simonly basic to only include those samples
sed -i "s/0819//g" SNP_simonly_basic.vcf
sed -i "s/0819//g" mono_exclusive_sim.recode.vcf
bcftools query -l mono_exclusive_sim.recode.vcf > samplist13.txt #Pull directly
vcftools --vcf SNP_simonly_basic.vcf --keep samplist13sim.txt --recode --recode-INFO-all --out SNP_simonly_basic13
	#Find way to merge without the samples being out of order
#vcf-concat SNP_simonly_basic13.recode.vcf mono_exclusive_sim.recode.vcf > monoandpoly_sim.vcf
		#Whereas vcf-merge brings together different samples
		#Before you can concatenate them, they must be ordered the same way with vcf-shuffle-cols
#vcf-shuffle-cols -t template.vcf.gz file.vcf.gz > out.vcf
vcf-shuffle-cols -t mono_exclusive_sim.recode.vcf SNP_simonly_basic13.recode.vcf > SNP_simonly_basic_shuffled.vcf
vcf-concat SNP_simonly_basic_shuffled.vcf mono_exclusive_sim.recode.vcf > monoandpoly_sim.vcf
#Seems like it might work but they are NOT in order
#Let's see if it works with the haplotyper
cp samplist.txt samplist13edit.txt
sed -i "s/0819//g" samplist13edit.txt

nano s_haps_0616.sh

source /programs/miniconda3/bin/activate rad_haplotyper
rad_haplotyper.pl -v monoandpoly_sim.vcf -r reference.fasta -x 10 \
-p samplist13edit.txt --a haps_simonly_monANDpol_0616ima.txt --genepop haps_simonly_monANDpol_0616gp.txt
mv stats.out haps_stats_simonly_monANDpol_0616.out

sbatch --nodes=1 --ntasks=10 --mem=60000 -o haps_monANDpol.out -J monohaps_10th_60G s_haps_0616.sh

#Create a list of the contigs from the mono and the poly to compare afterwards if it made haplotypes for the mono (also to see how many are different)
grep -v "#" monoandpoly_sim.vcf | wc -l
428037
grep -v "#" mono_exclusive_sim.recode.vcf | wc -l
415286
grep -v "#" SNP_simonly_basic_shuffled.vcf | wc -l
12751

#Make lists of just contigs for each set
grep -v "#" mono_exclusive_sim.recode.vcf | awk '{print $1}' | sort | uniq > contiglist_mono.txt
grep -v "#" SNP_simonly_basic_shuffled.vcf | awk '{print $1}' | sort | uniq > contiglist_poly.txt
wc -l contiglist_*
 1773 contiglist_mono.txt
 1536 contiglist_poly.txt
 3309 total #If they were all different

sort contiglist_mono.txt contiglist_poly.txt | uniq -d | less #to output the duplicated lines after merging the lists together to find the duplicated ones
sort contiglist_mono.txt contiglist_poly.txt | uniq -d | wc -l
1227
#This line does the same thing as the above: Print the overlap
comm -12 contiglist_poly.txt  contiglist_mono.txt | wc -l

#Pull out ones that appear in mono but not poly
#find lines only in file2
comm -13 file1 file2 
comm -13 contiglist_poly.txt  contiglist_mono.txt > contiglist_onlyinmono.txt
#Check math
comm -13 contiglist_poly.txt  contiglist_mono.txt | wc -l
546 #Looks good
#So if any of the contigs in contiglist_onlyinmono.txt appear in the haplotypes, then we're golden
	#I'm checking part way through by looking at CTM_001.haps
while read con; do
grep $con CTM_001.haps
done < ../monomorphic_filering/combine_with_poly/contiglist_onlyinmono.txt 
	#Looks good! There's definitely some there and they just look like they have one read
	#Confirm that they actually make it to the final file and that radhaplotyper doesn't filter them out
while read con; do
grep $con haps_simonly_monANDpol_0616gp.txt
done < ../monomorphic_filering/combine_with_poly/contiglist_onlyinmono.txt 
	#Also good
#Uh oh, even if some where included, A LOT of contigs were rejected from the haplotyper

while read con; do
grep "PASSED" haps_stats_simonly_monANDpol_0616.out | grep $con
done < ../monomorphic_filering/combine_with_poly/contiglist_onlyinmono.txt 
#dDocent_Contig_1058	107	1	39	39	1.000	PASSED	0	0	0	
#Okay, this one was still included, this was not in the poly contig list
	#SO it is calling some


#Date: June 16th
#Make LD 0.2 prune data set
#Do it under settingupvcfs/LD_prune/real
#settingupvcfs/SNP_simonly_b1215_1fp.vcf
#settingupvcfs/SNP_solonly_b1215_1fp2.recode.vcf

#Script below doesn't work
#a) I don't need to run the whole loop, just plink
#b) Needs --ld-window-r2 flag to give it the cutoff number

PROJECTS=(
simonly
solonly
solrefall
)

for p in ${PROJECTS[@]}
do
plink --file SNP_"$p"_1fprad --r2 --ld-window-r2 0.20 --noweb --no-sex --allow-no-sex --out "$p"_1fp_pLD02
awk '{ print $6 }' "$p"_1fp_pLD02.ld | awk 'BEGIN { FS = ":" } ; { print $1,$2 }' | tail -n+2 > "$p"_1fp_pLD02_chr.txt
vcftools --vcf SNP_"$p"_1fprad.vcf --exclude-positions "$p"_1fp_pLD02_chr.txt --recode --out SNP_"$p"_LD02prune
done
rename .recode.vcf .vcf *LD02prune*

#Date: June 16th
#Remake trees with LD 0.2 data set
#In SNAPP Notes

#Date: June 16th
#Get FEEMS graphs from python
source /programs/miniconda3/bin/activate feems
jupyter notebook --ip=0.0.0.0 --port=8016 --no-browser
#B outer missing


#Date: June 17th
#Why did sol bcf fail?
#Cause the script is bad?
#Rerun on server
#/local/storage/Spisula/Mounting/freebayes_0608/align121_sol
manage_slurm new cbsumm12
#After a few hours, I could just create my own version of the thing since I'll be subsetting the number of loci anyway to match similis more closely

#Date: June 17th
#Freebayes rather than bcf by parrellelizing it by contig and saving it
#On new server
#Mount local storage
/programs/bin/labutils/mount_server cbsuhare /storage
#Make slurm cluster
manage_slurm new cbsulm09 #Do not do in background with & - it may error
#Takes about 3 minutes to set up
#What do I want to do?
#bam files from align_sol (and from 0508 and similis but those are secondary priority)
#references
ls /fs/cbsuhare/storage/Spisula/Mounting
cp /fs/cbsuhare/storage/Spisula/Mounting/EEMS/ . -r
#Move stuff to the mounting
cp /workdir/hh693/dDocent/dDoc_0528/*_*.bam* . &
cp /workdir/hh693/dDocent/dDoc_0528/*reference* . &
cp /workdir/hh693/dDocent/dDoc_0528/*_*cov.stats* . &
#Move onto server
ls /fs/cbsuhare/storage/Spisula/Mounting

cp -r /fs/cbsuhare/storage/Spisula/Mounting/bamfiles/sim* . &
cp -r /fs/cbsuhare/storage/Spisula/Mounting/bamfiles/*sol* . &

#Might as well start by running with sim to figure out the moves
#Run with only 13 samples each
nano simsamps13.txt
#Make one just a list of samples and one a list of bams
cp simsamps13.txt simbams13.txt
sed 's/-RG.bam//g' simbams13.txt > simsamps13.txt
#Make list of contigs
grep ">" reference.fasta | sed 's/>//g' > contigs_sim.txt

#Get general freebayes code with good options
#From above
while read p; do
echo "echo "$p""
echo "freebayes -r $p -f reference.fasta --min-coverage 20 --limit-coverage 1000 -g 20000 -n 4 -L tmpbam.txt --report-monomorphic > bayescalls_bycontig/mono_$p.vcf"
done < listofcontigs_simref.txt > j_free.txt
#NOW! Rather than doing it as a parallel job list, instead save each bit to a slurm script

mkdir freecall13_sim
mkdir freecall13_sim/scripts/
mkdir freecall13_sim/bycontig/
nano ../s_header.txt
nano ../s_emailheader.txt
mkdir freecall13_sim/slurmouts

count=0
while read p; do
#echo $p
count=$[count + 1]
cat ../s_header.txt > freecall13_sim/scripts/s_$p.sh
if (( $count % 500 == 0 )) #multiple of 500 (remainder 0)
then
#Add the email bit
echo "Im gonna email you about this one: $p"
cat ../s_emailheader.txt > freecall13_sim/scripts/s_$p.sh
fi
echo "freebayes -r $p -f reference.fasta --min-coverage 20 --limit-coverage 1000 -g 20000 -n 4 -L simbams13.txt --report-monomorphic > freecall13_sim/bycontig/mono_$p.vcf" >> freecall13_sim/scripts/s_$p.sh
done < contigs_sim.txt > emaillist.txt &
#I can't have it email me for every contig...
#Put the script that runs all of these slurm scripts inside one that emails me?
#I could have it email me every thousand

#Now the script to start them running
	#NOPE #nano freecall13_sim/scripts/s_runall.sh
#Instead just manually go into the last script and add the email bit
tail contigs_sim.txt 
nano freecall13_sim/scripts/s_dDocent_Contig_4042.sh
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL

#Still not perfect, because there may be others still finishing
head contigs_sim.txt > temp.txt

while read p; do
sbatch --nodes=1 --ntasks=1 --mem=10000 -o freecall13_sim/slurmouts/o_$p.out -J $p freecall13_sim/scripts/s_$p.sh
done < temp.txt

#Still not working on large machines
#Run on cbsumm12 to start and then try to increase the number of nodes as needed
#sbatch: error: Batch job submission failed: Socket timed out on send/recv operation
	#Occurs sometimes
#Takes longer to start the jobs than to run them... not an efficient use of the server
	#What did Honggang say?
#Also script did not work
	#I forgot to set the freebayes environment
	#Add to header file
	#I suspect it still might be better to divide this up differently
#Jobs are listed in htop as in D - uninteruptible sleep...?
	#I left one of them to run and it pops in and out of running

#Try just running freebayes

while read p; do
freebayes -r $p -f reference.fasta --min-coverage 20 --limit-coverage 1000 -g 20000 -n 4 -L simbams13.txt --report-monomorphic > freecall13_sim/bycontig/mono_$p.vcf &
done < temp.txt
#Also switched from dead to alive but also takes up more of the core and finishes
#Worked
#Run it as a dig dang job list, but maybe just do it in parts

while read p; do
echo "echo $p"
echo "freebayes -r $p -f reference.fasta --min-coverage 20 --limit-coverage 1000 -g 20000 -n 4 -L simbams13.txt --report-monomorphic > freecall13_sim/bycontig/mono_$p.vcf"
done < contigs_sim.txt > j_simfree.txt

cat ../s_emailheader.txt > freecall13_sim/scripts/s_joball.sh
nano freecall13_sim/scripts/s_joball.sh

sbatch --nodes=1 --ntasks=22 --mem=125000 -o freecall13_sim/slurmouts/o_jobversion.out -J job_simcall freecall13_sim/scripts/s_joball.sh

count=0
let num=`wc -l contigs_sim.txt | awk '{print $1}'`  #Assign variable to function output
while read p; do
echo "echo $p"
count=$[count + 1]
echo "echo $count/$num"
per=$[$count * 100 /$num]
echo "echo "$per%""
echo "freebayes -r $p -f reference.fasta --min-coverage 20 --limit-coverage 1000 -g 20000 -n 4 -L simbams13.txt --report-monomorphic > freecall13_sim/bycontig/mono_$p.vcf"
done < contigs_sim.txt > j_simfree.txt

#See if I can get it to print %
count=0
let num=`wc -l contigs_sim.txt | awk '{print $1}'`  #Assign variable to function output
while read p; do
echo "echo $p"
count=$[count + 1]
echo "echo $count/$num"
per=$[$count * 100 /$num]
echo "$per%"
done < contigs_sim.txt

#Add a large 40 core one if I can for solidissima
#Find a way to check if any of them failed

#Rather than add a 40 core where I can't even see the htop, why not just do it lonestar style x2
	#Only down side is copying things over but I have to imagine that's better than trying to run it on one server while the data is on another... right?
cbsumm31
cp -r /fs/cbsuhare/storage/Spisula/Mounting/bamfiles/sol* . &
#Very annoyingly, much of the memory on this machine happens to be taken up by something else...?
#Try another (just briefly)
#Better on cbsumm29
#sbatch --nodes=1 --ntasks=10 --mem=40000 -o out.out -J test test.sh

nano solsamps13.txt
#Make one just a list of samples and one a list of bams
cp solsamps13.txt solbams13.txt
sed 's/-RG.bam//g' solbams13.txt > solsamps13.txt
#Make list of contigs
grep ">" reference.fasta | sed 's/>//g' > contigs_sol.txt

mkdir freecall13_sol
mkdir freecall13_sol/scripts/
mkdir freecall13_sol/bycontig/

count=0
let num=`wc -l contigs_sol.txt | awk '{print $1}'`  #Assign variable to function output
while read p; do
echo "echo $p"
count=$[count + 1]
echo "echo $count/$num"
per=$[$count * 100 /$num]
echo "echo "$per%""
echo "freebayes -r $p -f reference.fasta --min-coverage 20 --limit-coverage 1000 -g 20000 -n 4 -L solbams13.txt --report-monomorphic > freecall13_sol/bycontig/mono_$p.vcf"
done < contigs_sol.txt > freecall13_sol/scripts/j_solfree.txt

nano freecall13_sol/scripts/s_joball.sh

sbatch --nodes=1 --ntasks=38 --mem=250000 -o outs/free_j1.out -J j_solcall freecall13_sol/scripts/s_joball.sh
#Estimate time - 31 minutes, 710/62302 -> 45.3 hours
#100 min 1867/62302 -> 55 hours (the ones that take longer are taking longer)

#Start on another server working from the back half or so

#Bring the vcfs together
#vcf-concat SNP_simonly_basic13.recode.vcf mono_exclusive_sim.recode.vcf > monoandpoly_sim.vcf
#Whereas vcf-merge brings together different samples
vcf-concat A.vcf.gz B.vcf.gz C.vcf.gz | gzip -c > out.vcf.gz
#It can do multiple at once
vcf-concat freecall13_sim/bycontig/mono_dDocent_Contig_3180.vcf \
freecall13_sim/bycontig/mono_dDocent_Contig_978.vcf \
freecall13_sim/bycontig/mono_dDocent_Contig_3173.vcf \
freecall13_sim/bycontig/mono_dDocent_Contig_2154.vcf \
freecall13_sim/bycontig/mono_dDocent_Contig_995.vcf \
freecall13_sim/bycontig/mono_dDocent_Contig_3079.vcf > tempmerge.vcf
#concats them in the order provided
#You can call a list of files with the -f flag
freecall13_sim/
ls bycontig/* > listofvcfs.txt
vcf-concat -f listofvcfs.txt > sim_merge.vcf
	#Pretty fast!
	
#Can I split a file into even parts?
split -l 500 myfile segment
#This will output six 500-line files: segmentaa, segmentab, segmentac, segmentad, segmentae, and segmentaf.

#Continue running back up freebayes for far side sol
	#Do it in reverse 1000 at a time
#Reset if you make any changes 
rm freecall13_sol/subsetcontigs/list_*
split -l 2000 contigs_sol.txt freecall13_sol/subsetcontigs/list_
#Works but now I need a list of them but in reverse order - reverse file order
	#tac is reverse cat (easy peasy)
ls freecall13_sol/subsetcontigs/list_* | tac > freecall13_sol/subsetcontigs/subsets.txt
#Actually just get run names
sed 's/freecall13_sol\/subsetcontigs\/list_//g' freecall13_sol/subsetcontigs/subsets.txt | sed 's/.txt//g' > freecall13_sol/subsetcontigs/subsets2.txt #Actually just get run names

#While read the subset list, do once of these for each
while read r; do
count=0
let num=`wc -l contigs_sol.txt | awk '{print $1}'`  #Assign variable to function output
while read p; do
echo "echo $p"
count=$[count + 1]
echo "echo $count/$num"
per=$[$count * 100 /$num]
echo "echo "$per%""
echo "freebayes -r $p -f reference.fasta --min-coverage 20 --limit-coverage 1000 -g 20000 -n 4 -L solbams13.txt --report-monomorphic > freecall13_sol/bycontig/mono_$p.vcf"
done < freecall13_sol/subsetcontigs/list_$r > freecall13_sol/scripts/j_sub_$r.txt
done < freecall13_sol/subsetcontigs/subsets2.txt

nano ../header.txt

#Make several slurm scripts
while read r; do
cat ../header.txt > freecall13_sol/scripts/s_sub_$r.txt
echo "parallel -j 22 < freecall13_sol/scripts/j_sub_$r.txt" >> freecall13_sol/scripts/s_sub_$r.txt
done < freecall13_sol/subsetcontigs/subsets2.txt

mkdir outs
#Run them
while read r; do
sbatch --nodes=1 --ntasks=23 --mem=124000 -o outs/free_j_sub_$r.out -J j_sol_$r freecall13_sol/scripts/s_sub_$r.txt
done < freecall13_sol/subsetcontigs/subsets2.txt

#bam file was just names, not bams
scancel -u hh693

#Nice!
#Tomorrow, add together
ls freecall13_sol/bycontig/ | wc -l
#From each, aiming for the total of 62302
#Oh... for some reason, there were some left over in the counts from the batched subset one
#So the counts are not accurate probably
#Search for only the non-empty ones
find freecall13_sol/bycontig/ -maxdepth 1 -size +0 -print | wc -l
#Perfect!!
	#-maxdepth 1 - this tells find to search the current dir only, remove to look in all sub dirs or change the number to go down 2, 3 or more levels.
	#-size +0 this tells find to look for files with size larger than 0 bytes. 0 can be changed to any size you would want.
	#-print tells find to print out the full path to the file it finds

#For memory, looks like with 39 samples it takes about 2G (give MINIMUM of 2.5G x threads) of memory per thread running
#Well shit, the FIRST fast round is still running, so it hasn't even gotten though 2000 in 10 hours
	#Humm.... it's still running bf but it also has 5,900 vcfs
	#I've got 24,073 total after 13 hours
	#But it looks like maybe the ones from cbsumm12 are also the early starter ones???
	less freecall13_sol/scripts/s_sub_bf.txt 
	parallel -j 22 < freecall13_sol/scripts/j_sub_bf.txt
	less freecall13_sol/scripts/j_sub_bf.txt
	#Starts with contig 5 - WHY?
	#Yep, it contains all contigs
	#Because I didn't actaully make my internal while loop call the right thing

#Date: June 19th
#Getting the freebayes off of the server
#And merging files
ls bycontig/ > listofvcfs.txt
vcf-concat -f ../listofvcf_sol.txt > ../sol_merge.vcf
cp -r freecall13_sol/ /fs/cbsuhare/storage/Spisula/Mounting/unmount/freebayes/
sbatch --nodes=1 --ntasks=1 --mem=40000 -o merge_sol.out -J merge_sol s_mergesol.sh

grep "#" mono_dDocent_Contig_10000.vcf > ../headmerge_sol.vcf
while read p; do
grep -v "#" $p >> ../headmerge_sol.vcf
done < ../listofvcf_sol.txt
sbatch --nodes=1 --ntasks=1 --mem=10000 -o headmerge_sol.out -J headmerge_sol s_headmerge.sh

ls /local/storage/Spisula/Mounting/unmount/freebayes/freecall13_sol/bycontig/ > listofvcf_sol.txt

#When it's done test the number of chromosomes in it by doing
grep -v "#" sol_merge.vcf | awk '{print $1}' | uniq | wc -l

#Start filter for similis
#Augmented filtering for monomorphic sites
#Step 1
vcftools --vcf ../headmerge_sol.vcf --max-missing 0.5 --minQ 30 --recode --recode-INFO-all --out raw.g5mac3

vcftools --vcf ../sim_merge.vcf --minQ 30 --recode --recode-INFO-all --out Qtest &
vcftools --vcf ../sim_merge.vcf --max-missing 0.5 --recode --recode-INFO-all --out misstest &
#Test how many
vcftools --vcf Qtest.recode.vcf  --max-alleles 2 --recode --recode-INFO-all --out Qtest2 &
vcftools --vcf misstest.recode.vcf  --max-alleles 2 --recode --recode-INFO-all --out misstest2 &

grep -v "#" Qtest2.recode.vcf | grep -v "1/1" | grep -v "0/1" | wc -l
grep -v "#" misstest2.recode.vcf | grep -v "1/1" | grep -v "0/1" | wc -l

vcftools --vcf raw.g5mac3.recode.vcf --minDP 3 --recode --recode-INFO-all --out raw.g5mac3dp3 
	#Test how much monomorphic
	vcftools --vcf ../headmerge_sol.vcf --max-alleles 2 --recode --recode-INFO-all --out count_unfiltered &
	grep -v "#" count_unfiltered.recode.vcf | grep -v "1/1" | grep -v "0/1" | wc -l
	#594,443 loci
	vcftools --vcf raw.g5mac3dp3.recode.vcf --max-alleles 2 --recode --recode-INFO-all --out count_filter1 &
	grep -v "#" count_filter1.recode.vcf | grep -v "1/1" | grep -v "0/1" | wc -l
	#44 loci
	vcftools --vcf DP3g95maf05.recode.vcf --max-alleles 2 --recode --recode-INFO-all --out count_filter2 &
	grep -v "#" count_filter2.recode.vcf | grep -v "1/1" | grep -v "0/1" | wc -l
	#44 loci
	
#Step 1.5
vcftools --vcf raw.g5mac3dp3.recode.vcf --min-meanDP 20 --max-missing 1.0 --recode --recode-INFO-all --out DP3g95maf05
#Step 3
	#Do not do Allele Balance
	#/programs/vcflib-1.0.1/bin/vcffilter -s -f "AB > 0.2 & AB < 0.8 | AB < 0.01" DP3g95p5maf05.recode.vcf > DP3g95p5maf05.fil1.vcf
	mv DP3g95maf05.recode.vcf DP3g95p5maf05.fil1.vcf
#Step 4
/programs/vcflib-1.0.1/bin/vcffilter -f "SAF / SAR > 100 & SRF / SRR > 100 | SAR / SAF > 100 & SRR / SRF > 100" -s DP3g95p5maf05.fil1.vcf > DP3g95p5maf05.fil2.vcf
awk '!/#/' DP3g95p5maf05.fil2.vcf | wc -l
#Step 5
/programs/vcflib-1.0.1/bin/vcffilter -f "MQM / MQMR > 0.9 & MQM / MQMR < 1.05" DP3g95p5maf05.fil2.vcf > DP3g95p5maf05.fil3.vcf
awk '!/#/' DP3g95p5maf05.fil3.vcf | wc -l 
#Step 6
/programs/vcflib-1.0.1/bin/vcffilter -f "PAIRED > 0.05 & PAIREDR > 0.05 & PAIREDR / PAIRED < 1.75 & PAIREDR / PAIRED > 0.25 | PAIRED < 0.05 & PAIREDR < 0.05" -s DP3g95p5maf05.fil3.vcf > DP3g95p5maf05.fil4.vcf
awk '!/#/' DP3g95p5maf05.fil4.vcf | wc -l 
#Step 7
/programs/vcflib-1.0.1/bin/vcffilter -f "QUAL / DP > 0.25" DP3g95p5maf05.fil4.vcf > DP3g95p5maf05.fil5.vcf
cut -f8 DP3g95p5maf05.fil5.vcf | grep -oe "DP=[0-9]*" | sed -s 's/DP=//g' > DP3g95p5maf05.fil5.DEPTH
awk '!/#/' DP3g95p5maf05.fil5.vcf | cut -f1,2,6 > DP3g95p5maf05.fil5.vcf.loci.qual
awk '{ sum += $1; n++ } END { if (n > 0) print sum / n; }' DP3g95p5maf05.fil5.DEPTH
	4136.76 #mean depth
	6778.09 + 3*sqrt(6778.09) = 4329.7
		#Insert that value
paste DP3g95p5maf05.fil5.vcf.loci.qual DP3g95p5maf05.fil5.DEPTH | awk -v x=7025 '$4 > x' | awk '$3 < 2 * $4' > DP3g95p5maf05.fil5.lowQDloci
vcftools --vcf DP3g95p5maf05.fil5.vcf --site-depth --exclude-positions DP3g95p5maf05.fil5.lowQDloci --out DP3g95p5maf05.fil5
cut -f3 DP3g95p5maf05.fil5.ldepth > DP3g95p5maf05.fil5.site.depth
		#Insert number of individuals
awk '!/D/' DP3g95p5maf05.fil5.site.depth | awk -v x=39 '{print $1/x}' > meandepthpersite
vcftools --vcf  DP3g95p5maf05.fil5.vcf --recode-INFO-all --out DP3g95p5maf05.FIL --max-meanDP 200 --exclude-positions DP3g95p5maf05.fil5.lowQDloci --recode 
#Step 8
	#Do not recode SNPs or remove indels
#Step 9
	#Do not filter for HWE
#Step 10
	#Do not do LD at 1pf
#Step #11
	#Haplotype caller
cp DP3g95p5maf05.FIL.recode.vcf ../solonly_mono.vcf

#Date: June 20th
#Divmigrate with the LD02 data set
#Plot the Tree produced with the LD02 dataset
	#Plot trees in DensiTree
	#Plot log in Tracer

#Date: June 20th
#Filter solidissima and run haplotyper

cp mono/simonly_mono.vcf dDocent/dDoc_928_similisonly/

nano s_haps_0620.sh
nano s_sol_haps_0620.sh

#!/bin/bash -l

#SBATCH --partition=regular
#SBATCH --job-name=temp
#SBATCH --output=temp.out
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --mail-user=hh693@cornell.edu

source /programs/miniconda3/bin/activate rad_haplotyper
rad_haplotyper.pl -v solonly_mono.vcf -r reference.fasta -x 40 \
-p samplist13edit.txt --a haps_solonly_mono_0620ima.txt --genepop haps_solonly_mono_0620gp.txt
mv stats.out haps_stats_solonly_mono_0620.out

sbatch --nodes=1 --ntasks=40 --mem=80000 -o simhaps_0620.out -J haps_sim_30th s_haps_0620.sh
sbatch --nodes=1 --ntasks=40 --mem=100000 -o solhaps_0620.out -J haps_sol_30th s_sol_haps_0620.sh

#Count number of sites to filter to
less haps_simonly_mono_0620ima.txt
#It has one line with dDoc per haplotype
grep "dDocent_Contig" haps_simonly_mono_0620ima.txt | wc -l
#1050 haplotype loci
#How do I randomly subset the solidissima?
#Well... it doesn't look like they are in order so maybe I can just take the first few in the file
#How do I do that?
	#Because it has the same number of samples as similis, then I should be able to wc -l the whole sim file and take that much head from the ima of solidissima
#What about genpop?
	#Ooph, no ideas there...

#Subsample the vcf first
#I have to also change the popmap file because to only include the three populations (and it still might not work)
#Actually it only takes sol 22 minutes to run for all sites

MW_001	Bin
PT_018	Bin
PT_009	Bin
PT_019	Bin
PT_008	Bin
MW_002	Bin
PT_007	Bin
SL4106_4	Bin
MW_016	Bin
PT_010	Bin
PT_013	Bin
PT_006	Bin
PT_003	Bin
FNJ_014	Boff
FNJ_001	Boff
FNJ_012	Boff
GBE_336	Boff
FNJ_002	Boff
GBE_384	Boff
413_038	Boff
FNJ_040	Boff
GBE_335	Boff
413_046	Boff
FNJ_011	Boff
413_042	Boff
GBE_334	Boff
BLP_190	A
BLP_195	A
WV1_010	A
WV5_009	A
BLP_233	A
BLP_231	A
WV5_003	A
CGC_007	A
WV5_001	A
MCX_019	A
WV1_002	A
SL2108_3	A
SL2108_2	A


GLD_001	NLI
GLD_002	NLI
GLD_003	NLI
GLD_004	NLI
GLD_005	NLI
GLD_006	NLI
GLD_007	NLI
GLD_008	NLI
PEC_011	NLI
PEC_012	NLI
PEC_013	NLI
RP20_009	NLI
RP20_010	NLI
GA12_008	GA
GA12_002	GA
GA12_019	GA
GA12_020	GA
GA12_009	GA
GA12_025	GA
GA12_001	GA
GA12_018	GA
GA12_010	GA
GA12_014	GA
GA12_015	GA
GA12_004	GA
GA12_017	GA
CTM_010	MA
CTM_001	MA
CTM_002	MA
ELP_001	MA
CTM_007	MA
CTM_003	MA
CTM_006	MA
ELP_008	MA
MW_049	MA
ELP_006	MA
ELP_007	MA
ELP_009	MA
ELP_002	MA

#Okay so while that's running pretty much I'll be just trying to figure out the stats
	#Maybe I can subset the ima for sol and then to arl to genpop
wc -l haps_simonly_mono_0620ima.txt 
82229 haps_simonly_mono_0620ima.txt
#Output comes in a text file too!
#BUT it is across all populations (which is good I need that too but also make subsets)

head -82229 haps_solonly_mono_0620ima.txt > haps_solonly_mono_subset_0620ima.txt

#Date: June 20th
#Neighbor joining tree
#Plink can make a distance matrix from a vcf
/programs/plink-1.9-x86_20210606/plink --vcf ../start_HERE/SNP_simonly_LDprune.vcf --distance square --allow-extra-chr --vcf-half-call r

#Date: June 21st
#Poptree in vcf2poptree (for individual level data)
#Population data done in R

#Date: August 1st
#I need to redo PCA and STRUCTURE for similis with the LD r2 < 0.5 data
#Done

#Date: August 5th
#Filling in data
#Count average trimmed paired reads
wc -l #then divide by 4

ls /local/storage/Spisula/GBS/similis_clean/sync_trim_6/*R1*.fastq > temp_sim.txt
ls /local/storage/Spisula/GBS/solidis_clean/sync_trim_6_final/*R1*.fq.gz > temp_sol.txt
#the sols are zipped
zcat my.fastq.gz | echo $((`wc -l`/4))

touch simcounts.txt
while read p; do
wc -l $p >> simcounts.txt
done < temp_sim.txt

touch solcounts.txt
while read p; do
zcat $p | echo $((`wc -l`)) >> solcounts.txt
done < temp_sol.txt

#Do awk math on first column
sum=$(awk '{print $1}' simcounts.txt | paste -sd+ | bc); echo "$sum / $(cat simcounts.txt | wc -l)" | bc -l
#Then remember to divide by 4!
#Done
#Then get coverage from bam files
ls *-RG.bam > bamlist.txt
#Don't include the concatenated bam
samtools coverage -b bamlist.txt > sim_coverage.txt
#Takes forever, instead do it over a loop
samtools coverage -b bamlist.txt > sim_coverage.txt
#Takes forever still, do in paralelle
while read p; do
echo "samtools coverage -b bamlist.txt -r $p -H >> sim_coverage.txt"
done < temp.contig.list > j_coverage.txt

#should put in a slurm wrapper so its easy to kill
parallel -j 22 < j_coverage.txt
sbatch --nodes=1 --ntasks=22 --mem=22000 -o temp.coverage.out -J sim_coverage s_j_covergagesim.sh

grep ">dD" reference.fasta | sed -s 's/>//g' > temp.contig.list
while read p; do
echo "samtools coverage -b bamlist.txt -r $p -H >> sol_coverage.txt"
done < temp.contig.list > j_coverage.txt
sbatch --nodes=1 --ntasks=22 --mem=52000 -o temp.coveragesol.out -J sol_coverage s_j_sol_coverage.sh
#Get average and std dev
#Header (cause I'm concattenating, I removed the header)
#rname  startpos  endpos    numreads  covbases  coverage  meandepth  meanbaseq  meanmapq

#Actaully, why don't I remove that add only add it back if asked:
#Calculated coverage
Similis = 20919.8x #for the regions in the catalog where it mapped (that's high)
#average
#Calculated a different way because the other one gave an error
awk '{ total += $7; count++ } END { print total/count }' sim_coverage.txt

#Count SNPs by just piping the vcf into vcftools without any output or calculation
#Count haplotypes with
grep "dDoc" haps_simONLY_0126.gen | wc -l

#Get number of contigs and total bp count
grep -v ">" reference.fasta | wc #Includes Ns in the middle
grep -v ">" reference.fasta | sed -s 's/N//g' | wc


#Date: Aug 11th
#Attempt BUSCO for dDocent stuff just to see how many we got

#Download
#different than before
#download and build image (replacing 5.1.3--pyhdfd78af_0 with latest tag)
singularity pull busco_5.1.3.sif docker://quay.io/biocontainers/busco:5.1.3--pyhdfd78af_0
#run command
./busco_5.1.3.sif busco -h
./busco_5.1.3.sif busco --in ./sim_ref.fasta -l metazoa_odb10 --mode genome --cpu 10 --out BUSCO_dD_sim -f >& BUSCO_dDoc_sim.log
./busco_5.1.3.sif busco --in ./sol_ref.fasta -l metazoa_odb10 --mode genome --cpu 10 --out BUSCO_dD_sol2 -f >& BUSCO_dDoc_sol2.log

#Run on all isoforms Trinity
./busco_5.1.3.sif busco --in ./Trinity_sim.fasta -l metazoa_odb10 --mode transcriptome --cpu 6 --out BUSCO_allisometa_sim -f >& BUSCO_allisometa_sim.log &
./busco_5.1.3.sif busco --in ./Trinity_sol.fasta -l metazoa_odb10 --mode transcriptome  --cpu 6 --out BUSCO_allisometa_sol2 -f >& BUSCO_allisometa_sol2.log &
#Doesn't run well in the background, if not working, just run without saving the log file manually and just let it print out?
./busco_5.1.3.sif busco --in ./Trinity_sim.fasta -l ./mollusca_odb10 --mode transcriptome --cpu 6 --out BUSCO_allisomoll_sim -f
./busco_5.1.3.sif busco --in ./Trinity_sol.fasta -l ./mollusca_odb10 --mode transcriptome  --cpu 6 --out BUSCO_allisomoll_sol -f >& temp_solmol.log &
./busco_5.1.3.sif busco --in ./Trinity_sim.fasta -l ./metazoa_odb10 --mode transcriptome --cpu 6 --out BUSCO_allisometa_sim -f
./busco_5.1.3.sif busco --in ./Trinity_sol.fasta -l ./metazoa_odb10 --mode transcriptome  --cpu 6 --out BUSCO_allisometa_sol -f >& temp_solmet.log &

./busco_5.1.3.sif busco --in ./Trinity_sim.fasta -l ./mollusca_odb10 --mode transcriptome --cpu 6 --out BUSCO_allisomoll_sim -f
	--------------------------------------------------
	|Results from dataset mollusca_odb10              |
	--------------------------------------------------
	|C:81.1%[S:79.9%,D:1.2%],F:2.1%,M:16.8%,n:5295    |
	|4291	Complete BUSCOs (C)                       |
	|4230	Complete and single-copy BUSCOs (S)       |
	|61		Complete and duplicated BUSCOs (D)     	  |
	|110	Fragmented BUSCOs (F)                     |
	|894	Missing BUSCOs (M)                        |
	|5295	Total BUSCO groups searched               |
	--------------------------------------------------
#Where can I view which ones?
less BUSCO_allisomoll_sim/run_mollusca_odb10/full_table.tsv

#I have 1750 mollusca genes and 330 metazoa genes

#Date: Aug 14th
#Rerunning STRUCTURE
#Set input files to be structure format
	#Do I have to do it in pgdspider?
	#THEN REMOVE ALL HEADERS?
#Numbers of Things
	#SNPS sol
	2761	loci
	390		ind
	
	#SNPS sim
	1505	loci
	125 	ind
	
	#HAPS sol
	3159	loci
	423		ind
	
	#HAPS sim
	1260 	loci
	124		ind

#Set mainparams
cp mainparams mp_SNPsol
cp mp_SNPsol mp_SNPsim

#Set extraparam
	#Not needed
#Set joblist
touch j_batch0814.txt
touch j_batch0815.txt
for j in {1..20}
do
mkdir SNP_sol/run$j
mkdir SNP_sim/run$j
mkdir haps_sol/run$j
mkdir haps_sim/run$j
for f in {1..10}
do
echo "structure -K $f -o SNP_sol/run$j/out_K$f -i SNP_solonly.structure -L 2761 -N 390 >& SNP_sol/run$j/my_run_K$f.log -m mp_SNPsol" >> j_batch0815.txt
echo "structure -K $f -o SNP_sim/run$j/out_K$f -i SNP_simonly.structure -L 1505 -N 125 >& SNP_sim/run$j/my_run_K$f.log -m mp_SNPsim" >> j_batch0815.txt
echo "structure -K $f -o haps_sol/run$j/out_K$f -i haps_solonly.structure -L 3159 -N 432 >& haps_sol/run$j/my_run_K$f.log -m mp_HAPsol" >> j_batch0815.txt
echo "structure -K $f -o haps_sim/run$j/out_K$f -i haps_simonly.structure -L 1260 -N 124 >& haps_sim/run$j/my_run_K$f.log -m mp_HAPsim" >> j_batch0815.txt
done
done

#!/bin/bash -l

#SBATCH --partition=regular
#SBATCH --job-name=temp
#SBATCH --output=temp.out
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --mail-user=hh693@cornell.edu

parallel -j 40 < j_batch0814.txt

sbatch --nodes=1 --ntasks=40 --mem=90000 -o STRUCT2.out -J STRUCT_run20 s_STRUCT.sh

#Date: Aug 15th
#Use fasta-puller to get out the genes
#Sol
TRINITY_DN9081_c0_g1_i8:75-14108
#Sim
TRINITY_DN125_c0_g1_i19:105-16541

#It may not have the same isoform as the one that mapped best, but I guess theoretically it should because these should have been the ones which were only single copy in the original.
#Give it a shot

samtools faidx Trinity_sol_longiso2.fasta TRINITY_DN9081_c0_g1_i8:75-14108 > outputs/39at6447.fasta
samtools faidx Trinity_sim_longiso2.fasta TRINITY_DN125_c0_g1_i19:105-16541 >> outputs/39at6447.fasta

#They aren't labeled, but I guess it don't matter, especially if sol is always first
#Pull off of server and put into blast
https://blast.ncbi.nlm.nih.gov/Blast.cgi?BLAST_SPEC=blast2seq&LINK_LOC=align2seq&PAGE_TYPE=BlastSearch
#Paste each half into a different box
97.61%
#Great!

#blast on server
#save to two separate files
samtools faidx Trinity_sol_longiso2.fasta TRINITY_DN9081_c0_g1_i8:75-14108 > outputs/39at6447_sol.fasta
samtools faidx Trinity_sim_longiso2.fasta TRINITY_DN125_c0_g1_i19:105-16541 >> outputs/39at6447_sim.fasta

blastn -query outputs/39at6447_sol.fasta -subject outputs/39at6447_sim.fasta > outputs/39at6447_b.txt

grep "Iden" outputs/39at6447_b.txt | awk '{print $3}'
#^ That gets me the ratio of matching to not. I think that's about as good as it can get cause the % has no decimals
#So then I just save a big ol' file of those on top of eachother.

touch percentids.txt
while read -r -a fields; do
  name=${fields[1]}     
  solcor=${fields[3]}
  simcor=${fields[5]}
  echo ${fields[3]}
  echo ${fields[5]}
#echo "samtools faidx Trinity_sol_longiso2.fasta $solcor > outputs/$name.sol.fasta"
samtools faidx Trinity_sim_longiso2.fasta $simcor > outputs/$name.sim.fasta
#blastn -query outputs/$name.sol.fasta -subject outputs/$name.sim.fasta > outputs/$anme.b.txt
#grep "Iden" outputs/$name.b.txt | head -1 | awk '{print $3}' >> percentids.txt
done < temp.txt 

#Try dividing them into collumns
awk '{print $3}' temp.txt > tempsolcor.txt
awk '{print $5}' temp.txt > tempsimcor.txt
awk '{print $1}' temp.txt > tempgenes.txt

#For loop this time
for j in {1..20}
do
sed -n -e $jp tempsolcor.txt
done

#Nah, lets up set up the commands in excel. I'm done with this
#I did not get the same, but rather similar results across methods. Fine
1.16% +- 0.51% divergence
#From ~51 samples

#All samples
parallel -j 6 < j_calcids.txt

#Date: Aug 22nd
#Filter solidissima reference similis
#Step 1
#Basic Filtering
vcftools --vcf TotalRawSNPs.vcf --keep simo.txt --recode --recode-INFO-all --out filter_simonly/raw.simo
vcftools --vcf raw.simo.recode.vcf --max-missing 0.5 --mac 3 --minQ 30 --recode --recode-INFO-all --out raw.g5mac3
vcftools --vcf raw.g5mac3.recode.vcf --minDP 3 --recode --recode-INFO-all --out raw.g5mac3dp3 
vcftools --vcf raw.g5mac3dp3.recode.vcf --missing-indv
awk '$5>0.5' out.imiss | cut -f1 > lowDP.indv 
vcftools --vcf raw.g5mac3dp3.recode.vcf --remove lowDP.indv --recode --recode-INFO-all --out raw.g5mac3dplm
vcftools --vcf raw.g5mac3dplm.recode.vcf --max-missing 0.90 --min-meanDP 20 --recode --recode-INFO-all --out DP3g95maf05 --min-meanDP 20
#Step 2
awk '$2 == "GA\r"' ../samplist.txt > 1.keep
awk '$2 == "SCC\r"' ../samplist.txt > 2.keep
awk '$2 == "NLI\r"' ../samplist.txt > 3.keep

for f in {1..6}
do
	vcftools --vcf DP3g95maf05.recode.vcf --keep $f.keep --missing-site --out $f
done
cat 1.lmiss 2.lmiss 3.lmiss | awk '!/CHR/' | awk '$6 > 0.1' | cut -f1,2 >> badloci
vcftools --vcf DP3g95maf05.recode.vcf --exclude-positions badloci --recode --recode-INFO-all --out DP3g95p5maf05

#Steps 3-7
		#Allele Balance
/programs/vcflib-1.0.1/bin/vcffilter -s -f "AB > 0.2 & AB < 0.8 | AB < 0.01" DP3g95p5maf05.recode.vcf > DP3g95p5maf05.fil1.vcf
		#check how many loci now
awk '!/#/' DP3g95p5maf05.recode.vcf | wc -l && awk '!/#/' DP3g95p5maf05.fil1.vcf | wc -l
	#The next filter we will apply filters out sites that have reads from both strands.
/programs/vcflib-1.0.1/bin/vcffilter -f "SAF / SAR > 100 & SRF / SRR > 100 | SAR / SAF > 100 & SRR / SRF > 100" -s DP3g95p5maf05.fil1.vcf > DP3g95p5maf05.fil2.vcf
awk '!/#/' DP3g95p5maf05.fil2.vcf | wc -l
	#The next filter looks at the ratio of mapping qualities between reference and alternate alleles
	#The rationale here is that, again, because RADseq loci and alleles all should start from the same genomic location there should not be large discrepancy between the mapping qualities of two alleles.
/programs/vcflib-1.0.1/bin/vcffilter -f "MQM / MQMR > 0.9 & MQM / MQMR < 1.05" DP3g95p5maf05.fil2.vcf > DP3g95p5maf05.fil3.vcf
awk '!/#/' DP3g95p5maf05.fil3.vcf | wc -l &
	#Yet another filter that can be applied is whether or not their is a discrepancy in the properly paired status of for reads supporting reference or alternate alleles.
/programs/vcflib-1.0.1/bin/vcffilter -f "PAIRED > 0.05 & PAIREDR > 0.05 & PAIREDR / PAIRED < 1.75 & PAIREDR / PAIRED > 0.25 | PAIRED < 0.05 & PAIREDR < 0.05" -s DP3g95p5maf05.fil3.vcf > DP3g95p5maf05.fil4.vcf			
awk '!/#/' DP3g95p5maf05.fil4.vcf | wc -l &
	#In short, with whole genome samples, it was found that high coverage can lead to inflated locus quality scores. Heng proposed that for read depths greater than the mean depth plus 2-3 times the square root of mean depth that the quality score will be twice as large as the depth in real variants and below that value for false variants.
	#I actually found that this is a little too conservative for RADseq data, likely because the reads aren’t randomly distributed across contigs. I implement two filters based on this idea. the first is removing any locus that has a quality score below 1/4 of the depth.
		#This part is complicated - do it with just all samples first
/programs/vcflib-1.0.1/bin/vcffilter -f "QUAL / DP > 0.25" DP3g95p5maf05.fil4.vcf > DP3g95p5maf05.fil5.vcf
cut -f8 DP3g95p5maf05.fil5.vcf | grep -oe "DP=[0-9]*" | sed -s 's/DP=//g' > DP3g95p5maf05.fil5.DEPTH
awk '!/#/' DP3g95p5maf05.fil5.vcf | cut -f1,2,6 > DP3g95p5maf05.fil5.vcf.loci.qual
awk '{ sum += $1; n++ } END { if (n > 0) print sum / n; }' DP3g95p5maf05.fil5.DEPTH
		54235.7 #mean depth			#his was 1.9k so this is a lot higher
#Now the the mean plus 3X the square root of the mean - done myself in google/wolfram
		54235.7 + 3*sqrt(54235.7) =54934.4
		#Insert that value
paste DP3g95p5maf05.fil5.vcf.loci.qual DP3g95p5maf05.fil5.DEPTH | awk -v x=52674 '$4 > x' | awk '$3 < 2 * $4' > DP3g95p5maf05.fil5.lowQDloci
vcftools --vcf DP3g95p5maf05.fil5.vcf --site-depth --exclude-positions DP3g95p5maf05.fil5.lowQDloci --out DP3g95p5maf05.fil5
cut -f3 DP3g95p5maf05.fil5.ldepth > DP3g95p5maf05.fil5.site.depth
	#Now let’s calculate the average depth by dividing the above file by the number of individuals 31
		#I have 484 individuals
		#Insert number
awk '!/D/' DP3g95p5maf05.fil5.site.depth | awk -v x=126 '{print $1/x}' > meandepthpersite
vcftools --vcf  DP3g95p5maf05.fil5.vcf --recode-INFO-all --out DP3g95p5maf05.FIL --max-meanDP 200 --exclude-positions DP3g95p5maf05.fil5.lowQDloci --recode 
#Step #8
	#Recode to SNPs only and remove indels
/programs/vcflib-1.0.1/bin/vcfallelicprimitives DP3g95p5maf05.FIL.recode.vcf --keep-info --keep-geno > DP3g95p5maf05.prim.vcf
grep -v "#" DP3g95p5maf05.prim.vcf | wc -l 
vcftools --vcf DP3g95p5maf05.prim.vcf --remove-indels --min-alleles 2 --max-alleles 2 --recode --recode-INFO-all --out SNP.DP3g95p5maf05
	#Filter to two alleles only
#Step #9
	#HWE
	#curl -L -O https://github.com/jpuritz/dDocent/raw/master/scripts/filter_hwe_by_pop.pl
	#chmod +x filter_hwe_by_pop.pl
	../filter_hwe_by_pop.pl -v SNP.DP3g95p5maf05.recode.vcf -p ../samplist.txt -o SNP.DP3g95p5maf05.HWE -h 0.001
	#Separates by popmap so it is okay they they are different species
cp SNP.DP3g95p5maf05.HWE.recode.vcf SNP_HWE_solrefall_0528_basic.vcf

#Step #10
	#Filter to only one per radtag by minor allele frequency
	#Actual calculation
vcftools --vcf SNP.DP3g95p5maf05.recode.vcf --freq --out freqlist
		#Open freqlist.fq in excel
		#Data by tabs and :
		#Before sorting, measure the minimum between the two (because it isn't minor allele, it's reference vs alternate)
		#Sort by max minor allele frequency then sort by unique contig
			#By removeing duplicates > expand selection > only select column A
		#Export by keeping column A and B without headers
		freq_keep.txt
			#1fprad = 1 (by frequency) per radtag
vcftools --vcf SNP.DP3g95p5maf05.recode.vcf --positions freq_keep.txt --recode --recode-INFO-all --out SNP_solrefall_0528_1fprad 

#Step #10.a
	#Filter for LD before 1fp
bcftools view -H SNP_solrefall_0528_1fprad.recode.vcf | cut -f 1 | uniq | awk '{print $0"\t"$0}' > solrefsim_1fprad.chrom-map.txt
vcftools --vcf SNP_solrefall_0528_1fprad.recode.vcf --chrom-map solrefsim_1fprad.chrom-map.txt --out SNP_solrefsim_1fprad --plink
awk '{$1=""}1' SNP_solrefsim_1fprad.ped > SNP_solrefsim_1fprad_1fprad2.ped
mv SNP_solrefsim_1fprad_1fprad2.ped SNP_solrefsim_1fprad.ped
plink --file SNP_solrefsim_1fprad --noweb --r2 --no-sex --allow-no-sex --out solrefsim_1fp_pLD
--r2 0.80 #This wasn't working becuase there are none above 0.8

#Date: Aug 22ng
#Pull values from the structure runs
for f in {1..20}
do
for g in {1..10}
do
	grep "Estimated Ln Prob of Data" run$f/my_run_K$g.log | awk '{print $7}' >> a.txt
	#echo $g >> d.txt
done
done

Estimated Ln Prob of Data   = -154534.1

#Date: Aug 23rd
#Filter haplotype data by LD
bcftools view -H SNP_solrefall_0528_1fprad.recode.vcf | cut -f 1 | uniq | awk '{print $0"\t"$0}' > solrefsim_1fprad.chrom-map.txt
vcftools --vcf SNP_solrefall_0528_1fprad.recode.vcf --chrom-map solrefsim_1fprad.chrom-map.txt --out SNP_solrefsim_1fprad --plink
awk '{$1=""}1' SNP_solrefsim_1fprad.ped > SNP_solrefsim_1fprad_1fprad2.ped
mv SNP_solrefsim_1fprad_1fprad2.ped SNP_solrefsim_1fprad.ped
plink --file SNP_solrefsim_1fprad --noweb --r2 --no-sex --allow-no-sex --out solrefsim_1fp_pLD

#convert genepop to plink
#cannot do in command line, attempt in pdgspider or tell Matt it doesn't work

#Date: Aug 23rd
#Get number of raw SNPS
#Remove indels and others from raw SNPs
dDoc_928_similisonly
dDoc_0528
align121_sol 
#Remove more than 2 alleles
#Recode as SNPs
#Step #8
	#Recode to SNPs only and remove indels
/programs/vcflib-1.0.1/bin/vcfallelicprimitives DP3g95p5maf05.FIL.recode.vcf --keep-info --keep-geno > DP3g95p5maf05.prim.vcf
grep -v "#" DP3g95p5maf05.prim.vcf | wc -l 
vcftools --vcf DP3g95p5maf05.prim.vcf --remove-indels --min-alleles 2 --max-alleles 2 --recode --recode-INFO-all --out SNP.DP3g95p5maf05
#Or, actually, I think Final Recode just has it
vcftools --vcf Final.recode.vcf
#Sol only 
573,504
#Make sure those are only two alleles
vcftools --vcf Final.recode.vcf --remove-indels --min-alleles 2 --max-alleles 2 #--recode --recode-INFO-all --out temp.vcf
#Yep, this is the same as normal Final.recode.vcf

#Date: Aug 23rd
#Calculate average SNPs per contig and contig length distribution
#Use basic filtering step, homologous
grep -v "^#" SNP_simonly_basic.vcf | cut -f 1 | sort | uniq -c #> ../SNPspercontig_allsim.txt
#Save File
grep -v "^#" SNP_simonly_basic.vcf | cut -f 1 | sort | uniq -c > ../SNPsperContig/SpC_sim.txt
grep -v "^#" SNP_solonly_basic.vcf | cut -f 1 | sort | uniq -c > ../SNPsperContig/SpC_sol.txt
#Have to do for each taxonomic unit

#Also need to subsample for the overall overallones
cd ../start_HERE
cat popmaps/popmap_sim.txt | shuf | head -13 > 0.keep
vcftools --vcf SNP_simonly_basic.vcf --keep 0.keep --mac 1 --min-alleles 2 --max-alleles 2 --recode --recode-INFO-all --out ../SNPsperContig/SpC_sim_0.txt
cat popmaps/popmap_sol_region.txt | shuf | head -13 > 0.keep
vcftools --vcf SNP_solonly_basic.vcf --keep 0.keep --mac 1 --min-alleles 2 --max-alleles 2 --recode --recode-INFO-all --out ../SNPsperContig/SpC_sol_0.txt
cd ../SNPsperContig
grep -v "^#" SpC_sim_0.txt.recode.vcf | cut -f 1 | sort | uniq -c | awk '{print $1}' > SpC_sim_0.txt
grep -v "^#" SpC_sol_0.txt.recode.vcf | cut -f 1 | sort | uniq -c | awk '{print $1}' > SpC_sol_0.txt
awk '{ total += $1; count++ } END { print total/count }' SpC_sim_0.txt
awk '{ total += $1; count++ } END { print total/count }' SpC_sol_0.txt
awk '{x+=$1;y+=$1^2}END{print sqrt(y/NR-(x/NR)^2)}' SpC_sim_0.txt #Equal to Stdev.P in excel
awk '{x+=$1;y+=$1^2}END{print sqrt(y/NR-(x/NR)^2)}' SpC_sol_0.txt #Equal to Stdev.P in excel


#Do I get consistent answers?
#Sim		#Sol
4.67064		3.76058
4.60797		3.76116
4.68894		3.74476
4.73125		3.70107
4.1862		3.66308
4.63514		3.80623
#Yeah, fairly consistent!

#popmaps include, ABS2, and sol_region, and sim
#Step 2
awk '$2 == "GA\r"' popmaps/popmap_sim.txt | shuf | head -13  > 1.keep
awk '$2 == "SCC\r"' popmaps/popmap_sim.txt | shuf | head -13  > 2.keep
awk '$2 == "NLI\r"' popmaps/popmap_sim.txt | shuf | head -13 > 3.keep
for f in {1..3}
do
	vcftools --vcf SNP_simonly_basic.vcf --keep $f.keep --mac 1 --min-alleles 2 --max-alleles 2 --recode --recode-INFO-all --out ../SNPsperContig/SpC_sim_$f.txt
	#Filtering my min/max alleles is only about the maximum number of alleles, not how many are actually present in this subset of samples.
	#So minor allele count must be atleast 1
done
#Sol by Region
awk '$2 == "536\r"' popmaps/popmap_sol_region.txt | shuf | head -13 > 1.keep
awk '$2 == "GBE\r"' popmaps/popmap_sol_region.txt | shuf | head -13 >> 1.keep
awk '$2 == "CCB\r"' popmaps/popmap_sol_region.txt | shuf | head -13 > 2.keep
awk '$2 == "SCC\r"' popmaps/popmap_sol_region.txt | shuf | head -13 > 3.keep
awk '$2 == "SLI\r"' popmaps/popmap_sol_region.txt | shuf | head -13 > 4.keep
awk '$2 == "NAN\r"' popmaps/popmap_sol_region.txt | shuf | head -13 > 5.keep
awk '$2 == "413\r"' popmaps/popmap_sol_region.txt | shuf | head -13 > 6.keep
awk '$2 == "FNJ\r"' popmaps/popmap_sol_region.txt | shuf | head -13 >> 6.keep
#Sol by A/B
awk '$2 == "A\r"' popmaps/popmap_ABS2.txt | shuf | head -13 > 7.keep
awk '$2 == "B\r"' popmaps/popmap_ABS2.txt | shuf | head -13 > 8.keep
awk '$2 == "Bin"' popmaps/popmap_OTUs_inoffsim.txt | shuf | head -13 > 9.keep
awk '$2 == "Boff"' popmaps/popmap_OTUs_inoffsim.txt | shuf | head -13 > 10.keep
for f in {1..10}
do
	vcftools --vcf SNP_solonly_basic.vcf --keep $f.keep --mac 1 --min-alleles 2 --max-alleles 2 --recode --recode-INFO-all --out ../SNPsperContig/SpC_sol_$f.txt
done

cd ../SNPsperContig
for f in {1..3}
do
grep -v "^#" SpC_sim_$f.txt.recode.vcf | cut -f 1 | sort | uniq -c | awk '{print $1}' > SpC_sim_$f.txt
done
for f in {1..10}
do
grep -v "^#" SpC_sol_$f.txt.recode.vcf | cut -f 1 | sort | uniq -c | awk '{print $1}' > SpC_sol_$f.txt
done

for f in {0..3}
do
awk '{ total += $1; count++ } END { print total/count }' SpC_sim_$f.txt > tmpmean
awk '{x+=$1;y+=$1^2}END{print sqrt(y/NR-(x/NR)^2)}' SpC_sim_$f.txt > tmpstd
cat "sim" $f tmpmean tmpstd
done
for f in {0..10}
do
awk '{ total += $1; count++ } END { print total/count }' SpC_sol_$f.txt > tmpmean
awk '{x+=$1;y+=$1^2}END{print sqrt(y/NR-(x/NR)^2)}' SpC_sol_$f.txt > tmpstd
cat "sol" $f tmpmean tmpstd
done

#Distribution of contig legnths
grep -v ">" reference.fasta | awk '{ print length }'
grep -v ">" reference.fasta | awk '{ print length }' | awk '{ total += $1; count++ } END { print total/count }'
#Similis 		#Solidissima
309.453			253.561

#Date: Sept 20th
#Run on new server
sbatch --nodes=1 --ntasks=22 --mem=120000 dDoc_sim0330.sh

#For later I'll need the contigs
1028	sim
1028	sim
1036	sim
1036	sim
1268	sim
1268	sim
1296	sim
1296	sim
1960	sim
1960	sim
1965	sim
1965	sim
2642	sim
2642	sim
2651	sol
2651	sol
2788	sol
2788	sol
2956	sim
2956	sim
3976	sol
3976	sol

#Date: Oct 3rd
#Filtering similis only file
#What the heck, did it not transfer??????

#Date: Oct 4th
#Average read depth of coverage per individual per mapping.
#From bam files?
	#Output DP from vcf files
	--depth
vcftools --depth --vcf TotalRawSNPs.vcf
# This file has the suffix ".idepth".

vcftools --depth --vcf dDoc_0920_simall/TotalRawSNPs.vcf --out simall &

vcftools --depth --vcf dDoc_0121_solonly/TotalRawSNPs.vcf --out solonly &
vcftools --depth --vcf dDoc_928_simonly/TotalRawSNPs.vcf --out simonly &


			#Sol	#Sim
#Sol only	65.3		-
#Sim only	-		185.0
#Sim all	247.3	182.7
#Sol all	62.0	42.0