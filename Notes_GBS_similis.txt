### SIMILIS GBS DATA

### GOALS AND STEPS (to be fleshed out)
#Trimmomatic and remove padding
	# resync
	# cut to be the same length
		#Do I cut from the 5' or 3' end? How?
#Feed clean reads to Stacks
	#Check for M test
	#Full run 
#Output desired is likely a vcf file
#Turn a vcf file into a PCA plot and determine other useful/interesting analyses


######################### NOTES dDocent ########################

##ALL WORK DONE AFTER NOVEMBER 26TH 2021 WAS DONE IN THE SOLALL NOTES#

#Do with both dedup and undedup data
#pre length trimming for stacks

#undedup
similis_clean_dups/sync_trim_6
#dedup
GBS_trimmingmethods_tests/sync_trim_5

#rename in storage/Mounting
mv sync_trim_6 undedup_sync_6_ddoc
mv sync_trim_5 dedup_sync_5_ddoc

#may need to rename files because they are .trim.sync.fasta and that's a lot

#ddocent set up and parameters
Trimmed reads will fail de novo assembly
If performing de novo assembly, it’s essential that no read trimming or adapter removal has taken place before the dDocent pipeline. If a reference is being supplied, then trimmed reads may be used.
#Okay so never mind

#My "raw" reads are demultiplexed so I just need those
/fs/cbsuhare/storage/Spisula/Mounting/similis_seq_raw/

#sample name convention
Pop1_Sample1.F.fq.gz Pop1_Sample1.R.fq.gz

#wrote this by pulling the sample sames into excel and then copying, using concatenate and left(ID,len()-3)

#then do the interactive pipeline simply by typing ddocent (glad I'm not using slurm)
source /programs/miniconda3/bin/activate dDocent-2.8.13
dDocent

#did not work, fastq may not work, change to fq
rename fastq fq *.gz
#fixed

#interactive parameter setting
150 individuals
all threads (24)
quality trim yes
PE assembly yes
#Reads will be assembled with Rainbow
#CD-HIT will cluster reference sequences by similarity. The -c parameter (% similarity to cluster) may need to be changed for your taxa.
0.85
#initial clustering of forward reads will be set to either 0.8 or c parameter mins 0.1 which ever is higher. Let’s use 0.85
#example uses 0.85 so I'll use that
map reads yes
# You may need to adjust -A -B and -O parameters for your taxa.
#use defaults for now - by entering them
		# I think B is like M from STACKS so I may want to try a higher values
freebayes yes

#dDocent will require input during the assembly stage.  Please wait until prompt says it is safe to move program to the background.
#Trimming reads and simultaneously assembling reference sequences

#Getting lots of errors like this
parallel: Error: Parsing of --jobs/-j/--max-procs/-P failed


gnuplot> plot 'uniqseq.data' with lines notitle
              ^
         line 0: Bad data on line 10 of file uniqseq.data


sort: invalid --parallel argument 'yes'
#maybe start over
#aaaah, I can't
kill 9737
kill 12043

#start fresh
screen
source /programs/miniconda3/bin/activate dDocent-2.8.13
dDocent

#now some of my files are missing?? - cause it was gunzipping and replacing Run with only 34
#replace files and rename
#then even if it has trouble, just let it run with defaults and then you can rezip all files

#restarted and worked
3 #for data cutoff 1 (my graph looked very different)


###UUUUGGGH It disconnected and gross again

#Ask for interactive session on main server
srun --account hh693 --job-name "InteractiveJob" --ntasks 34 --mem 90000 --time 8:00:00 --pty bash
#Not working, just try it

#oops this time I did 
4 #for data cutoff 1 (my graph looked very different from theirs, same as last time)


#3 would be better for the data

#I'm not sure it took the input but I set it to 8 and then did the
#ctrl Z, bg, disown -h

#started around 8pm, got to vcfcombine at 11:45am


#yep, based on the run file, it did not take my input for K1 (DO NOT PRESS ENTER AFTER IT HAS SHOWN GRAPH)
#I can run it non-interactive:

touch config_921.txt

Number of Processors
20
Maximum Memory
0
Trimming
yes
Assembly?
yes
Type_of_Assembly
PE
Clustering_Similarity%
0.85
Minimum within individual coverage level to include a read for assembly (K1)
3
Minimum number of individuals a read must be present in to include for assembly (K2)
8
Mapping_Reads?
no
Calling_SNPs?
no
Email
hh693@cornell.edu



Mapping_Match_Value
1
Mapping_MisMatch_Value
4
Mapping_GapOpen_Penalty
6



touch dDoc921.sh
nano dDoc921.sh

#!/bin/bash -l

#SBATCH --partition=regular
#SBATCH --job-name=dDoc920_th20
#SBATCH --output=dDoc920.out
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --mail-user=hh693@cornell.edu
source /programs/miniconda3/bin/activate dDocent-2.8.13
dDocent config_921.txt



#Then also try only assembling with similis and making solidissima align later
#You can take the reference.fasta produced by it and align trimmed reads to that (using the R1, R2 files)

# Read Trimming Customization
# By default, dDocent looks for Illumina TruSeq adapters and trims off basepairs with a quality score less than 20. Both BWA and FreeBayes take base quality scores into account, so excessive trimming is not necessary nor recommended. To modify this find line 458.
      #This likely would not take care of the padding which could cause issues, and I'm not sure if I could modify the script...
      
#adegenet still not working so prepare to import my output into R asap
#I think the raw.#.vcfs are just dividing it up for parallel and then get combined.
#Try PCAing from them 

cp raw.10.vcf test_raw_a.vcf
cp raw.21.vcf test_raw_b.vcf
#semi-randomly chosen

#turn vcf into pca
grep -v "##" test_raw_a.vcf | sed 's/#//g' > simplifiedfilvervcf.txt
sed 's/0\/0/0/g' simplifiedfilvervcf.txt | sed 's/0\/1/1/g' | sed 's/1\/1/2/g' > simvcfsimplereadyforR.txt

sed 's/.\/./NA/g' simvcfsimplereadyforR.txt > vcfwithnanforR.txt
#Then remove the header to import into R
#And the # infront of CHROM so that R does not ignore the first line as a comment

#How the heck did I do this? Try just downloading it
#And, solved in 5 minutes dang
#Open in excel, make new tab, left(D3,3) etc then save again, but next time I could do this next step first and then do left (1)
sed 's/0\/0/0/g' simplifiedfilvervcf_nocolons.txt | sed 's/0\/1/1/g' | sed 's/1\/1/2/g' | sed 's/.\/./NA/g' > simvcfsimplereadyforR_nocolon.txt
#used left(Location, 1) then copy and pasted values only, then replace Ns and . with NA


#Don't forget about this:
sed 's/SFJ/2/g' WL_format2.structure |  sed 's/GBE/1/g'| sed 's/CCB/3/g'| sed 's/SCC/4/g'| sed 's/NLI/5/g'| sed 's/SLI/6/g'| sed 's/GA /7 /g' > WL_format4.structure

#now try for TotalRawSNPs.vcf
grep -v "##" totalsnps_920.vcf | sed 's/#//g' > simpsnps_920.txt
sed 's/0\/0/0/g' simpsnps_920.txt | sed 's/0\/1/1/g' | sed 's/1\/1/2/g' > simpsnps_920_forR.txt

sed 's/.\/./NA/g' simpsnps_920_forR.txt > simpsnps_920_forR.txt
#Actually, I should be using Final.recode.vcf


#Notes on parameters
#K1 and K2 values (number of times the contig appears within individuals and between individuals, respectively)
#DDOCENT was then run again to map reads of only forward-reads using BWA (Li & Durbin 2009) with parameters A (match score), B (mismatch score), and O (gap penalty) set to 1, 3, and 5, respectively

# K1, K2, A, B, and O

After filtering, kept 330295 out of a possible 2600267 Sites
#Removed none of the 150 samples


#Set the cutoff too low, and extraneous reads will be included further down the pipeline and eat up valuable computational time and make it more likely that sequencing errors are included in the data. Set the cutoff too high, and usable polymorphic loci may be excluded from subsequent analyses.
MW, GLD, PEC, WFH, PPB, RP20, GA12
.F.fq.gz

#this but I just want the graphs
source /programs/miniconda3/bin/activate dDocent-2.8.13
dDocent

#K1 is most certainly 3 or 4
#K2 could vary
	#Create different references with different K2 parameters
	# 3 = conservative
	# 11-15 = 10% of my data set
	# 8-10 is fine says Matt

#Why would I change -c?
	#How tightly clustered they are
		#If they are mapping multiple places, that could mean my -c was too high because there are multiple sites that should be the same
				#Then you are going to have a bais: to lower heterozygosity - not all the alleles can agreegate at a locus
				#Dont want to separate alleles into different loci
				#
		#If my -c was too low, then I have too few clusters and snps are lost?
				#Then you are going to bring in paralogs and bias heterozygoisty too high
		#Check with Matt
			#Permissive is better at this early stage
		#Check mapping statistic for things that % map uniquely (does it vary across c values?)
			#Does dDcocent only use unique alignments? Is that the deafault for bwa mem?
		
#A - matchingness = 1
#B - mismatch = 4 (4 times worse than 1)
#O - gaps = 6 times worse than 1 (what is the likelihood of a deletion vs a SNP in nature?)


#Align my trims

#For next week - start abstract of similis abstract for NSA
	#Write abstract for similis and the questions I can answer with this - keep it general

#48-8-16= 24 so use 20 cores

#Which samples do I want in my reference?
#for dDoc_921

#set up list
cp ../dDoc_920_b/*.F.fq.gz . &
cp ../dDoc_920_b/*.R.fq.gz . &



#Which do I want to remove?
#solidissima
rm 422*
rm GBE*
#SLI:
rm BLP*
rm CUP*
#CCB:
rm BAR*
rm PT*

#one's with bad read quality
#based on M seq from multiqc on raw reads
rm MW_055*
rm MW_056*
rm MW_012*
rm MW_013*
rm MW_009*
rm MW_021*
rm MW_020*

#have approximately the same number of samples per site (remove some MW and GA)
GA 	= GA 					= 23			= 23
SCC = MW + ELP/CTM/WFH/PPB	= 60 + 34		= 94 (now only 34 MW, atleast more even)
NLI = RP20 + GLD + PEC		= 2 + 8 + 3 	= 13
#remove about 1/2 of MW for evenness

422					SJF		South Jersey Federal Survey
BAR					CCB		Cape Cod Bay 2017 inside bay
BLP					SLI		Long Island 2019
CTM					SCC		Cape Cod Bay 2017 south of bay
CUP					SLI		Long Island 2019
ELP					SCC		Cape Cod Bay 2017 south of bay
GA					GA		Georgia
GBE					GBE		Georges Bank
GLD					NLI		Long Island 2019 
MW					SCC		Southern Cape Cod 2012 south of bay
PEC					NLI		Long Island 2019
PPB					SCC		Cape Cod Bay 2017 south of bay
PT					CCB 	Cape Cod Bay 2012
RP20				NLI
WFH					SCC		Cape Cod Bay 2017 south of bay Buzzard Bay


#based on high duplication in R1
rm MW_016*
rm MW_006*
rm MW_009*
rm MW_040*
rm MW_041*
rm MW_049*
rm MW_001*
rm MW_011*
rm MW_050*
rm MW_033*
rm MW_047*
rm MW_043*
rm MW_059*
rm MW_032*
rm MW_048*
rm MW_005*
rm MW_025*
#based on % failed in multiqc
rm MW_052*
#randomly generated
rm MW_039*
rm MW_033*
rm MW_060*
rm MW_029*
rm MW_060*
rm MW_028*
rm MW_061*
rm MW_014*
rm MW_044*
rm MW_053*
rm MW_011*
rm MW_049*
rm MW_059*
rm MW_022*
#now MW is only half of SCC
#reference made with 104 samples

#run ddoc through slurm script
sbatch --nodes=1 --ntasks=20 --mem=150000 dDoc921.sh
#worked first try wow

#Date: Sept 27th
#align my trimmed reads for stacks to refereence created for 921
cp *ref* ../dDoc_927/

#which trimmed reads to align?:
#pre length trimming for stacks (figured out when I started with docent)

#undedup
similis_clean_dups/sync_trim_6/*.fastq
#dedup
GBS_trimmingmethods_tests/sync_trim_5/*.fastq

#within dDoc_927 theres a undedup and a dedup subfolder
#If dDocent is not being used for trimming, trimmed reads must already be in the directory and must follow the naming convention below:
#Pop1_001.R1.fq.gz  Pop1_001.R2.fq.gz
#Pop1_002.R1.fq.gz  Pop1_002.R2.fq.gz

#using excel document

#qzip everything
#slow without parralel

ls *.fq > listtobezipped

while read p; do
echo "gzip $p"
done < listtobezipped > j_gzip.txt

parallel -j 10 < j_gzip.txt &
#done for both dedup and undedup
#unfortunately for docent input, the file names need to be identical but I do believe I properly put the undeduped and deduped in their correct folders

#run first on deduped

touch dDoc927_dedup.sh
nano dDoc927_dedup.sh

#!/bin/bash -l

#SBATCH --partition=regular
#SBATCH --job-name=dDoc927_dd_th24_mem120
#SBATCH --output=dDoc927_dedup.out
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --mail-user=hh693@cornell.edu
source /programs/miniconda3/bin/activate dDocent-2.8.13
dDocent config_927.txt

sbatch --nodes=1 --ntasks=24 --mem=120000 dDoc927_dedup.sh

touch config_927dd.txt

Number of Processors
24
Maximum Memory
0
Trimming
no
Assembly?
no
Mapping_Reads?
yes
Mapping_Match_Value
1
Mapping_MisMatch_Value
4
Mapping_GapOpen_Penalty
6
Calling_SNPs?
yes
Email
hh693@cornell.edu

#for dedup done on different server, no slurm
source /programs/miniconda3/bin/activate dDocent-2.8.13
dDocent config_927dd.txt

#listb is a list of all sample ids

while read p; do
echo echo "$p"
echo "bwa mem -t 2 reference.fasta $p.R1.fq.gz $p.R2.fq.gz > $p.align.sam"
done < listb > j_bwamem.txt

parallel -j 12 < j_bwamem.txt >& bwamem.log & #this saves command line output and then also puts it in the background

#otherwise use default bwa mem values
#wow, also looks like it worked on the first try

#change to .bam probably
samtools view -@ 4 -b A8100.chr18.paired.sam > A8100.chr18.paired.bam
samtools sort -@ 4 -o A8100.chr18.paired.sorted.bam A8100.chr18.paired.bam
samtools index A8100.chr18.all.bam												#example I am using here combines single and paired end bam files, but I have no reason to align single ends
samtools flagstat A8100.chr18.all.bam

#sam2bam
while read p; do
echo echo "$p"
echo "samtools view -@ 4 -b sams/$p.align.sam > bams/$p.align.bam"
done < listb > j_2bam.txt

parallel -j 6 < j_2bam.txt >& sam2bam.log &

#sort bam
while read p; do
echo echo "Reading $p"
echo "samtools view -@ 4 -o bams/$p.sorted.bam bams/$p.align.bam"
done < listb > j_sortbam.txt
parallel -j 6 < j_sortbam.txt >& sortbam.log &

#index bam
#DID NOT WORK
while read p; do
echo echo "Indexing $p"
echo "samtools index bams/$p.sorted.bam"
done < listb > j_indexbam.txt
parallel -j 12 < j_indexbam.txt >& indexbam.log &       #did not work

grep Indexing indexbam.log | tail #see how far along we are

#In the future, I could put all those in a script and just not have parallel run in the background to run each step for all samples then the next
#Can I even mark duplicates if I could not remove PCR duplicates before?
while read p; do
echo echo "Marking $p"
echo "/programs/sambamba-0.7.1/sambamba markdup -t 2 bams/$p.sorted.bam bams/$p.mrkdup.bam"
done < listb > j_markdupsbam.txt
parallel -j 6 < j_markdupsbam.txt >& markdupsbam.log &

#in order to input into freebayes it recommends:
# from https://github.com/freebayes/freebayes
#ensure alignments have read groups
#sort the alignments
#mark duplicates
#run freebayes
#filter output

cp reference.fasta ref.fa
samtools faidx ref.fa

#I probably need to figure out what read groups are and how to deal with that first
#mine do not have RGs yet so
samtools addreplacerg [-r rg-line | -R rg-ID] [-m mode] [-u] [-o out.bam] in.bam

while read p; do
echo echo "Making Group for $p"
echo "samtools addreplacerg -r ID:$p -r SM:$p -o bamsRG2/$p.r.bam bams/$p.mrkdup.bam" 
done < listb > j_addRG2.txt
parallel -j 24 < j_addRG2.txt >& addRG2b.log & 

#
ls *bam > listbams 

#merge multiple sorted bam files
samtools merge -r --threads 20 -b listbams all.bam

# set environment
export PATH=/programs/freebayes-1.3.5/bin:$PATH
export PATH=/programs/freebayes-1.3.5/scripts:$PATH
export PATH=/programs/vcflib-1.0.1/bin:$PATH
export PATH=/programs/vcflib-1.0.1/scripts:$PATH
# To run freebayes-parallel (use 24 cores in this example, ref.fa.fai can be generated with command "samtools faidx ref.fa"):
freebayes-parallel <(fasta_generate_regions.py ref.fa.fai 100000) 24 -f ref.fa bamsRG2/allR2.bam > out_927.vcf >& freebayes_927.log &
#did not work: could not find SM:

#repeat the above, adding in SM as well, same as ID
#From bamsRG2
samtools merge -r --threads 24 -b listbams allR2.bam
#Now it needs to be indexed but can't be because why would it?


#to get just snps afterwords
vcffilter -f "TYPE = snp"
#in order to use Honggang's freebayes protocol I need to have bed files which I do not
#but he got it from the ddocent script but pulling it apart so maybe I can do that

#Date: Sept 28th
source /programs/miniconda3/bin/activate dDocent-2.8.13
dDocent
#try interactive
#maybe try having a list of samples that are the default samples but just have them not use those:
ls *fq.gz > lista
sed 's/.R1./.F./g' lista > listb
sed -i 's/.R2./.R./g' listb
while read p; do
touch $p
done < listb

#It says gzip: WFH_008.R.fq.gz: unexpected end of file
#Meaning it is looking at the wrong ones and thinking that they are empty

#So instead try changing the file names to the default R and F for the trimmed reads
rename .R1. .F. *fq.gz
rename .R2. .R. *fq.gz
#Okay so that did not work either. Maybe have the same, trimmed reads as both R1 and F
sed 's/.R.fq.gz//g' listb > listc
sed -i 's/.F.fq.gz//g' listc
sort listc | uniq > listd     #uniq -u means only those that are unique, uniq without -u means each distinct one

while read p; do
echo "cp $p.R1.fq.gz $p.F.fq.gz"
echo "cp $p.R2.fq.gz $p.R.fq.gz"
done < listd > j_copy.txt
parallel -j 24 < j_copy.txt

#IT LOOKS LIKE IT MIGHT BE WORKING!!!
control and Z simultaneously
Type bg and press enter
Type disown -h

#for undedup
ls *.fq > tozip
while read p; do
echo "gzip $p"
done < tozip > j_gz.txt
parallel -j 23 < j_gz.txt &

#Note: I think MW_01.fq might be an error and it used to be MW_017 in both - could have been an issue since 9_20

#undedup is taking longer than dedup
#that makes sense
#leave over night

#use PGDspider to output data as genepop and structure
#also could try plink

#then try structure plots and first PCAs
#also PCAs the old back country way

#Date: Sept 29th
cp -r /programs/PGDSpider_2.1.1.5 /workdir
cd /workdir/PGDSpider_2.1.1.5
##use java 8 before you start PGDSpider2
export JAVA_HOME=/usr/local/jdk1.8.0_121
export PATH=$JAVA_HOME/bin:$PATH

java -Xmx1024m -Xms512m -jar PGDSpider2-cli.jar ../hh693/dedup/Total.vcf dedup_928.STRUCTURE STRUCTURE spid1

#my missing value code is .
#I have a feeling putting that in here might not go over well

#try vcf filtering for loci with no missing data and then doing 
head -200 TotalRawSNPs.vcf | grep "\.:" #again

vcftools --gzvcf raw.vcf.gz --max-missing 0.5 --mac 3 --minQ 30 --recode --recode-INFO-all --out raw.g5mac3

#For example, to write out site-wise sequence depths only at sites that have no missing data, include the --max-missing argument.
vcftools --vcf deduprawSNPs.vcf --max-missing 1.0 --recode --recode-INFO-all --out raw.nomiss.vcf
#the recode part is what saves it as a new vcf

#Lots of warnings about INFO entry:
After filtering, kept 150 out of 150 Individuals
After filtering, kept 5728 out of a possible 239244 Sites

head -200 deduprawSNPs.vcf | grep "\.:" | wc -l
head -200 raw.nomiss.vcf.recode.vcf | grep "\.:"
#returned nothing! So maybe it worked!

#Now set minor allele to mac 
vcftools --vcf raw.nomiss.vcf.recode.vcf --mac 3 --minQ 30 --recode --recode-INFO-all --out dedupraw_mac3q30

#same warnings but
After filtering, kept 1936 out of a possible 5728 Sites


#repeat for undedup
vcftools --vcf undedupraw.vcf --max-missing 1.0 --recode --recode-INFO-all --out undedupraw.nomiss

#Wow! Still have the same pattern of less SNPs
After filtering, kept 4949 out of a possible 140231 Sites

vcftools --vcf undedupraw.nomiss.recode.vcf --mac 3 --minQ 30 --recode --recode-INFO-all --out undedupraw_mac3q30

After filtering, kept 1617 out of a possible 4949 Sites
#Not that different, certainly comparable for a PCA



#MUST ANSWER WHAT I WAS DOING TO DEDUP THEM!
#Also look at unfiltered allele count and depth

#maybe make a graph of missing loci @ % missing data for each
vcftools --vcf dedup_raw.vcf --max-missing 1.0 --mac 3 --minQ 30 --recode --recode-INFO-all --out dedup_nmq30 &
vcftools --vcf undedup_raw.vcf --max-missing 1.0 --mac 3 --minQ 30 --recode --recode-INFO-all --out undedup_nmq30 &

#might have worked but I forgot that I have to run structure in the command line and then just put it into the program for visualization

#run STRUCTURE on the .STRUCTURE file
#be aware that I have not filtered for single snpp per haplotype yet

#you just run structure in the folder with main params adjusted, I already set up a parallel k test

#need number of loci for each one
1936
1617
sbatch --nodes=1 --ntasks=4 --mem=100000 s_STRUCT_ddoc.sh
#Structure does not like it's snps to have names

#either sed 's/SNP_//g' leave only number or remove AND

#but set numbers to make sense
		#sed 's/SFJ/2/g' dedup_929_s.structure | 
		#sed 's/GBE/1/g'|
		#sed 's/CCB/3/g'|
		#sed 's/SCC/4/g'|
		#sed 's/NLI/5/g'|
		#sed 's/SLI/6/g'|
		#sed 's/GA /7 /g' > dedup_929_sitecode.structure
		#PGspider was smart and already did this except it chose dumb numbers
		SFJ	1
		CCB	2
		NLI	7
		SCC	4
		GA	5
		GBE	6
		SLI	3
#Theres definitely missing data in here even though this is only loci without missing data
#just remove header
awk NR\>1 dedup_929_s.structure > dedup_929_nohead.structure
#try replacing tabs with spaces
sed 's/\t/ /g' dedup_929_nohead.structure > dedup_929_noheadnt.structure

#set missing to '-9'?
#there's no space at the end of each line so it looks like -9^M?
awk '{print $0" "}' dedup_929_noheadnt.structure > dedup_929_nhwsnt.structure
#ah ha! now atleast that revealed the weird things

#now maybe I can just remove that locus because it is -9 for everybody with the ^M and then reduce loci number by one
sed 's/....$//' < dedup_929_nhwsnt.structure > dedup_929_rl4.structure
head -1 dedup_929_rl4.structure | wc 
1    1812    4006
#so theres one for name, one for population and the 1810 for SNPS
#WHY THE HECK?

#working now

#set up undedup to run
awk NR\>1 undedup_929_s.structure | awk '{print $0" "}' > undedup_929_noheadnt.structure
sed 's/....$//' < undedup_929_noheadnt.structure > undedup_929_rl4.structure
head -1 undedup_929_rl4.structure | wc 
      1    1521    3295
      #so 1519
sbatch --nodes=1 --ntasks=4 --mem=40000 s_STRUCT_u_ddoc.sh

#TO SET THE POPULATION ORDER MYSELF FOR STRUCTURE - MAKE POP MAP WITH NUMBERS


# spid-file generated: Wed Sept 29 06:22:29 CEST 2021

# STRUCTURE Parser questions
PARSER_FORMAT=VCF

# Are individual names (labels) included in the input file?
STRUCTURE_PARSER_IND_NAMES_QUESTION=true
# Enter the number of markers (loci) listed in the input file:
STRUCTURE_PARSER_NUMBER_LOCI_QUESTION=
# What is the missing value code (-9, -999, ...):
STRUCTURE_PARSER_MISSING_CODE_QUESTION=-9
# How are Microsat alleles coded?
STRUCTURE_PARSER_MICROSAT_CODING_QUESTION=REPEATS
# What is the ploidy of the data?
STRUCTURE_PARSER_PLOIDY_QUESTION=DIPLOID_TWO_ROWS
# Is the "PopData" column (population identifier) present in the input file?
STRUCTURE_PARSER_POP_DATA_PRESENT_QUESTION=true
# Enter the size of the repeated motif (1, 2, 3, ...):
STRUCTURE_PARSER_REPEAT_SIZE_QUESTION=
# Is the "Phase Information" row present?
STRUCTURE_PARSER_PHASE_ROW_QUESTION=false
# Are marker (locus) names included?
STRUCTURE_PARSER_LOCI_NAMES_QUESTION=true
# Select the type of the data:
STRUCTURE_PARSER_DATA_TYPE_QUESTION=MICROSAT
# Are the "Recessive Alleles" row and/or the "Inter-Marker Distance" row present in the input file?
STRUCTURE_PARSER_ADDITIONAL_ROW_QUESTION=None

# Arlequin Writer questions
WRITER_FORMAT=ARLEQUIN

# Enter the locus/locus combination you want to write to the Arlequin file:
ARLEQUIN_WRITER_LOCUS_COMBINATION_QUESTION=
# Specify which data type should be included in the Arlequin file  (Arlequin can only analyze one data type per file):
ARLEQUIN_WRITER_DATA_TYPE_QUESTION=DNA

#what order should they be in for my numerical popmap
#by lat
1	GBE
2	SFJ
3	CCB
4	SCC
5	NLI
6	SLI
7	GA
#by solsim
1	GBE
2	SFJ
3	CCB
4	SLI
5	SCC
6	NLI
7	GA

#measure number of reads
grep -v "#" undedup/WFH_008.F.fq.gz | wc -l
#It's zipped, doesn't work
zcat my.fastq.gz | echo $((`wc -l`/4))

zcat undedup/WFH_008.F.fq.gz | echo $((`wc -l`/4))

#depth
vcftools --vcf dedup_raw.vcf --depth
mv out.idepth dedupdepth.txt
vcftools --vcf undedup_raw.vcf --depth
mv out.idepth undedupdepth.txt




#Redo analysis (start from calling SNPs) after removing samples with bad data 
#MW_056





#Structure Ln output
#undedup
----------------- K=1 --------------------
Estimated Ln Prob of Data   = -94097.2
Mean value of ln likelihood = -93772.4
Variance of ln likelihood   = 649.7

Mean value of Fst_1         = 0.3770

----------------- K=2 --------------------
Estimated Ln Prob of Data   = -89391.1
Mean value of ln likelihood = -88798.1
Variance of ln likelihood   = 1186.1
Mean value of alpha         = 0.0465

Mean value of Fst_1         = 0.3880
Mean value of Fst_2         = 0.2765


------------------ K=3 --------------------
Estimated Ln Prob of Data   = -88397.4
Mean value of ln likelihood = -87366.7
Variance of ln likelihood   = 2061.4
Mean value of alpha         = 0.0882

Mean value of Fst_1         = 0.2001
Mean value of Fst_2         = 0.3233
Mean value of Fst_3         = 0.3770
#how do I tell which one to use?
#do I have to use a complex R analysis?
#https://www.researchgate.net/post/Problem_finding_optimal_k_using_findclusters_in_adegent
dudi.pca:barplot(pca.object$eig)

#plot these and you look for the elbow


#You may try ADMIXTURE which applies cross validation technique to define the appropriate K
#http://dalexander.github.io/admixture/admixture-manual.pdf
/programs/admixture/admixture [options]
#Hey oh: Those familiar with STRUCTURE know that it provides a means of identifying the “best”value for K, the number of populations, based on computing the model evidence for each possible K value. 


#preparing vcf for alternative PCA
#0/2 > 1
#1/2 > 3
#2/2 > 4
#0/3 > 1
#UP TO 9????!!!???
# all in loci dDocent_Contig_374 pos 82
#remove locus

#Filter sites with more than one allele
#The other big difference between STACKs and dDocent was multiple sites with more than two alleles.


#Date: Oct 5th 2021
#Filtering from undedup dDoc_928
cp dDoc_928/undedup/TotalRawSNPs.vcf
#What might I do differently with dDocent? Reduce number of MWs in catalog creation
#But for now
	#My goals
		#(??) Remove poor quality ID? - just remove bad samples?
		#Keep graphs of where your cut off are
		#HWE
		#Quality
		#Min and max depth
		#Last step: filter to only one per contig (to avoid strong LD)

	#From dDocent filtering guide
	#vcftools --gzvcf raw.vcf.gz --max-missing 0.5 --mac 3 --minQ 30 --recode --recode-INFO-all --out raw.g5mac3
	#We are going to only keep variants that have been successfully genotyped in 50% of individuals, a minimum quality score of 30, and a minor allele count of 3.
vcftools --vcf TotalRawSNPs.vcf --max-missing 0.5 --mac 3 --minQ 30 --recode --recode-INFO-all --out raw.g5mac3
#After filtering, kept 150 out of 150 Individuals
#Outputting VCF file...
#After filtering, kept 39150 out of a possible 140231 Sites
		#kept 30% of data
#Run Time = 30.00 seconds
	#The next filter we will apply is a minimum depth for a genotype call and a minimum mean depth
vcftools --vcf raw.g5mac3.recode.vcf --minDP 3 --recode --recode-INFO-all --out raw.g5mac3dp3 
#After filtering, kept 39150 out of a possible 39150 Sites
		#kept all data? I have high depth. Why do people remove too deep?
		#Check Xureub paper and their filtering
#Run Time = 25.00 seconds
	#Why only 3 reads? Well Puritz has a lot to say about that: This command will recode genotypes that have less than 3 reads. I’ll give you a second to take a deep breath. Yes, we are keeping genotypes with as few as 3 reads. We talked about this in the lecture portion of this course, but the short answer is that sophisticated multisample variant callers like FreeBayes and GATK can confidently call genotypes with few reads because variants are assessed across all samples simultaneously.
	#The next step is to get rid of individuals that did not sequence well. We can do this by assessing individual levels of missing data.
		#Woot! I want that
	#creates out.imiss
	#cat out.imiss
less out.imiss
	#We are looking for high F_MISS
		#Which are the important MWs again? 12 and 13 have high missing.
				#important samples
					#MW1-3 - Nick's Admixed
						#~21%
					#MW16 - has a low tag score but non-failure in raw reads
						#32.5% missing FMISS
					#MW22 - My heterozygote from primer testing
					#MW31
					#MW33
						#21.5%
					#PEC13
					#CTM008 - straight solidissima
						#21.0%
		#I can definitely remove those with more than 35% missing data
	#Let’s take a look at a histogram 
		#Woot!
		#Oops he using "mawk" which I cannot figure out so I'll graph it myself
		#New Notes_GBS_... file that is excel with data and graph
		#Most of data and I think all of the important samples have less that 0.5 F_MISS
	#mawk '$5 > 0.5' out.imiss | cut -f1 > lowDP.indv
	#I have to figure it out with awk
awk '$5>0.35' out.imiss | cut -f1 > lowDP.indv 
		#Is mawk just also awk?
	#feed list of samples to remove in vcftools
vcftools --vcf raw.g5mac3dp3.recode.vcf --remove lowDP.indv --recode --recode-INFO-all --out raw.g5mac3dplm
	#After filtering, kept 143 out of 150 Individuals
	#Now that we have removed poor coverage individuals, we can restrict the data to variants called in a high percentage of individuals and filter by mean depth of genotypes
vcftools --vcf raw.g5mac3dplm.recode.vcf --max-missing 0.95 --maf 0.05 --recode --recode-INFO-all --out DP3g95maf05 --min-meanDP 20
	#After filtering, kept 143 out of 143 Individuals
	#After filtering, kept 6732 out of a possible 39150 Sites
	#Run Time = 6.00 seconds
	#This applied a genotype call rate (95%) across all individuals. With two localities, this is sufficient, but when you have multiple localities being sampled You are also going to want to filter by a population specific call rate. VCFtools won’t calculate this directly, but it is an easy workaround.
	#popmap
	#And which site do I want, I want one from each of the seven big things
	#Now we need to create two lists that have just the individual names for each population
	#mawk '$2 == "BR"' popmap > 1.keep && mawk '$2 == "WL"' popmap > 2.keep
awk '$2 == "GBE\r"' popmap > 1.keep && awk '$2 == "SFJ\r"' popmap > 2.keep
awk '$2 == "CCB\r"' popmap > 3.keep && awk '$2 == "SLI\r"' popmap > 4.keep
awk '$2 == "SCC\r"' popmap > 5.keep && awk '$2 == "NLI\r"' popmap > 6.keep && awk '$2 == "GA\r"' popmap > 7.keep
#add \r "carriage return" because it is at the end of the line OR "includes" awk 'index($2, "GBE")' popmap
			#by solsim
			1	GBE
			2	SFJ
			3	CCB
			4	SLI
			5	SCC
			6	NLI
			7	GA
	#vcftools --vcf DP3g95maf05.recode.vcf --keep 1.keep --missing-site --out 1
	#vcftools --vcf DP3g95maf05.recode.vcf --keep 2.keep --missing-site --out 2
for f in {1..7}
do
	vcftools --vcf DP3g95maf05.recode.vcf --keep $f.keep --missing-site --out $f
done
	#combine the files to look across all for f_miss within each site
cat 1.lmiss 2.lmiss 3.lmiss 4.lmiss 5.lmiss 6.lmiss 7.lmiss | awk '!/CHR/' | awk '$6 > 0.1' | cut -f1,2 >> badloci
	#10% from any single site
	1853 badloci # out of 6000 loci
#Do this step TWICE - once with all samples and once with only similis sites
cat 5.lmiss 6.lmiss 7.lmiss | awk '!/CHR/' | awk '$6 > 0.1' | cut -f1,2 >> badloci_simonly
	139 badloci_simonly # out of 6000 loci
		#makes sense - not having data missing from SFJ is really limiting
vcftools --vcf DP3g95maf05.recode.vcf --exclude-positions badloci --recode --recode-INFO-all --out DP3g95p5maf05
		#After filtering, kept 5445 out of a possible 6732 Sites
vcftools --vcf DP3g95maf05.recode.vcf --exclude-positions badloci_simonly --remove soldis4rm.indv --recode --recode-INFO-all --out DP3g95p5maf05_simonly
		#before running that
		touch soldis4rm.indv
INDV
PT_016
GBE0819_171
GBE0819_172
GBE_332
GBE_333
GBE_335
CUP0819_392
422_014
422_033
BAR_002
		#then the only thing missing is that some of these sites might not contain any variants
		#After filtering, kept 133 out of 143 Individuals
		#After filtering, kept 6604 out of a possible 6732 Sites
	#Puritz has code that does this for more than two sites but I did it myself!
	#From this point forward, the filtering steps assume that the vcf file was generated by FreeBayes
		#Got 'em
	#The first filter we will apply will be on allele balance. Allele balance is: a number between 0 and 1 representing the ratio of reads showing the reference allele to all reads, considering only reads from individuals called as heterozygous Because RADseq targets specific locations of the genome, we expect that the allele balance in our data (for real loci) should be close to 0.5 We can use the vcffilter program from vcflib.
		#vcffilter from vcflib
		#/programs/vcflib-1.0.1/bin/
		/programs/vcflib-1.0.1/bin/vcffilter 
/programs/vcflib-1.0.1/bin/vcffilter -s -f "AB > 0.25 & AB < 0.75 | AB < 0.01" DP3g95p5maf05.recode.vcf > DP3g95p5maf05.fil1.vcf
/programs/vcflib-1.0.1/bin/vcffilter -s -f "AB > 0.25 & AB < 0.75 | AB < 0.01" DP3g95p5maf05_simonly.recode.vcf > DP3g95p5maf05_simonly.fil1.vcf
		#check how many loci now
		#mawk '!/#/' DP3g95p5maf05.recode.vcf | wc -l
awk '!/#/' DP3g95p5maf05.recode.vcf | wc -l && awk '!/#/' DP3g95p5maf05.fil1.vcf | wc -l
		5445>3483 #loci	
awk '!/#/' DP3g95p5maf05_simonly.recode.vcf | wc -l && awk '!/#/' DP3g95p5maf05_simonly.fil1.vcf | wc -l
		6604>4463 #loci
		#You’ll notice that we’ve filtered a lot of loci. In my experience though, I find that most of these tend to be errors of some kind. However, this will be data dependent. I encourage you to explore your own data sets.
		#I may not want to remove these, proceed possibly to future steps using DP3g95p5maf05.recode.vcf instead
	#The next filter we will apply filters out sites that have reads from both strands.
		#Mine overlap some times. What is wrong with a little bit of both?
			#These loci are likely to be paralogs, microbe contamination, or weird PCR chimeras
			#Check how many of mine it would remove
			/programs/vcflib-1.0.1/bin/vcffilter -f "SAF / SAR > 100 & SRF / SRR > 100 | SAR / SAF > 100 & SRR / SRF > 100" -s DP3g95p5maf05.fil1.vcf > DP3g95p5maf05.fil2.vcf
			awk '!/#/' DP3g95p5maf05.fil2.vcf | wc -l
			3483>3369
			#removes very few, might as well
			/programs/vcflib-1.0.1/bin/vcffilter -f "SAF / SAR > 100 & SRF / SRR > 100 | SAR / SAF > 100 & SRR / SRF > 100" -s DP3g95p5maf05_simonly.fil1.vcf > DP3g95p5maf05_simonly.fil2.vcf
			awk '!/#/' DP3g95p5maf05_simonly.fil2.vcf | wc -l
			4463>4286
	#The program SAMtools is a great way to visualize alignments right from the terminal. > ? - look at bam files
	#The next filter looks at the ratio of mapping qualities between reference and alternate alleles
	#The rationale here is that, again, because RADseq loci and alleles all should start from the same genomic location there should not be large discrepancy between the mapping qualities of two alleles.
/programs/vcflib-1.0.1/bin/vcffilter -f "MQM / MQMR > 0.9 & MQM / MQMR < 1.05" DP3g95p5maf05.fil2.vcf > DP3g95p5maf05.fil3.vcf
awk '!/#/' DP3g95p5maf05.fil3.vcf | wc -l
3369>3264 #not very much - ok
/programs/vcflib-1.0.1/bin/vcffilter -f "MQM / MQMR > 0.9 & MQM / MQMR < 1.05" DP3g95p5maf05_simonly.fil2.vcf > DP3g95p5maf05_simonly.fil3.vcf
awk '!/#/' DP3g95p5maf05_simonly.fil3.vcf | wc -l
4286>4148
	#Since de novo assembly is not perfect, some loci will only have unpaired reads mapping to them. This is not a problem. The problem occurs when all the reads supporting the reference allele are paired but not supporting the alternate allele. That is indicative of a problem.
	#Yet another filter that can be applied is whether or not their is a discrepancy in the properly paired status of for reads supporting reference or alternate alleles.
	#vcffilter -f "PAIRED > 0.05 & PAIREDR > 0.05 & PAIREDR / PAIRED < 1.75 & PAIREDR / PAIRED > 0.25 | PAIRED < 0.05 & PAIREDR < 0.05" -s DP3g95p5maf05.fil3.vcf > DP3g95p5maf05.fil4.vcf
/programs/vcflib-1.0.1/bin/vcffilter -f "PAIRED > 0.05 & PAIREDR > 0.05 & PAIREDR / PAIRED < 1.75 & PAIREDR / PAIRED > 0.25 | PAIRED < 0.05 & PAIREDR < 0.05" -s DP3g95p5maf05.fil3.vcf > DP3g95p5maf05.fil4.vcf			
awk '!/#/' DP3g95p5maf05.fil4.vcf | wc -l
3264>3196
/programs/vcflib-1.0.1/bin/vcffilter -f "PAIRED > 0.05 & PAIREDR > 0.05 & PAIREDR / PAIRED < 1.75 & PAIREDR / PAIRED > 0.25 | PAIRED < 0.05 & PAIREDR < 0.05" -s DP3g95p5maf05_simonly.fil3.vcf > DP3g95p5maf05_simonly.fil4.vcf			
awk '!/#/' DP3g95p5maf05_simonly.fil4.vcf | wc -l
4148>4066
	#In short, with whole genome samples, it was found that high coverage can lead to inflated locus quality scores. Heng proposed that for read depths greater than the mean depth plus 2-3 times the square root of mean depth that the quality score will be twice as large as the depth in real variants and below that value for false variants.
	#I actually found that this is a little too conservative for RADseq data, likely because the reads aren’t randomly distributed across contigs. I implement two filters based on this idea. the first is removing any locus that has a quality score below 1/4 of the depth.
		#This part is complicated - do it with just all samples first
/programs/vcflib-1.0.1/bin/vcffilter -f "QUAL / DP > 0.25" DP3g95p5maf05.fil4.vcf > DP3g95p5maf05.fil5.vcf
cut -f8 DP3g95p5maf05.fil5.vcf | grep -oe "DP=[0-9]*" | sed -s 's/DP=//g' > DP3g95p5maf05.fil5.DEPTH
awk '!/#/' DP3g95p5maf05.fil5.vcf | cut -f1,2,6 > DP3g95p5maf05.fil5.vcf.loci.qual
awk '{ sum += $1; n++ } END { if (n > 0) print sum / n; }' DP3g95p5maf05.fil5.DEPTH
22610.8 #mean depth			#his was 1.9k so this is a lot higher
#Now the the mean plus 3X the square of the mean - done myself in google/wolfram
23061.9
paste DP3g95p5maf05.fil5.vcf.loci.qual DP3g95p5maf05.fil5.DEPTH | awk -v x=23061 '$4 > x' | awk '$3 < 2 * $4' > DP3g95p5maf05.fil5.lowQDloci
		#this will remove 63 more loci
vcftools --vcf DP3g95p5maf05.fil5.vcf --site-depth --exclude-positions DP3g95p5maf05.fil5.lowQDloci --out DP3g95p5maf05.fil5
cut -f3 DP3g95p5maf05.fil5.ldepth > DP3g95p5maf05.fil5.site.depth
	#Now let’s calculate the average depth by dividing the above file by the number of individuals 31
		#I have 143 individuals
awk '!/D/' DP3g95p5maf05.fil5.site.depth | awk -v x=143 '{print $1/x}' > meandepthpersite
		#Plot myself
#Date: Oct 6th
			#Again, a very different distribution than his which goes from 20-150 as a right-squeed bell curve
		#Loci that have high mean depth are indicative of either paralogs or multicopy loci. Either way we want to remove them. 
		#Here, I’d remove all loci above a mean depth of 102.5.
		#Based on my graph, I'd remove all the ones near 500, BUT are all of them too high? Do I have lots of paralogs?
			#Actually, still higher but not as bad as I thought
			#I will remove all > 200
vcftools --vcf  DP3g95p5maf05.fil5.vcf --recode-INFO-all --out DP3g95p5maf05.FIL --max-meanDP 200 --exclude-positions DP3g95p5maf05.fil5.lowQDloci --recode 
After filtering, kept 2926 out of a possible 3194 Sites
	#repeat with similis only
/programs/vcflib-1.0.1/bin/vcffilter -f "QUAL / DP > 0.25" DP3g95p5maf05_simonly.fil4.vcf > DP3g95p5maf05_simonly.fil5.vcf
cut -f8 DP3g95p5maf05_simonly.fil5.vcf | grep -oe "DP=[0-9]*" | sed -s 's/DP=//g' > DP3g95p5maf05_simonly.fil5.DEPTH
awk '!/#/' DP3g95p5maf05_simonly.fil5.vcf | cut -f1,2,6 > DP3g95p5maf05_simonly.fil5.vcf.loci.qual
awk '{ sum += $1; n++ } END { if (n > 0) print sum / n; }' DP3g95p5maf05_simonly.fil5.DEPTH
20461.9 #mean depth			#his was 1.9k so this is a lot higher
#Now the the mean plus 3X the square of the mean - done myself in google/wolfram
20891
paste DP3g95p5maf05_simonly.fil5.vcf.loci.qual DP3g95p5maf05_simonly.fil5.DEPTH | awk -v x=20891 '$4 > x' | awk '$3 < 2 * $4' > DP3g95p5maf05_simonly.fil5.lowQDloci
		#this will remove 90 more loci
vcftools --vcf DP3g95p5maf05_simonly.fil5.vcf --site-depth --exclude-positions DP3g95p5maf05_simonly.fil5.lowQDloci --out DP3g95p5maf05_simonly.fil5
cut -f3 DP3g95p5maf05_simonly.fil5.ldepth > DP3g95p5maf05_simonly.fil5.site.depth
	#Now let’s calculate the average depth by dividing the above file by the number of individuals 31
		#I have 143 individuals
awk '!/D/' DP3g95p5maf05_simonly.fil5.site.depth | awk -v x=143 '{print $1/x}' > meandepthpersite_simonly
		#Plot myself
			#I will remove all > 200 for this also
vcftools --vcf  DP3g95p5maf05_simonly.fil5.vcf --recode-INFO-all --out DP3g95p5maf05_simonly.FIL --max-meanDP 200 --exclude-positions DP3g95p5maf05_simonly.fil5.lowQDloci --recode 
After filtering, kept 3749 out of a possible 4059 Sites
	#The next filter to apply is HWE.
	#We don’t want to apply it across the board, since population structure will create departures from HWE as well. We need to apply this by population. 
#curl -L -O https://github.com/jpuritz/dDocent/raw/master/scripts/filter_hwe_by_pop.pl
#chmod +x filter_hwe_by_pop.pl
	#Let’s filter our SNPs by population specific HWE First, we need to convert our variant calls to SNPs To do this we will use another command from vcflib called vcfallelicprimatives
	#***It is possible that I should do this by site rather than pop***
	#ERROR, VALUES TOO HIGH I MIGHT HAVE TAKEN THE WRONG FILE INPUT HERE
/programs/vcflib-1.0.1/bin/vcfallelicprimitives DP3g95p5maf05.FIL.recode.vcf --keep-info --keep-geno > DP3g95p5maf05.prim.vcf
vcftools --vcf DP3g95p5maf05.prim.vcf --remove-indels --recode --recode-INFO-all --out SNP.DP3g95p5maf05
./filter_hwe_by_pop.pl -v SNP.DP3g95p5maf05.recode.vcf -p popmap -o SNP.DP3g95p5maf05.HWE -h 0.001 #what is this last parameter?
Kept 2935 of a possible 3017 loci (filtered 82 loci)   #with h=0.01 (filtered 117 loci)
#simonly
/programs/vcflib-1.0.1/bin/vcfallelicprimitives DP3g95p5maf05_simonly.FIL.recode.vcf --keep-info --keep-geno > DP3g95p5maf05_simonly.prim.vcf
vcftools --vcf DP3g95p5maf05_simonly.prim.vcf --remove-indels --recode --recode-INFO-all --out SNP.DP3g95p5maf05_simonly
./filter_hwe_by_pop.pl -v SNP.DP3g95p5maf05_simonly.recode.vcf -p popmap -o SNP.DP3g95p5maf05_simonly.HWE -h 0.001
Kept 3777 of a possible 3868 loci (filtered 91 loci)   #with h=0.01  (filtered 132 loci)
	#J.P.: Note, I would not normally use such a high -h value. It’s purely for this example. Typically, errors would have a low p-value and would be present in many populations.
		#So redo with a lower -h value
			#    -h, --hwe
    		#   Minimum cutoff for Hardy-Weinberg p-value (for test as
 	        #   implemented in vcftools) [Default: 0.001]
 	        #Use default value
	#We have now created a thoroughly filtered VCF, and we should have confidence in these SNP calls.
		#WOOOHOOOO
	#However, I can also look for haplotypes in the bam files LATER using a script linked on this page
#Now filter to a separate file only one SNP per contig
	#Ideas
	#Sort file by first column and take first unique entry only
#This filtered the FIRST locus per contig
	#sort -k1,1 -u file
	grep -v "#" SNP.DP3g95p5maf05_simonly.HWE.recode.vcf | wc -l && grep -v "#" SNP.DP3g95p5maf05_simonly.HWE.recode.vcf | sort -k1,1 -u | wc -l
		3777>1022
		#Well darn
	grep -v "#" SNP.DP3g95p5maf05.HWE.recode.vcf | wc -l && grep -v "#" SNP.DP3g95p5maf05.HWE.recode.vcf | sort -k1,1 -u | wc -l
		2935 > 796
#save to new file
grep "#" SNP.DP3g95p5maf05_simonly.HWE.recode.vcf > SNP_simonly.head.vcf
grep -v "#" SNP.DP3g95p5maf05_simonly.HWE.recode.vcf | sort -k1,1 -u > SNP_simonly.data.vcf
cat SNP_simonly.head.vcf SNP_simonly.data.vcf > SNP_JPfilter105noLD_simonly.vcf
grep "#" SNP.DP3g95p5maf05.HWE.recode.vcf > SNP.head.vcf
grep -v "#" SNP.DP3g95p5maf05.HWE.recode.vcf | sort -k1,1 -u > SNP.data.vcf
cat SNP.head.vcf SNP.data.vcf > SNP_JPfilter105noLD.vcf
#save the not LD filtered as well to names I recognize
cp SNP.DP3g95p5maf05_simonly.HWE.recode.vcf SNP_JPfilter105a_simonly.vcf
cp SNP.DP3g95p5maf05.HWE.recode.vcf SNP_JPfilter105a.vcf
#these were moved to 
.../filterDoc_105/finish_105
#Woot

#PCA from vcf - the jank way
#remove header (acutally I may not need to if R ignores #)
#remove # from CHROM so R reads header
#copy and paste header and first few columns to new tab
#left(,3)
#remove any loci with more than 2 alleles


#WHY IS THERE 0|0 < that in this?? UUUGG


#LATE OCTOBER CONSIDER:
#What might I do differently with dDocent? Reduce number of MWs in catalog creation

#Date: Nov 8th
#How to call haplotype data from dDocent pipeline/Freebayes
-E This parameter affects how haplotypes are called in the VCF file. The default value is 3, meaning that variants within 3 bp of each other will be called as a single contiguous haplotype. This represents the highest tradeoff between haplotype length and sensitivity. See this discussion for more information.

#Note for SNP data: 
To properly look at SNPs only, complex variants need to be decomposed with vcfallelicprimatives from the vcflib package and then INDELs can be filtered with VCFtools or vcflib.

#Haplotype data into adegenet to allelic
#May not require remapping - after filtering I can do an added step to turn it into haplotype data
#Include indels with haplotypes (start with just SNP data first)


#Date: Oct 25th
#Fix similis removing BLP in 1012
#And sneaky solidissima
#first four steps are the same
mkdir filterdDoc_1012sf
cd filterdDoc_1012sf
cp ../DP3g95maf05.recode.vcf .
cp ../popmap .

#create better keep lists
awk '$2 == "GBE\r" || $2 == "SFJ\r" || $2 == "CCB\r" || awk $2 == "SLI\r"' popmap > 1.keep 
awk '$2 == "SCC\r"' popmap > 5.keep
awk '$2 == "NLI\r"' popmap > 6.keep && awk '$2 == "GA\r"' popmap > 7.keep
#remove specific ones
nano 5.keep
#confirm which to remove
less ../../STRUCTURE/filter_1012/my_run_sim_K3.log 
	#CTM_008
	#MW_001
	#MW_002
	#MW_016
	#MW_033
for f in {5..7}
do
	vcftools --vcf DP3g95maf05.recode.vcf --keep $f.keep --missing-site --out $f
done
cat 5.lmiss 6.lmiss 7.lmiss | awk '!/CHR/' | awk '$6 > 0.1' | cut -f1,2 >> badloci_simonly

#change the soldis for remove
cp ../../filterDoc_105/soldis4rm.indv .
nano soldis4rm.indv
#above plus:
	#BLP0819_180
	#BLP0819_182 
	#BLP0819_179
#saved as: solrm_update1025.indv
vcftools --vcf DP3g95maf05.recode.vcf --exclude-positions badloci_simonly --remove solrm_update1025.indv --recode --recode-INFO-all --out DP3g95p5maf05_simonly
#kept 126 out of 143 individuals
	#Where were the other 7? Poor quality prob
/programs/vcflib-1.0.1/bin/vcffilter -s -f "AB > 0.2 & AB < 0.8 | AB < 0.01" DP3g95p5maf05_simonly.recode.vcf > DP3g95p5maf05_simonly.fil1.vcf
awk '!/#/' DP3g95p5maf05_simonly.recode.vcf | wc -l && awk '!/#/' DP3g95p5maf05_simonly.fil1.vcf | wc -l
		26343>18397 #loci
/programs/vcflib-1.0.1/bin/vcffilter -f "SAF / SAR > 100 & SRF / SRR > 100 | SAR / SAF > 100 & SRR / SRF > 100" -s DP3g95p5maf05_simonly.fil1.vcf > DP3g95p5maf05_simonly.fil2.vcf
awk '!/#/' DP3g95p5maf05_simonly.fil2.vcf | wc -l
		17287
/programs/vcflib-1.0.1/bin/vcffilter -f "MQM / MQMR > 0.9 & MQM / MQMR < 1.05" DP3g95p5maf05_simonly.fil2.vcf > DP3g95p5maf05_simonly.fil3.vcf
awk '!/#/' DP3g95p5maf05_simonly.fil3.vcf | wc -l
		16673
/programs/vcflib-1.0.1/bin/vcffilter -f "PAIRED > 0.05 & PAIREDR > 0.05 & PAIREDR / PAIRED < 1.75 & PAIREDR / PAIRED > 0.25 | PAIRED < 0.05 & PAIREDR < 0.05" -s DP3g95p5maf05_simonly.fil3.vcf > DP3g95p5maf05_simonly.fil4.vcf			
awk '!/#/' DP3g95p5maf05_simonly.fil4.vcf | wc -l
		16285
/programs/vcflib-1.0.1/bin/vcffilter -f "QUAL / DP > 0.25" DP3g95p5maf05_simonly.fil4.vcf > DP3g95p5maf05_simonly.fil5.vcf
cut -f8 DP3g95p5maf05_simonly.fil5.vcf | grep -oe "DP=[0-9]*" | sed -s 's/DP=//g' > DP3g95p5maf05_simonly.fil5.DEPTH
awk '!/#/' DP3g95p5maf05_simonly.fil5.vcf | cut -f1,2,6 > DP3g95p5maf05_simonly.fil5.vcf.loci.qual
awk '{ sum += $1; n++ } END { if (n > 0) print sum / n; }' DP3g95p5maf05_simonly.fil5.DEPTH
		16291.6
#Now the the mean plus 3X the square root of the mean - done myself in google/wolfram
		16674.5
paste DP3g95p5maf05_simonly.fil5.vcf.loci.qual DP3g95p5maf05_simonly.fil5.DEPTH | awk -v x=16674.5 '$4 > x' | awk '$3 < 2 * $4' > DP3g95p5maf05_simonly.fil5.lowQDloci
vcftools --vcf DP3g95p5maf05_simonly.fil5.vcf --site-depth --exclude-positions DP3g95p5maf05_simonly.fil5.lowQDloci --out DP3g95p5maf05_simonly.fil5
cut -f3 DP3g95p5maf05_simonly.fil5.ldepth > DP3g95p5maf05_simonly.fil5.site.depth
	#Now let’s calculate the average depth by dividing the above file by the number of individuals 31
		#I have 126 individuals
awk '!/D/' DP3g95p5maf05_simonly.fil5.site.depth | awk -v x=125 '{print $1/x}' > meandepthpersite_simonly
vcftools --vcf  DP3g95p5maf05_simonly.fil5.vcf --recode-INFO-all --out DP3g95p5maf05_simonly.FIL --max-meanDP 200 --exclude-positions DP3g95p5maf05_simonly.fil5.lowQDloci --recode 
		12047 out of a possible 13269 Sites
#save to new file
grep "#" DP3g95p5maf05_simonly.FIL.recode.vcf > SNP_simonly.head.vcf
grep -v "#" DP3g95p5maf05_simonly.FIL.recode.vcf | sort -k1,1 -u > SNP_simonly.data.vcf
cat SNP_simonly.head.vcf SNP_simonly.data.vcf > SNP_JPfilter1012sf_noLD_simonly.vcf

#these were in 
.../filterDoc_1012/filterDoc_1012sf

#Do PCA with non-LD sites
#How do prep vcf?
#Looks like basically just download
#I could also try converting it to a different thing using PGBspider

#remove loci with more than two alleles in the future before excel step
	#FILTER FOR THAT BEFORE FILTERING TO ONE SNP PER RADTAG
	#Slash also remove non-SNPS, like insertions and stuff

#Date: Oct 31st
#Align to similis transcriptome
..trinity_refs/Trinity_sim.fasta
#BWA 0.7.17	
#full path:	/programs/bwa-0.7.17/bwa [options]
/programs/bwa-0.7.17/bwa
mem ref.fa reads.fq > aln-se.sam
bwa mem ref.fa read1.fq read2.fq > aln-pe.sam
#take reads from synctrim6
../similis_clean_dups/sync_trim_6/"$p"_R1.trim.sync.fastq ../similis_clean_dups/sync_trim_6/"$p"_R2.trim.sync.fastq >& similis_clean_dups/sync_trim_6/"$p"_resync6.log

/programs/bwa-0.7.17/bwa mem ..trinity_refs/Trinity_sim.fasta ../similis_clean_dups/sync_trim_6/"$p"_R1.trim.sync.fastq ../similis_clean_dups/sync_trim_6/"$p"_R2.trim.sync.fastq > aln_"$p".sam >& "$p"_bwalog.log
/programs/bwa-0.7.17/bwa index ../trinity_refs/Trinity_sim.fasta


while read p; do
echo "echo $p"
echo "/programs/bwa-0.7.17/bwa mem ../trinity_refs/Trinity_sim.fasta ../similis_clean_dups/sync_trim_6/"$p"_R1.trim.sync.fastq ../similis_clean_dups/sync_trim_6/"$p"_R2.trim.sync.fastq > aln_"$p".sam >& "$p"_bwalog.log"
done <../sim_sample.txt > j_bwa.txt


#!/bin/bash -l

#SBATCH --partition=regular
#SBATCH --job-name=j_sstacks_thread30_mem160G
#SBATCH --output=sstacks_jlist_test.out
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --mail-user=hh693@cornell.edu
parallel -j 30 < j_bwa.txt


sbatch --nodes=1 --ntasks=30 --mem=160000 s_bwa_111.sh

/programs/bwa-0.7.17/bwa index ../trinity_refs/Trinity_sim.fasta

parallel -j 30 < j_bwa.txt

#Date: Nov 13th
#Instead take the reads from somewhere else - filtered but not filtered for paired reads only

#But for the ones that I have (bwa finishes with .sam), how do I get to vcf
#.sam > .bam > sorted bam > m pileup > vcf
#https://www.ebi.ac.uk/sites/ebi.ac.uk/files/content.ebi.ac.uk/materials/2014/140217_AgriOmics/dan_bolser_snp_calling.pdf
samtools view -S -b my.sam > my.bam
samtools sort my.bam my-sorted
samtools faidx my.fasta
samtools mpileup -g -f my.fasta my-sorted1.bam my-sorted-2.bam my-sorted-n.bam > myraw.bcf
bcftools view -bvcg my-raw.bcf > my-var.bcf
#filtering step bcftools view my.var.bcf | vcfutils.pl varFilter - > my.var-final.vcf


while read p; do
echo "echo $p"
echo "samtools view -S -b aln_$p.sam > $p.bam"
done <../sim_sample.txt > j_sam2vcf.txt
while read p; do
echo "echo $p"
echo "samtools sort $p.bam $p_sort"
done <../sim_sample.txt | cat >> j_sam2vcf.txt
samtools faidx ../trinity_refs/Trinity_sim.fasta &

sbatch --nodes=1 --ntasks=24 --mem=150000 /
--job-name="j_sam2vcf_t24_mem150G" --output=sstacks_nov13.out s_sam2vcf.sh
#failed to read the headers
#alignments are empty, awesome <- data was saved to the logs
#Then mileup from only the similis

#Try with one sample to see what happens
p="ELP_010_S17"
 /programs/bwa-0.7.17/bwa mem ../trinity_refs/Trinity_sim.fasta ../similis_clean_dups/sync_trim_6/"$p"_R1.trim.sync.fastq ../similis_clean_dups/sync_trim_6/"$p"_R2.trim.sync.fastq > aln_"$p".sam >& "$p"_bwalog.log

#Figure out what reads to use for the unpaired alignment instead
#https://bioinformaticsworkbook.org/dataAnalysis/VariantCalling/freebayes-dnaseq-workflow.html#gsc.tab=0
#What if I just use dDocent for the alignment (it's just bwa + freebayes) with the reference being my fasta
	#Yes
	#Set up folder with ddocent set up inside GBS2transcriptome
	#Then find code and trimmed reads
	mkdir dDoc_aln2transcriptome_nov13
	cp ../trinity_refs/Trinity_sim.fasta dDoc_aln2transcriptome_nov13/reference.fasta

#To set up reads have names with .F and .R1

#Which ones do I want?
similis_clean_dups/trim_R1s_phase2/"$p"_R1.trim.fastq 
similis_clean_dups/ommatic_phase2/"$p"_R2.fastq

cp ../similis_clean_dups/ommatic_phase2/*_R2.fastq ./dDoc_aln2transcriptome_nov13 &
cp ../similis_clean_dups/trim_R1s_phase2/*_R1.trim.fastq ./dDoc_aln2transcriptome_nov13 &
#Rename using excel
parallel -j 24 < j_zip.txt
#touch to create psuedo untrimmed

#start without config file first to get graphs etc
#don't need to unless assembly
dDocent config.file



touch dDoc1113.sh
nano dDoc1113.sh

#!/bin/bash -l

#SBATCH --partition=regular
#SBATCH --job-name=temp
#SBATCH --output=temp.out
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --mail-user=hh693@cornell.edu
source /programs/miniconda3/bin/activate dDocent-2.8.13
dDocent config_1113.txt

sbatch --nodes=1 --ntasks=24 --mem=120000 \
--job-name=dDoc1113_th24_mem120 --output=dDoc1113.out dDoc1113.sh

touch config_1113.txt

Number of Processors
24
Maximum Memory
0
Trimming
no
Assembly?
no
Mapping_Reads?
yes
Mapping_Match_Value
1
Mapping_MisMatch_Value
4
Mapping_GapOpen_Penalty
6
Calling_SNPs?
yes
Email
hh693@cornell.edu

#PURITZ HAPLOTYPE CALLING
curl -L -O https://raw.githubusercontent.com/chollenbeck/rad_haplotyper/master/rad_haplotyper.pl
chmod +x rad_haplotyper.pl
#Do it first in dDoc_928/undedup > AKA filterDoc_1012
#filterdDoc_1012sf (no sneakies)
#and final_filter1012
#which one was right before noLD?

cp ../filterDoc_1012/DP3g95p5maf05.FIL.recode.vcf ./filter1012/JPfilter_all.vcf
cp ../filterDoc_1012/filterdDoc_1012sf/DP3g95p5maf05_simonly.FIL.recode.vcf ./filter1012/JPfilter_sim_nosneaks.vcf

#needs the bam files so I may not want to do it in this folder
perl rad_haplotyper.pl
Cant locate Bio/Cigar.pm in @INC (you may need to install the Bio::Cigar module) (@INC contains: /home/hh693/perl5/lib/perl5 /usr/local/lib/perl5/site_perl/5.22.0/x86_64-linux-thread-multi /usr/local/lib/perl5/site_perl/5.22.0 /usr/local/lib/perl5/5.22.0/x86_64-linux-thread-multi /usr/local/lib/perl5/5.22.0 /usr/local/lib/perl5/site_perl/5.12.2 /usr/local/lib/perl5/site_perl .) at rad_haplotyper.pl line 7.
BEGIN failed--compilation aborted at rad_haplotyper.pl line 7.
#Bioperl?


#Heterozygosity
#Output allele frequency for all sites in the input vcf file from chromosome 1
#vcftools --gzvcf input_file.vcf.gz --freq --chr 1 --out chr1_analysis
vcftools --vcf SNP_JPfilter1012sf_noLD_simonly.vcf --freq --out sim_noLD_freq
	#Gives freq of literal alleles
#balls, the thing I used for this before was an output from STACKS population
#heterozygosity per locus vcf?
	#https://www.biostars.org/p/291147/
	#Maybe bcftools
	#maybe vcflib which I like better vcflib
	#vcfhetcount	Calculate the heterozygosity rate: count the number of alternate alleles in heterozygous genotypes in all records in the vcf file
			#No
			#/programs/vcflib-1.0.1/bin/vcfhetcount SNP_JPfilter1012sf_noLD_simonly.vcf > testa
	#Maybe vcfsitesummarize
		#Lots of stuff in there, not sure any of it is helpful
	#genotypeSummary? No
	#OOOH! popStats
/programs/vcflib-1.0.1/bin/popStats --type GL \
--target 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124 \
--file SNP_JPfilter1012sf_noLD_simonly.vcf > testa
	#WOOHOO
	#outputs 9 collumns
	 #1. seqid                        
     #2. position                     
     #3. target allele frequency      
     #4. expected heterozygosity      
     #5. observed heterozygosity      
     #6. number of hets               
     #7. number of homozygous ref     
     #8. number of homozygous alt     
     #9. target Fis
#No pop IDS
#... BECAUSE I DID IT BY INDIVIDUAL NOT LOCUS ARG
	#SO INSTEAD, I COULD DO THAT OR I COULD STILL DO HET BY LOCUS BY SITE
		#I just need to set up the target arrays correctly and then merge tables
		#CHROM  POS     ID      REF     ALT     QUAL    FILTER  INFO    FORMAT  CTM_001 CTM_002 CTM_003 CTM_004 CTM_005 CTM_006 CTM_007 CTM_009 CTM_010 ELP_001 ELP_002 ELP_003 ELP_004 ELP_005 ELP_006 ELP_007 ELP_008 ELP_009 ELP_010 GA12_001        GA12_002        GA12_003        GA12_004        GA12_006        GA12_007        GA12_008        GA12_009        GA12_010        GA12_012        GA12_013        GA12_014        GA12_015        GA12_016        GA12_017        GA12_018        GA12_019        GA12_020        GA12_021        GA12_022        GA12_023        GA12_024        GA12_025        GLD0819_001     GLD0819_002 GLD0819_003     GLD0819_004     GLD0819_005     GLD0819_006     GLD0819_007     GLD0819_008     MW_003  MW_004  MW_005 MW_006  MW_007  MW_008  MW_010  MW_011  MW_014  MW_015  MW_018  MW_019  MW_01   MW_022  MW_023  MW_024  MW_025  MW_026  MW_027  MW_028  MW_029  MW_030  MW_031  MW_032  MW_034  MW_035  MW_036  MW_037  MW_038  MW_039  MW_040  MW_041  MW_042  MW_043  MW_044  MW_045  MW_046  MW_047  MW_048  MW_049  MW_050  MW_051  MW_052  MW_053  MW_054  MW_057  MW_058  MW_059  MW_060  MW_061  MW_062  MW_063  MW_064  MW_065  MW_066  MW_067  PEC0819_011     PEC0819_012     PEC0819_013     PPB_001 PPB_002 PPB_003 PPB_004 PPB_005 PPB_006 RP20_009        RP20_010        WFH_001 WFH_002 WFH_003 WFH_004 WFH_005 WFH_006 WFH_007 WFH_008
/programs/vcflib-1.0.1/bin/popStats --type GL \
--target 20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41 \
--file SNP_JPfilter1012sf_noLD_simonly.vcf > popStats_simnoLD_GA.txt
/programs/vcflib-1.0.1/bin/popStats --type GL \
--target 42,43,44,45,46,47,48,49,106,107,108,109,115,116 \
--file SNP_JPfilter1012sf_noLD_simonly.vcf > popStats_simnoLD_NLI.txt
/programs/vcflib-1.0.1/bin/popStats --type GL \
--target 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,109,110,111,112,113,114,117,118,119,120,121,122,123,124 \
--file SNP_JPfilter1012sf_noLD_simonly.vcf > popStats_simnoLD_SCC.txt

#Okay, but then if I want it per individual...
vcftools --vcf SNP_JPfilter1012sf_noLD_simonly.vcf --het --out het_simonly_analysis
#Nope

/programs/vcflib-1.0.1/bin/vcfstats SNP_JPfilter1012sf_noLD_simonly.vcf >teststats
#https://plantgenomics.univie.ac.at/radseq/snpfiltering/
	# ^ Useful figures but also uses populations to get the job done
	#Recommendation: use my other R package RADIATOR to filter data based on heterozygosity.
	#https://thierrygosselin.github.io/radiator/articles/get_started.html

#I could do it instead with grep
	#Filter to SNPs only (should do anyway - before 1per radtag)
	#Count number of 0/1s per line
	#Print a table of those thing (no wait, I already HAVE THAT!)
	#I don't need to do any grep stuff or in R, just make things as filter or not in my excel sheet
	#Then make a list of the ones to filter (AGAIN, DO BEFORE 1 per RADTAG)
	#vcftools with the list of either good or bad loci
	
#Date: Nov 14th
#Filter Trinity Alignment
#Haplotype Data from filtering
#New packages uploaded to server
export PERL5LIB=/programs/PERL/lib/perl5
perl rad_haplotyper.pl

#Ready to work
#options and how to best get the bam files
-x #threads
-n 		#use indels
#Did I grab the one before no LD? Probably
#I think it will be easiest to just do it in the folder with the bams
/workdir/hh693/dDoc_928/undedup
#cp
cp /workdir/hh693/haplotypecalling/filter1012/*.vcf /workdir/hh693/dDoc_928/undedup
#the vcfs I want are labeled haptest_

perl rad_haplotyper.pl -v haptest_JPfilter_sim_nosneaks.vcf -x 20 -n -o haptest_JP_haps.vcf -r reference.fasta
[W::hts_idx_load3] The index file is older than the data file: GA12_001-RG.bam.bai
		#Few with the error. Also:
			#GA12
			#MW28
Filtered 237 loci below missing data cutoff
Filtered 0 possible paralogs
Filtered 0 loci with low coverage or genotyping errors
Filtered 0 loci with an excess of haplotypes

#Takes about 10 minutes
#I need to tell it to save as a vcf
#rad_haplotyper.pl -v SNP.DP3g95p5maf05.HWE.recode.vcf -x 40 -mp 1 -u 20 -ml 4 -n -r reference.fasta
haptest_JP_haps.vcf #<- output
#vcftools summarize?
	#so grep
grep -v "##" haptest_JP_haps.vcf | wc -l
9672

#WOOT!
#Why are there more SNPs in that?
grep -v "##" haptest_JPfilter_sim_nosneaks.vcf | wc -l
12047
#Nevermind, this was before only one per contig

#save to new file
grep "#" haptest_JP_haps.vcf > haps_sim.head.vcf
grep -v "#" haptest_JP_haps.vcf | sort -k1,1 -u > haps_sim.data.vcf
cat haps_sim.head.vcf haps_sim.data.vcf > haps_noLD_simonly.vcf
grep -v "##" haps_noLD_simonly.vcf | wc -l
1317

#Transcriptome filtering 1113
#Where's the vcf?
#The output is stored in a file called mapping.results
dDoc1113.out
dDoc1113.sh
dDocent_main.LOG
dDocent.runs
lengths.txt
#had an issue with the empty files
parallel -j 12 < p_cp.txt 
#Do them all at one at one

#Date: Nov 15th
#Rerun with fixed files
#Test with fixed files, still call it 1113
	#took 2 hours last time, probably still a lot more when it actually works
	#Acctually also only took 2 hours.... Did it work?
	#NO
	ListUtil.c: loadable library and perl binaries are mismatched (got handshake key 0xdb00080, needed 0xed00080)
	*****
	***** ERROR: Requested column 4, but database file - only has fields 1 - 0.
	ListUtil.c: loadable library and perl binaries are mismatched (got handshake key 0xdb00080, needed 0xed00080)
	mawk: cannot open lengths.txt (No such file or directory)
	/programs/miniconda3/envs/dDocent-2.8.13/bin/dDocent: line 453: / 2 : syntax error: operand expected (error token is "/ 2 ")
	#Review specifically how to use dDoc to align to reference
		#It just says fasta
		#also missing lengths file
		#Compare to the other one
	https://stackoverflow.com/questions/45000585/listutil-c-loadable-library-and-perl-binaries-are-mismatched-got-handshake-key
	env | grep PERL
	Having PERL5LIB or PERL_LOCAL_LIB_ROOT in the output might cause the issue
	#Definitely could be the problem
	
	#Okay try removing PERL5LIB and PERL_LOCAL_LIB_ROOT from my ~/.bashrc, then reopen terminal and it worked for me.
	#Then, I can put it back together later: it looked like this:
PATH="/home/hh693/perl5/bin${PATH:+:${PATH}}"; export PATH;
PERL5LIB="/home/hh693/perl5/lib/perl5${PERL5LIB:+:${PERL5LIB}}"; export PERL5LIB;
PERL_LOCAL_LIB_ROOT="/home/hh693/perl5${PERL_LOCAL_LIB_ROOT:+:${PERL_LOCAL_LIB_ROOT}}"; export PERL_LOCAL_LIB_ROOT;
PERL_MB_OPT="--install_base \"/home/hh693/perl5\""; export PERL_MB_OPT;
PERL_MM_OPT="INSTALL_BASE=/home/hh693/perl5"; export PERL_MM_OPT;


source /programs/miniconda3/bin/activate dDocent-2.8.13
dDocent

Press control and Z simultaneously
Type bg and press enter
Type disown -h and press enter

kill 6924
kill 6760
#Run on another server!
cp -r /workdir/hh693/GBS2transcriptome/dDoc_aln2transcriptome_nov13 /local/storage/Spisula/Mounting
/programs/bin/labutils/mount_server cbsuhare /storage


sbatch --nodes=1 --ntasks=24 --mem=120000 \
--job-name=dDoc1113_th24_mem120 --output=dDoc1113_serv03.out dDoc1113.sh

#dDocent might expected paired end reads only and the unresynced are messing it up?
	#I'm still not sure it's working...
	#My 928 run does not have saved bam logs? Are they temp?
	
	#Started at Mon Nov 15 08:14:31 EST 2021
	#If it takes longer than till 11:00 then it's probably working
		#The bams are not 0 big! Probably working
		
#Prep filtering and haplotype steps
Creating alignment intervals

*****
***** ERROR: Requested column 4, but database file - only has fields 1 - 0.

Using FreeBayes to call SNPs
ls: cannot access 'mapped.*.bed': No such file or directory
0% 0:0=0s                                                                                                                                               
rm: cannot remove 'mapped.*.bed': No such file or directory
could not open "raw.*.vcf" exiting
mv: cannot stat 'raw.*.vcf': No such file or directory

Using VCFtools to parse TotalRawSNPS.vcf for SNPs that are called in at least 90% of individuals

dDocent has finished with errors in /workdir/hh693/dDoc_aln2transcriptome_nov13 

dDocent started Mon Nov 15 08:14:31 EST 2021 

dDocent finished Mon Nov 15 11:42:49 EST 2021 

Please check log files

After filtering, kept 0 out of a possible 0 Sites 

dDocent 2.8.13 
The 'd' is silent, hillbilly.

#Is there no config file in here? How has it been running at all then???
#NONE IF MY 9_28 EITHER?

#Run interactively?
#Needs to run mapping cause it did not save map bed
You have new mail in /var/spool/mail/hh693
#WHAT? <- I typed the wrong email to try to email myself when ddoc was finished

#Remove solidis and sneaky sol

422_014	sol
422_033	sol
BAR_002	sol
BLP0819_179	sol
BLP0819_180	sol
BLP0819_182	sol
CTM_008	sol
CUP0819_392	sol
GBE0819_171	sol
GBE0819_172	sol
GBE_332	sol
GBE_333	sol
GBE_335	sol
MW_001	sol
MW_002	sol
MW_016	sol
MW_033	sol

#First do with a subset of similis
#Start with only the R,F,R1,R2 gzips
	#And reference fasta
source /programs/miniconda3/bin/activate dDocent-2.8.13
dDocent
#Run interactive

#Still same error
#Try adding updated bedtools to path
export PATH=/programs/bedtools2-2.29.2/bin:$PATH
#Try with only 4 samples

#Still not wroking - came error
ERROR: Requested column 4, but database file - only has fields 1 - 0.

#Try doing dDocent from the beginning or bwa independent instead
5
3
/programs/miniconda3/envs/dDocent-2.8.13/bin/dDocent: line 1006: 28148 Segmentation fault      (core dumped) rainbow div -i rcluster -o rbdiv.out -f 0.5 -K 10
/programs/miniconda3/envs/dDocent-2.8.13/bin/dDocent: line 1047: / 100 + 1: syntax error: operand expected (error token is "/ 100 + 1")
You have new mail in /var/spool/mail/hh693

Rainbow is giving you an error because it appears that these reads were
trimmed and are not uniform length which is a requirement for both ddocent
and rainbow for assembly.

#Instead try a thing from .bam to snps
#Using bams from previous test
rename -- -RG. . *.bam* #-- tells it to ignore -R as an option


export PATH=/programs/freebayes-1.3.5/bin:$PATH
export PATH=/programs/freebayes-1.3.5/scripts:$PATH
export PATH=/programs/vcflib-1.0.1/bin:$PATH
export PATH=/programs/vcflib-1.0.1/scripts:$PATH
freebayes-parallel <(fasta_generate_regions.py ref.fa.fai 100000) 24 -f ref.fa aln.bam >out.vcf
## The script freebayes-parallel call freebayes with default setting. If you need to change the parameter to run freebayes, you can copy the freebayes-parallel to your workding directory:  cp /programs/freebayes-1.3.5/scripts/freebayes-parallel ./  . Then you can modify the line command=("freebayes" "$@") to add other parameters.  Testing on a small file before you run on the real job.
#To run freebayes-parallel (use 24 cores in this example, ref.fa.fai can be generated with command "samtools faidx ref.fa"):

#bamaddrg: adds read groups to input BAM files, streams BAM output on stdout.
#This is intended for use "fixing up" RG tags on the fly so that they reflect
#the source file from which the aligment originated from.  This allows the
#"safe" merging of many files from many individuals into one stream, suitable
#for input into downstream processing systems such as freebayes (a population
#variant detector).
#https://github.com/ekg/bamaddrg
	#Not sure how to install
	

bcftools mpileup [OPTIONS] -f ref.fa in.bam [in2.bam […​]]
#https://samtools.github.io/bcftools/bcftools.html#mpileup
#Just the sims
export PATH=/programs/bcftools-1.11/bin:$PATH
bcftools mpileup -f reference.fasta -o testsim1115.vcf --threads 24 CTM_001.bam CTM_002.bam CTM_003.bam CTM_004.bam CTM_005.bam CTM_006.bam CTM_007.bam CTM_009.bam CTM_010.bam ELP_001.bam ELP_002.bam ELP_003.bam ELP_004.bam ELP_005.bam ELP_006.bam ELP_007.bam ELP_008.bam ELP_009.bam ELP_010.bam GA12_001.bam GA12_002.bam GA12_003.bam GA12_004.bam GA12_006.bam GA12_007.bam GA12_008.bam GA12_009.bam GA12_010.bam GA12_012.bam GA12_013.bam GA12_014.bam GA12_015.bam GA12_016.bam GA12_017.bam GA12_018.bam GA12_019.bam GA12_020.bam GA12_021.bam GA12_022.bam GA12_023.bam GA12_024.bam GA12_025.bam GLD0819_001.bam GLD0819_002.bam GLD0819_003.bam GLD0819_004.bam GLD0819_005.bam GLD0819_006.bam GLD0819_007.bam GLD0819_008.bam MW_003.bam MW_004.bam MW_005.bam MW_006.bam MW_007.bam MW_008.bam MW_009.bam MW_010.bam MW_011.bam MW_012.bam MW_013.bam MW_014.bam MW_015.bam MW_018.bam MW_019.bam MW_017.bam MW_020.bam MW_021.bam MW_022.bam MW_023.bam MW_024.bam MW_025.bam MW_026.bam MW_027.bam MW_028.bam MW_029.bam MW_030.bam MW_031.bam MW_032.bam MW_034.bam MW_035.bam MW_036.bam MW_037.bam MW_038.bam MW_039.bam MW_040.bam MW_041.bam MW_042.bam MW_043.bam MW_044.bam MW_045.bam MW_046.bam MW_047.bam MW_048.bam MW_049.bam MW_050.bam MW_051.bam MW_052.bam MW_053.bam MW_054.bam MW_055.bam MW_056.bam MW_057.bam MW_058.bam MW_059.bam MW_060.bam MW_061.bam MW_062.bam MW_063.bam MW_064.bam MW_065.bam MW_066.bam MW_067.bam PEC0819_011.bam PEC0819_012.bam PEC0819_013.bam PPB_001.bam PPB_002.bam PPB_003.bam PPB_004.bam PPB_005.bam PPB_006.bam RP20_009.bam RP20_010.bam WFH_001.bam WFH_002.bam WFH_003.bam WFH_004.bam WFH_005.bam WFH_006.bam WFH_007.bam WFH_008.bam

[mpileup] 132 samples in 132 input files
[mpileup] maximum number of reads per input file set to -d 250
#Empty vcf file
bcftools mpileup -O v --ignore-RG -f reference.fasta -o testsim1115.vcf --threads 24 CTM_001.bam CTM_002.bam CTM_003.bam CTM_004.bam CTM_005.bam CTM_006.bam CTM_007.bam CTM_009.bam CTM_010.bam ELP_001.bam ELP_002.bam ELP_003.bam ELP_004.bam ELP_005.bam ELP_006.bam ELP_007.bam ELP_008.bam ELP_009.bam ELP_010.bam GA12_001.bam GA12_002.bam GA12_003.bam GA12_004.bam GA12_006.bam GA12_007.bam GA12_008.bam GA12_009.bam GA12_010.bam GA12_012.bam GA12_013.bam GA12_014.bam GA12_015.bam GA12_016.bam GA12_017.bam GA12_018.bam GA12_019.bam GA12_020.bam GA12_021.bam GA12_022.bam GA12_023.bam GA12_024.bam GA12_025.bam GLD0819_001.bam GLD0819_002.bam GLD0819_003.bam GLD0819_004.bam GLD0819_005.bam GLD0819_006.bam GLD0819_007.bam GLD0819_008.bam MW_003.bam MW_004.bam MW_005.bam MW_006.bam MW_007.bam MW_008.bam MW_009.bam MW_010.bam MW_011.bam MW_012.bam MW_013.bam MW_014.bam MW_015.bam MW_018.bam MW_019.bam MW_017.bam MW_020.bam MW_021.bam MW_022.bam MW_023.bam MW_024.bam MW_025.bam MW_026.bam MW_027.bam MW_028.bam MW_029.bam MW_030.bam MW_031.bam MW_032.bam MW_034.bam MW_035.bam MW_036.bam MW_037.bam MW_038.bam MW_039.bam MW_040.bam MW_041.bam MW_042.bam MW_043.bam MW_044.bam MW_045.bam MW_046.bam MW_047.bam MW_048.bam MW_049.bam MW_050.bam MW_051.bam MW_052.bam MW_053.bam MW_054.bam MW_055.bam MW_056.bam MW_057.bam MW_058.bam MW_059.bam MW_060.bam MW_061.bam MW_062.bam MW_063.bam MW_064.bam MW_065.bam MW_066.bam MW_067.bam PEC0819_011.bam PEC0819_012.bam PEC0819_013.bam PPB_001.bam PPB_002.bam PPB_003.bam PPB_004.bam PPB_005.bam PPB_006.bam RP20_009.bam RP20_010.bam WFH_001.bam WFH_002.bam WFH_003.bam WFH_004.bam WFH_005.bam WFH_006.bam WFH_007.bam WFH_008.bam

perl -e 'print "\tID:CTM_001\tSM:hs\tLB:CTM_001\tPL:Illumina\n\tID:CTM_002\tSM:hs\tLB:CTM_002\tPL:Illumina\n\tID:CTM_003\tSM:hs\tLB:CTM_003\tPL:Illumina\n\tID:CTM_004\tSM:hs\tLB:CTM_004\tPL:Illumina\n\tID:CTM_005\tSM:hs\tLB:CTM_005\tPL:Illumina\n\tID:CTM_006\tSM:hs\tLB:CTM_006\tPL:Illumina\n\tID:CTM_007\tSM:hs\tLB:CTM_007\tPL:Illumina\n\tID:CTM_009\tSM:hs\tLB:CTM_009\tPL:Illumina\n\tID:CTM_010\tSM:hs\tLB:CTM_010\tPL:Illumina\n\tID:ELP_001\tSM:hs\tLB:ELP_001\tPL:Illumina\n\tID:ELP_002\tSM:hs\tLB:ELP_002\tPL:Illumina\n\tID:ELP_003\tSM:hs\tLB:ELP_003\tPL:Illumina\n\tID:ELP_004\tSM:hs\tLB:ELP_004\tPL:Illumina\n\tID:ELP_005\tSM:hs\tLB:ELP_005\tPL:Illumina\n\tID:ELP_006\tSM:hs\tLB:ELP_006\tPL:Illumina\n\tID:ELP_007\tSM:hs\tLB:ELP_007\tPL:Illumina\n\tID:ELP_008\tSM:hs\tLB:ELP_008\tPL:Illumina\n\tID:ELP_009\tSM:hs\tLB:ELP_009\tPL:Illumina\n\tID:ELP_010\tSM:hs\tLB:ELP_010\tPL:Illumina\n\tID:GA12_001\tSM:hs\tLB:GA12_001\tPL:Illumina\n\tID:GA12_002\tSM:hs\tLB:GA12_002\tPL:Illumina\n\tID:GA12_003\tSM:hs\tLB:GA12_003\tPL:Illumina\n\tID:GA12_004\tSM:hs\tLB:GA12_004\tPL:Illumina\n\tID:GA12_006\tSM:hs\tLB:GA12_006\tPL:Illumina\n\tID:GA12_007\tSM:hs\tLB:GA12_007\tPL:Illumina\n\tID:GA12_008\tSM:hs\tLB:GA12_008\tPL:Illumina\n\tID:GA12_009\tSM:hs\tLB:GA12_009\tPL:Illumina\n\tID:GA12_010\tSM:hs\tLB:GA12_010\tPL:Illumina\n\tID:GA12_012\tSM:hs\tLB:GA12_012\tPL:Illumina\n\tID:GA12_013\tSM:hs\tLB:GA12_013\tPL:Illumina\n\tID:GA12_014\tSM:hs\tLB:GA12_014\tPL:Illumina\n\tID:GA12_015\tSM:hs\tLB:GA12_015\tPL:Illumina\n\tID:GA12_016\tSM:hs\tLB:GA12_016\tPL:Illumina\n\tID:GA12_017\tSM:hs\tLB:GA12_017\tPL:Illumina\n\tID:GA12_018\tSM:hs\tLB:GA12_018\tPL:Illumina\n\tID:GA12_019\tSM:hs\tLB:GA12_019\tPL:Illumina\n\tID:GA12_020\tSM:hs\tLB:GA12_020\tPL:Illumina\n\tID:GA12_021\tSM:hs\tLB:GA12_021\tPL:Illumina\n\tID:GA12_022\tSM:hs\tLB:GA12_022\tPL:Illumina\n\tID:GA12_023\tSM:hs\tLB:GA12_023\tPL:Illumina\n\tID:GA12_024\tSM:hs\tLB:GA12_024\tPL:Illumina\n\tID:GA12_025\tSM:hs\tLB:GA12_025\tPL:Illumina\n\tID:GLD0819_001\tSM:hs\tLB:GLD0819_001\tPL:Illumina\n\tID:GLD0819_002\tSM:hs\tLB:GLD0819_002\tPL:Illumina\n\tID:GLD0819_003\tSM:hs\tLB:GLD0819_003\tPL:Illumina\n\tID:GLD0819_004\tSM:hs\tLB:GLD0819_004\tPL:Illumina\n\tID:GLD0819_005\tSM:hs\tLB:GLD0819_005\tPL:Illumina\n\tID:GLD0819_006\tSM:hs\tLB:GLD0819_006\tPL:Illumina\n\tID:GLD0819_007\tSM:hs\tLB:GLD0819_007\tPL:Illumina\n\tID:GLD0819_008\tSM:hs\tLB:GLD0819_008\tPL:Illumina\n\tID:MW_003\tSM:hs\tLB:MW_003\tPL:Illumina\n\tID:MW_004\tSM:hs\tLB:MW_004\tPL:Illumina\n\tID:MW_005\tSM:hs\tLB:MW_005\tPL:Illumina\n\tID:MW_006\tSM:hs\tLB:MW_006\tPL:Illumina\n\tID:MW_007\tSM:hs\tLB:MW_007\tPL:Illumina\n\tID:MW_008\tSM:hs\tLB:MW_008\tPL:Illumina\n\tID:MW_009\tSM:hs\tLB:MW_009\tPL:Illumina\n\tID:MW_010\tSM:hs\tLB:MW_010\tPL:Illumina\n\tID:MW_011\tSM:hs\tLB:MW_011\tPL:Illumina\n\tID:MW_012\tSM:hs\tLB:MW_012\tPL:Illumina\n\tID:MW_013\tSM:hs\tLB:MW_013\tPL:Illumina\n\tID:MW_014\tSM:hs\tLB:MW_014\tPL:Illumina\n\tID:MW_015\tSM:hs\tLB:MW_015\tPL:Illumina\n\tID:MW_018\tSM:hs\tLB:MW_018\tPL:Illumina\n\tID:MW_019\tSM:hs\tLB:MW_019\tPL:Illumina\n\tID:MW_017\tSM:hs\tLB:MW_017\tPL:Illumina\n\tID:MW_020\tSM:hs\tLB:MW_020\tPL:Illumina\n\tID:MW_021\tSM:hs\tLB:MW_021\tPL:Illumina\n\tID:MW_022\tSM:hs\tLB:MW_022\tPL:Illumina\n\tID:MW_023\tSM:hs\tLB:MW_023\tPL:Illumina\n\tID:MW_024\tSM:hs\tLB:MW_024\tPL:Illumina\n\tID:MW_025\tSM:hs\tLB:MW_025\tPL:Illumina\n\tID:MW_026\tSM:hs\tLB:MW_026\tPL:Illumina\n\tID:MW_027\tSM:hs\tLB:MW_027\tPL:Illumina\n\tID:MW_028\tSM:hs\tLB:MW_028\tPL:Illumina\n\tID:MW_029\tSM:hs\tLB:MW_029\tPL:Illumina\n\tID:MW_030\tSM:hs\tLB:MW_030\tPL:Illumina\n\tID:MW_031\tSM:hs\tLB:MW_031\tPL:Illumina\n\tID:MW_032\tSM:hs\tLB:MW_032\tPL:Illumina\n\tID:MW_034\tSM:hs\tLB:MW_034\tPL:Illumina\n\tID:MW_035\tSM:hs\tLB:MW_035\tPL:Illumina\n\tID:MW_036\tSM:hs\tLB:MW_036\tPL:Illumina\n\tID:MW_037\tSM:hs\tLB:MW_037\tPL:Illumina\n\tID:MW_038\tSM:hs\tLB:MW_038\tPL:Illumina\n\tID:MW_039\tSM:hs\tLB:MW_039\tPL:Illumina\n\tID:MW_040\tSM:hs\tLB:MW_040\tPL:Illumina\n\tID:MW_041\tSM:hs\tLB:MW_041\tPL:Illumina\n\tID:MW_042\tSM:hs\tLB:MW_042\tPL:Illumina\n\tID:MW_043\tSM:hs\tLB:MW_043\tPL:Illumina\n\tID:MW_044\tSM:hs\tLB:MW_044\tPL:Illumina\n\tID:MW_045\tSM:hs\tLB:MW_045\tPL:Illumina\n\tID:MW_046\tSM:hs\tLB:MW_046\tPL:Illumina\n\tID:MW_047\tSM:hs\tLB:MW_047\tPL:Illumina\n\tID:MW_048\tSM:hs\tLB:MW_048\tPL:Illumina\n\tID:MW_049\tSM:hs\tLB:MW_049\tPL:Illumina\n\tID:MW_050\tSM:hs\tLB:MW_050\tPL:Illumina\n\tID:MW_051\tSM:hs\tLB:MW_051\tPL:Illumina\n\tID:MW_052\tSM:hs\tLB:MW_052\tPL:Illumina\n\tID:MW_053\tSM:hs\tLB:MW_053\tPL:Illumina\n\tID:MW_054\tSM:hs\tLB:MW_054\tPL:Illumina\n\tID:MW_055\tSM:hs\tLB:MW_055\tPL:Illumina\n\tID:MW_056\tSM:hs\tLB:MW_056\tPL:Illumina\n\tID:MW_057\tSM:hs\tLB:MW_057\tPL:Illumina\n\tID:MW_058\tSM:hs\tLB:MW_058\tPL:Illumina\n\tID:MW_059\tSM:hs\tLB:MW_059\tPL:Illumina\n\tID:MW_060\tSM:hs\tLB:MW_060\tPL:Illumina\n\tID:MW_061\tSM:hs\tLB:MW_061\tPL:Illumina\n\tID:MW_062\tSM:hs\tLB:MW_062\tPL:Illumina\n\tID:MW_063\tSM:hs\tLB:MW_063\tPL:Illumina\n\tID:MW_064\tSM:hs\tLB:MW_064\tPL:Illumina\n\tID:MW_065\tSM:hs\tLB:MW_065\tPL:Illumina\n\tID:MW_066\tSM:hs\tLB:MW_066\tPL:Illumina\n\tID:MW_067\tSM:hs\tLB:MW_067\tPL:Illumina\n\tID:PEC0819_011\tSM:hs\tLB:PEC0819_011\tPL:Illumina\n\tID:PEC0819_012\tSM:hs\tLB:PEC0819_012\tPL:Illumina\n\tID:PEC0819_013\tSM:hs\tLB:PEC0819_013\tPL:Illumina\n\tID:PPB_001\tSM:hs\tLB:PPB_001\tPL:Illumina\n\tID:PPB_002\tSM:hs\tLB:PPB_002\tPL:Illumina\n\tID:PPB_003\tSM:hs\tLB:PPB_003\tPL:Illumina\n\tID:PPB_004\tSM:hs\tLB:PPB_004\tPL:Illumina\n\tID:PPB_005\tSM:hs\tLB:PPB_005\tPL:Illumina\n\tID:PPB_006\tSM:hs\tLB:PPB_006\tPL:Illumina\n\tID:RP20_009\tSM:hs\tLB:RP20_009\tPL:Illumina\n\tID:RP20_010\tSM:hs\tLB:RP20_010\tPL:Illumina\n\tID:WFH_001\tSM:hs\tLB:WFH_001\tPL:Illumina\n\tID:WFH_002\tSM:hs\tLB:WFH_002\tPL:Illumina\n\tID:WFH_003\tSM:hs\tLB:WFH_003\tPL:Illumina\n\tID:WFH_004\tSM:hs\tLB:WFH_004\tPL:Illumina\n\tID:WFH_005\tSM:hs\tLB:WFH_005\tPL:Illumina\n\tID:WFH_006\tSM:hs\tLB:WFH_006\tPL:Illumina\n\tID:WFH_007\tSM:hs\tLB:WFH_007\tPL:Illumina\n\tID:WFH_008\tSM:hs\tLB:WFH_008\tPL:Illumina\n" ' > rg.txt
samtools merge -rh rg1.txt merged.bam CTM_001.bam CTM_002.bam CTM_003.bam CTM_004.bam CTM_005.bam CTM_006.bam CTM_007.bam CTM_009.bam CTM_010.bam ELP_001.bam ELP_002.bam ELP_003.bam ELP_004.bam ELP_005.bam ELP_006.bam ELP_007.bam ELP_008.bam ELP_009.bam ELP_010.bam GA12_001.bam GA12_002.bam GA12_003.bam GA12_004.bam GA12_006.bam GA12_007.bam GA12_008.bam GA12_009.bam GA12_010.bam GA12_012.bam GA12_013.bam GA12_014.bam GA12_015.bam GA12_016.bam GA12_017.bam GA12_018.bam GA12_019.bam GA12_020.bam GA12_021.bam GA12_022.bam GA12_023.bam GA12_024.bam GA12_025.bam GLD0819_001.bam GLD0819_002.bam GLD0819_003.bam GLD0819_004.bam GLD0819_005.bam GLD0819_006.bam GLD0819_007.bam GLD0819_008.bam MW_003.bam MW_004.bam MW_005.bam MW_006.bam MW_007.bam MW_008.bam MW_009.bam MW_010.bam MW_011.bam MW_012.bam MW_013.bam MW_014.bam MW_015.bam MW_018.bam MW_019.bam MW_017.bam MW_020.bam MW_021.bam MW_022.bam MW_023.bam MW_024.bam MW_025.bam MW_026.bam MW_027.bam MW_028.bam MW_029.bam MW_030.bam MW_031.bam MW_032.bam MW_034.bam MW_035.bam MW_036.bam MW_037.bam MW_038.bam MW_039.bam MW_040.bam MW_041.bam MW_042.bam MW_043.bam MW_044.bam MW_045.bam MW_046.bam MW_047.bam MW_048.bam MW_049.bam MW_050.bam MW_051.bam MW_052.bam MW_053.bam MW_054.bam MW_055.bam MW_056.bam MW_057.bam MW_058.bam MW_059.bam MW_060.bam MW_061.bam MW_062.bam MW_063.bam MW_064.bam MW_065.bam MW_066.bam MW_067.bam PEC0819_011.bam PEC0819_012.bam PEC0819_013.bam PPB_001.bam PPB_002.bam PPB_003.bam PPB_004.bam PPB_005.bam PPB_006.bam RP20_009.bam RP20_010.bam WFH_001.bam WFH_002.bam WFH_003.bam WFH_004.bam WFH_005.bam WFH_006.bam WFH_007.bam WFH_008.bam

#
samtools view CTM_009.bam -H | less
samtools view CTM_009.bam -H | tail
#I already have sample ID
#just try to merge it
samtools merge merged.bam CTM_001.bam CTM_002.bam CTM_003.bam CTM_004.bam CTM_005.bam CTM_006.bam CTM_007.bam CTM_009.bam CTM_010.bam ELP_001.bam ELP_002.bam ELP_003.bam ELP_004.bam ELP_005.bam ELP_006.bam ELP_007.bam ELP_008.bam ELP_009.bam ELP_010.bam GA12_001.bam GA12_002.bam GA12_003.bam GA12_004.bam GA12_006.bam GA12_007.bam GA12_008.bam GA12_009.bam GA12_010.bam GA12_012.bam GA12_013.bam GA12_014.bam GA12_015.bam GA12_016.bam GA12_017.bam GA12_018.bam GA12_019.bam GA12_020.bam GA12_021.bam GA12_022.bam GA12_023.bam GA12_024.bam GA12_025.bam GLD0819_001.bam GLD0819_002.bam GLD0819_003.bam GLD0819_004.bam GLD0819_005.bam GLD0819_006.bam GLD0819_007.bam GLD0819_008.bam MW_003.bam MW_004.bam MW_005.bam MW_006.bam MW_007.bam MW_008.bam MW_009.bam MW_010.bam MW_011.bam MW_012.bam MW_013.bam MW_014.bam MW_015.bam MW_018.bam MW_019.bam MW_017.bam MW_020.bam MW_021.bam MW_022.bam MW_023.bam MW_024.bam MW_025.bam MW_026.bam MW_027.bam MW_028.bam MW_029.bam MW_030.bam MW_031.bam MW_032.bam MW_034.bam MW_035.bam MW_036.bam MW_037.bam MW_038.bam MW_039.bam MW_040.bam MW_041.bam MW_042.bam MW_043.bam MW_044.bam MW_045.bam MW_046.bam MW_047.bam MW_048.bam MW_049.bam MW_050.bam MW_051.bam MW_052.bam MW_053.bam MW_054.bam MW_055.bam MW_056.bam MW_057.bam MW_058.bam MW_059.bam MW_060.bam MW_061.bam MW_062.bam MW_063.bam MW_064.bam MW_065.bam MW_066.bam MW_067.bam PEC0819_011.bam PEC0819_012.bam PEC0819_013.bam PPB_001.bam PPB_002.bam PPB_003.bam PPB_004.bam PPB_005.bam PPB_006.bam RP20_009.bam RP20_010.bam WFH_001.bam WFH_002.bam WFH_003.bam WFH_004.bam WFH_005.bam WFH_006.bam WFH_007.bam WFH_008.bam


#Nov 18th
#I did not save the stuff from Monday properly but I do have non-zero bam files so I will start from there
rename -- -RG.bam .bam *.bam
#The merged bam file is no larger than a single bam file...

#the 2nd command 'samtools mpileup genome.fa 1.bam 2.bam output.vcf' will produce a vcf with two samples (this is what your want in 99% of the use cases)

samtools mpileup -f reference.fasta CTM_001.bam CTM_002.bam CTM_003.bam CTM_004.bam CTM_005.bam CTM_006.bam CTM_007.bam CTM_009.bam CTM_010.bam ELP_001.bam ELP_002.bam ELP_003.bam ELP_004.bam ELP_005.bam ELP_006.bam ELP_007.bam ELP_008.bam ELP_009.bam ELP_010.bam GA12_001.bam GA12_002.bam GA12_003.bam GA12_004.bam GA12_006.bam GA12_007.bam GA12_008.bam GA12_009.bam GA12_010.bam GA12_012.bam GA12_013.bam GA12_014.bam GA12_015.bam GA12_016.bam GA12_017.bam GA12_018.bam GA12_019.bam GA12_020.bam GA12_021.bam GA12_022.bam GA12_023.bam GA12_024.bam GA12_025.bam GLD0819_001.bam GLD0819_002.bam GLD0819_003.bam GLD0819_004.bam GLD0819_005.bam GLD0819_006.bam GLD0819_007.bam GLD0819_008.bam MW_003.bam MW_004.bam MW_005.bam MW_006.bam MW_007.bam MW_008.bam MW_009.bam MW_010.bam MW_011.bam MW_012.bam MW_013.bam MW_014.bam MW_015.bam MW_018.bam MW_019.bam MW_017.bam MW_020.bam MW_021.bam MW_022.bam MW_023.bam MW_024.bam MW_025.bam MW_026.bam MW_027.bam MW_028.bam MW_029.bam MW_030.bam MW_031.bam MW_032.bam MW_034.bam MW_035.bam MW_036.bam MW_037.bam MW_038.bam MW_039.bam MW_040.bam MW_041.bam MW_042.bam MW_043.bam MW_044.bam MW_045.bam MW_046.bam MW_047.bam MW_048.bam MW_049.bam MW_050.bam MW_051.bam MW_052.bam MW_053.bam MW_054.bam MW_055.bam MW_056.bam MW_057.bam MW_058.bam MW_059.bam MW_060.bam MW_061.bam MW_062.bam MW_063.bam MW_064.bam MW_065.bam MW_066.bam MW_067.bam PEC0819_011.bam PEC0819_012.bam PEC0819_013.bam PPB_001.bam PPB_002.bam PPB_003.bam PPB_004.bam PPB_005.bam PPB_006.bam RP20_009.bam RP20_010.bam WFH_001.bam WFH_002.bam WFH_003.bam WFH_004.bam WFH_005.bam WFH_006.bam WFH_007.bam WFH_008.bam output.vcf
#Now I need to use bamtools mpileup
-m #call multiallelic if needed
bamtools mpileup -f reference.fasta -O v CTM_001.bam CTM_002.bam CTM_003.bam CTM_004.bam CTM_005.bam CTM_006.bam CTM_007.bam CTM_009.bam CTM_010.bam ELP_001.bam ELP_002.bam ELP_003.bam ELP_004.bam ELP_005.bam ELP_006.bam ELP_007.bam ELP_008.bam ELP_009.bam ELP_010.bam GA12_001.bam GA12_002.bam GA12_003.bam GA12_004.bam GA12_006.bam GA12_007.bam GA12_008.bam GA12_009.bam GA12_010.bam GA12_012.bam GA12_013.bam GA12_014.bam GA12_015.bam GA12_016.bam GA12_017.bam GA12_018.bam GA12_019.bam GA12_020.bam GA12_021.bam GA12_022.bam GA12_023.bam GA12_024.bam GA12_025.bam GLD0819_001.bam GLD0819_002.bam GLD0819_003.bam GLD0819_004.bam GLD0819_005.bam GLD0819_006.bam GLD0819_007.bam GLD0819_008.bam MW_003.bam MW_004.bam MW_005.bam MW_006.bam MW_007.bam MW_008.bam MW_009.bam MW_010.bam MW_011.bam MW_012.bam MW_013.bam MW_014.bam MW_015.bam MW_018.bam MW_019.bam MW_017.bam MW_020.bam MW_021.bam MW_022.bam MW_023.bam MW_024.bam MW_025.bam MW_026.bam MW_027.bam MW_028.bam MW_029.bam MW_030.bam MW_031.bam MW_032.bam MW_034.bam MW_035.bam MW_036.bam MW_037.bam MW_038.bam MW_039.bam MW_040.bam MW_041.bam MW_042.bam MW_043.bam MW_044.bam MW_045.bam MW_046.bam MW_047.bam MW_048.bam MW_049.bam MW_050.bam MW_051.bam MW_052.bam MW_053.bam MW_054.bam MW_055.bam MW_056.bam MW_057.bam MW_058.bam MW_059.bam MW_060.bam MW_061.bam MW_062.bam MW_063.bam MW_064.bam MW_065.bam MW_066.bam MW_067.bam PEC0819_011.bam PEC0819_012.bam PEC0819_013.bam PPB_001.bam PPB_002.bam PPB_003.bam PPB_004.bam PPB_005.bam PPB_006.bam RP20_009.bam RP20_010.bam WFH_001.bam WFH_002.bam WFH_003.bam WFH_004.bam WFH_005.bam WFH_006.bam WFH_007.bam WFH_008.bam -o output.vcf --threads 10
#Gives results like mpileup is not one of the options for bamtools

#Maybe could use something to go to vcf from a single bam then merge vcfs with bamtools merge
#bcftools isec [OPTIONS] A.vcf.gz B.vcf.gz […​]
	#Creates intersections, unions and complements of VCF files.
	#Not listed under the options on the server
		#Note that using "samtools mpileup" to generate BCF or VCF files is now deprecated.  To output these formats, please use "bcftools mpileup" instead.
samtools mpileup -f reference.fasta -o output.vcf CTM_001.bam CTM_002.bam CTM_003.bam CTM_004.bam CTM_005.bam CTM_006.bam CTM_007.bam CTM_009.bam CTM_010.bam ELP_001.bam ELP_002.bam ELP_003.bam ELP_004.bam ELP_005.bam ELP_006.bam ELP_007.bam ELP_008.bam ELP_009.bam ELP_010.bam GA12_001.bam GA12_002.bam GA12_003.bam GA12_004.bam GA12_006.bam GA12_007.bam GA12_008.bam GA12_009.bam GA12_010.bam GA12_012.bam GA12_013.bam GA12_014.bam GA12_015.bam GA12_016.bam GA12_017.bam GA12_018.bam GA12_019.bam GA12_020.bam GA12_021.bam GA12_022.bam GA12_023.bam GA12_024.bam GA12_025.bam GLD0819_001.bam GLD0819_002.bam GLD0819_003.bam GLD0819_004.bam GLD0819_005.bam GLD0819_006.bam GLD0819_007.bam GLD0819_008.bam MW_003.bam MW_004.bam MW_005.bam MW_006.bam MW_007.bam MW_008.bam MW_009.bam MW_010.bam MW_011.bam MW_012.bam MW_013.bam MW_014.bam MW_015.bam MW_018.bam MW_019.bam MW_017.bam MW_020.bam MW_021.bam MW_022.bam MW_023.bam MW_024.bam MW_025.bam MW_026.bam MW_027.bam MW_028.bam MW_029.bam MW_030.bam MW_031.bam MW_032.bam MW_034.bam MW_035.bam MW_036.bam MW_037.bam MW_038.bam MW_039.bam MW_040.bam MW_041.bam MW_042.bam MW_043.bam MW_044.bam MW_045.bam MW_046.bam MW_047.bam MW_048.bam MW_049.bam MW_050.bam MW_051.bam MW_052.bam MW_053.bam MW_054.bam MW_055.bam MW_056.bam MW_057.bam MW_058.bam MW_059.bam MW_060.bam MW_061.bam MW_062.bam MW_063.bam MW_064.bam MW_065.bam MW_066.bam MW_067.bam PEC0819_011.bam PEC0819_012.bam PEC0819_013.bam PPB_001.bam PPB_002.bam PPB_003.bam PPB_004.bam PPB_005.bam PPB_006.bam RP20_009.bam RP20_010.bam WFH_001.bam WFH_002.bam WFH_003.bam WFH_004.bam WFH_005.bam WFH_006.bam WFH_007.bam WFH_008.bam
	#working
	#[mpileup] 132 samples in 132 input files
	#[mpileup] Combined max depth is above 1M. Potential memory hog!


#Meeting with Qi
#Yeah, editting the bashch is fine
	#Fine to switch back and forth
	#comment the lines out instead of deleting
	#bam files are actually empty

#Run bwa myself
bwa mem -L 20,5 -t 24 -a -M -T 10 -A 1 -B 4 -O 6 -R "@RG\tID:WFH_008\tSM:WFH_008\tPL:Illumina" reference.fasta WFH_008.R1.fq.gz WFH_008.R2.fq.gz > test.bam
#This issue may have just been with not having quotes around the RG in the script line

#Run bwa not pe, separately for each
	#Then merge
#Or merge the F and R, with cat

#More helpful to do paired end
	#Helps ALOT with duplication events and complex genomes
	#Are my enzymes cutting into only unmethalated regions
		#If so, and for transcriptome, it probably tends to be single copy
		#Restrict by map quality (mapping to repeating region)
			#I could be very strict during the alignment with this
			#Increase the MisMatch Penalty to avoid

#Do you have enough markers left after an LD filter?

#Do Freebayes to make the vcf
	#Follow the log file from my successful dDoc run for the freebayes command

freebayes -b cat-RRG.bam -t mapped.$1.bed -v raw.$1.vcf -f reference.fasta -m 5 -q 5 -E 3 --min-repeat-entropy 1 -V --populations popmap -n 10 -F 0.1 &>fb.$1.error.log

wget -nH -np -N -r --cut-dirs 2 --no-check-certificate --user hare --password Represent-Woolen-Opportunity-Absence-6 https://umgcdownload.msi.umn.edu/hare/211115_A00223_0705_BHKNWNDRXY/Hare_Project_004









#Date: Oct 12th-14th 2021
#Filtering from undedup dDoc_928
cp dDoc_928/undedup/TotalRawSNPs.vcf filterDoc_1012
#the first three steps are the same
	#so pull that one
cp filterDoc_105/raw.g5mac3dplm.recode.vcf filterDoc_1012
	#After filtering, kept 143 out of 150 Individuals
	#Now that we have removed poor coverage individuals, we can restrict the data to variants called in a high percentage of individuals and filter by mean depth of genotypes
vcftools --vcf raw.g5mac3dplm.recode.vcf --max-missing 0.85 --recode --recode-INFO-all --out DP3g95maf05 --min-meanDP 20
30642 out of a possible 39150 Sites
#I would like to make a graph of how many loci are removed at each step along here
	#output results concattenated into one file, dont print errors (no option for), do not recode or save
	#not easy to do, just print them out and record
	vcftools --vcf raw.g5mac3dplm.recode.vcf --min-meanDP 20 --max-missing 0.95

	#Now we need to create two lists that have just the individual names for each population
	#Except pair all solidissima together
	#mawk '$2 == "BR"' popmap > 1.keep && mawk '$2 == "WL"' popmap > 2.keep
awk '$2 == "GBE\r" || $2 == "SFJ\r" || $2 == "CCB\r"' popmap > 1.keep 
awk '$2 == "SLI\r"' popmap > 4.keep && awk '$2 == "SCC\r"' popmap > 5.keep
awk '$2 == "NLI\r"' popmap > 6.keep && awk '$2 == "GA\r"' popmap > 7.keep
	#add \r "carriage return" because it is at the end of the line OR "includes" awk 'index($2, "GBE")' popmap
			#by solsim
			1	GBE #> except now all sol in pop 1
			2	SFJ
			3	CCB
			4	SLI
			5	SCC
			6	NLI
			7	GA
	#vcftools --vcf DP3g95maf05.recode.vcf --keep 1.keep --missing-site --out 1
	#vcftools --vcf DP3g95maf05.recode.vcf --keep 2.keep --missing-site --out 2
for f in {1..7}
do
	vcftools --vcf DP3g95maf05.recode.vcf --keep $f.keep --missing-site --out $f
done
	#combine the files to look across all for f_miss within each site
#cat 1.lmiss 2.lmiss 3.lmiss 4.lmiss 5.lmiss 6.lmiss 7.lmiss | awk '!/CHR/' | awk '$6 > 0.1' | cut -f1,2 >> badloci
#try instead only concatenating the three others 

cat 1.lmiss 4.lmiss 5.lmiss 6.lmiss 7.lmiss | awk '!/CHR/' | awk '$6 > 0.1' | cut -f1,2 >> badloci
	#10% from any single site
	21660 badloci # out of 30642 loci
#Do this step TWICE - once with all samples and once with only similis sites
cat 5.lmiss 6.lmiss 7.lmiss | awk '!/CHR/' | awk '$6 > 0.1' | cut -f1,2 >> badloci_simonly
	5672 badloci_simonly # out of 30642 loci
vcftools --vcf DP3g95maf05.recode.vcf --exclude-positions badloci --recode --recode-INFO-all --out DP3g95p5maf05
		#After filtering, kept 5445 out of a possible 6732 Sites
cp ../filterDoc_105/soldis4rm.indv .
vcftools --vcf DP3g95maf05.recode.vcf --exclude-positions badloci_simonly --remove soldis4rm.indv --recode --recode-INFO-all --out DP3g95p5maf05_simonly
		#Allele Balance
/programs/vcflib-1.0.1/bin/vcffilter -s -f "AB > 0.2 & AB < 0.8 | AB < 0.01" DP3g95p5maf05.recode.vcf > DP3g95p5maf05.fil1.vcf
/programs/vcflib-1.0.1/bin/vcffilter -s -f "AB > 0.2 & AB < 0.8 | AB < 0.01" DP3g95p5maf05_simonly.recode.vcf > DP3g95p5maf05_simonly.fil1.vcf
		#check how many loci now
awk '!/#/' DP3g95p5maf05.recode.vcf | wc -l && awk '!/#/' DP3g95p5maf05.fil1.vcf | wc -l
		18132>11347 #loci	
awk '!/#/' DP3g95p5maf05_simonly.recode.vcf | wc -l && awk '!/#/' DP3g95p5maf05_simonly.fil1.vcf | wc -l
		26290>18351 #loci
	#The next filter we will apply filters out sites that have reads from both strands.
/programs/vcflib-1.0.1/bin/vcffilter -f "SAF / SAR > 100 & SRF / SRR > 100 | SAR / SAF > 100 & SRR / SRF > 100" -s DP3g95p5maf05.fil1.vcf > DP3g95p5maf05.fil2.vcf
awk '!/#/' DP3g95p5maf05.fil2.vcf | wc -l
/programs/vcflib-1.0.1/bin/vcffilter -f "SAF / SAR > 100 & SRF / SRR > 100 | SAR / SAF > 100 & SRR / SRF > 100" -s DP3g95p5maf05_simonly.fil1.vcf > DP3g95p5maf05_simonly.fil2.vcf
awk '!/#/' DP3g95p5maf05_simonly.fil2.vcf | wc -l
	#The next filter looks at the ratio of mapping qualities between reference and alternate alleles
	#The rationale here is that, again, because RADseq loci and alleles all should start from the same genomic location there should not be large discrepancy between the mapping qualities of two alleles.
/programs/vcflib-1.0.1/bin/vcffilter -f "MQM / MQMR > 0.9 & MQM / MQMR < 1.05" DP3g95p5maf05.fil2.vcf > DP3g95p5maf05.fil3.vcf
awk '!/#/' DP3g95p5maf05.fil3.vcf | wc -l
		10383
/programs/vcflib-1.0.1/bin/vcffilter -f "MQM / MQMR > 0.9 & MQM / MQMR < 1.05" DP3g95p5maf05_simonly.fil2.vcf > DP3g95p5maf05_simonly.fil3.vcf
awk '!/#/' DP3g95p5maf05_simonly.fil3.vcf | wc -l
		16628
	#Yet another filter that can be applied is whether or not their is a discrepancy in the properly paired status of for reads supporting reference or alternate alleles.
/programs/vcflib-1.0.1/bin/vcffilter -f "PAIRED > 0.05 & PAIREDR > 0.05 & PAIREDR / PAIRED < 1.75 & PAIREDR / PAIRED > 0.25 | PAIRED < 0.05 & PAIREDR < 0.05" -s DP3g95p5maf05.fil3.vcf > DP3g95p5maf05.fil4.vcf			
awk '!/#/' DP3g95p5maf05.fil4.vcf | wc -l
		10156
/programs/vcflib-1.0.1/bin/vcffilter -f "PAIRED > 0.05 & PAIREDR > 0.05 & PAIREDR / PAIRED < 1.75 & PAIREDR / PAIRED > 0.25 | PAIRED < 0.05 & PAIREDR < 0.05" -s DP3g95p5maf05_simonly.fil3.vcf > DP3g95p5maf05_simonly.fil4.vcf			
awk '!/#/' DP3g95p5maf05_simonly.fil4.vcf | wc -l
		16241
	#In short, with whole genome samples, it was found that high coverage can lead to inflated locus quality scores. Heng proposed that for read depths greater than the mean depth plus 2-3 times the square root of mean depth that the quality score will be twice as large as the depth in real variants and below that value for false variants.
	#I actually found that this is a little too conservative for RADseq data, likely because the reads aren’t randomly distributed across contigs. I implement two filters based on this idea. the first is removing any locus that has a quality score below 1/4 of the depth.
		#This part is complicated - do it with just all samples first
/programs/vcflib-1.0.1/bin/vcffilter -f "QUAL / DP > 0.25" DP3g95p5maf05.fil4.vcf > DP3g95p5maf05.fil5.vcf
cut -f8 DP3g95p5maf05.fil5.vcf | grep -oe "DP=[0-9]*" | sed -s 's/DP=//g' > DP3g95p5maf05.fil5.DEPTH
awk '!/#/' DP3g95p5maf05.fil5.vcf | cut -f1,2,6 > DP3g95p5maf05.fil5.vcf.loci.qual
awk '{ sum += $1; n++ } END { if (n > 0) print sum / n; }' DP3g95p5maf05.fil5.DEPTH
		19758.5 #mean depth			#his was 1.9k so this is a lot higher
#Now the the mean plus 3X the square root of the mean - done myself in google/wolfram
		20180.2
		#Insert that value
paste DP3g95p5maf05.fil5.vcf.loci.qual DP3g95p5maf05.fil5.DEPTH | awk -v x=20180 '$4 > x' | awk '$3 < 2 * $4' > DP3g95p5maf05.fil5.lowQDloci
vcftools --vcf DP3g95p5maf05.fil5.vcf --site-depth --exclude-positions DP3g95p5maf05.fil5.lowQDloci --out DP3g95p5maf05.fil5
cut -f3 DP3g95p5maf05.fil5.ldepth > DP3g95p5maf05.fil5.site.depth
	#Now let’s calculate the average depth by dividing the above file by the number of individuals 31
		#I have 143 individuals
awk '!/D/' DP3g95p5maf05.fil5.site.depth | awk -v x=143 '{print $1/x}' > meandepthpersite
		#Too busy to - Plot myself - assume similar distribution
vcftools --vcf  DP3g95p5maf05.fil5.vcf --recode-INFO-all --out DP3g95p5maf05.FIL --max-meanDP 200 --exclude-positions DP3g95p5maf05.fil5.lowQDloci --recode 
		7552 out of a possible 8234 Sites
	#repeat with similis only
/programs/vcflib-1.0.1/bin/vcffilter -f "QUAL / DP > 0.25" DP3g95p5maf05_simonly.fil4.vcf > DP3g95p5maf05_simonly.fil5.vcf
cut -f8 DP3g95p5maf05_simonly.fil5.vcf | grep -oe "DP=[0-9]*" | sed -s 's/DP=//g' > DP3g95p5maf05_simonly.fil5.DEPTH
awk '!/#/' DP3g95p5maf05_simonly.fil5.vcf | cut -f1,2,6 > DP3g95p5maf05_simonly.fil5.vcf.loci.qual
awk '{ sum += $1; n++ } END { if (n > 0) print sum / n; }' DP3g95p5maf05_simonly.fil5.DEPTH
		16318.8
#Now the the mean plus 3X the square root of the mean - done myself in google/wolfram
		16702
paste DP3g95p5maf05_simonly.fil5.vcf.loci.qual DP3g95p5maf05_simonly.fil5.DEPTH | awk -v x=16702 '$4 > x' | awk '$3 < 2 * $4' > DP3g95p5maf05_simonly.fil5.lowQDloci
vcftools --vcf DP3g95p5maf05_simonly.fil5.vcf --site-depth --exclude-positions DP3g95p5maf05_simonly.fil5.lowQDloci --out DP3g95p5maf05_simonly.fil5
cut -f3 DP3g95p5maf05_simonly.fil5.ldepth > DP3g95p5maf05_simonly.fil5.site.depth
	#Now let’s calculate the average depth by dividing the above file by the number of individuals 31
		#I have 133 individuals (Might have done this wrong for Doc105)
awk '!/D/' DP3g95p5maf05_simonly.fil5.site.depth | awk -v x=133 '{print $1/x}' > meandepthpersite_simonly
vcftools --vcf  DP3g95p5maf05_simonly.fil5.vcf --recode-INFO-all --out DP3g95p5maf05_simonly.FIL --max-meanDP 200 --exclude-positions DP3g95p5maf05_simonly.fil5.lowQDloci --recode 
		12013 out of a possible 13231 Sites
#No HWE
#filter one SNP per contig
	grep -v "#" DP3g95p5maf05_simonly.FIL.recode.vcf | wc -l && grep -v "#" DP3g95p5maf05_simonly.FIL.recode.vcf | sort -k1,1 -u | wc -l
			1558 #balls
	grep -v "#" DP3g95p5maf05.FIL.recode.vcf | wc -l && grep -v "#" DP3g95p5maf05.FIL.recode.vcf | sort -k1,1 -u | wc -l
		
#save to new file
grep "#" DP3g95p5maf05_simonly.FIL.recode.vcf > SNP_simonly.head.vcf
grep -v "#" DP3g95p5maf05_simonly.FIL.recode.vcf | sort -k1,1 -u > SNP_simonly.data.vcf
cat SNP_simonly.head.vcf SNP_simonly.data.vcf > SNP_JPfilter105noLD_simonly.vcf
grep "#" DP3g95p5maf05.FIL.recode.vcf > SNP.head.vcf
grep -v "#" DP3g95p5maf05.FIL.recode.vcf | sort -k1,1 -u > SNP.data.vcf
cat SNP.head.vcf SNP.data.vcf > SNP_JPfilter105noLD.vcf

#these were moved to 
.../filterDoc_1012/finish_1012


#Date: Oct 14th
#STRUCTURE BABY
#Using my one SNP per contig from both stringent and non and both sim and all
#From K=1 to K=15 (number of sites)

#first use PGBSpider to convert
#make a popmap file with numbers instead of populations
#by solsim
1	GBE
2	SFJ
3	CCB
4	SLI
5	SCC
6	NLI
7	GA
#re-upload those to the server and run structure on them
#divide into four slurm jobs, each doing one set of things
#run by editing main params and then running
sbatch --nodes=1 --ntasks=6 --mem=40000 s_STRUCT_ddoc.sh --output strct_1012a

#First, remove headers
tail -n +2 SNP_JPfilter105noLD_simonly.structure | less
#Crap, I left BLP in the simonly filter
tail -n +8 SNP_JPfilter105noLD_simonly.structure | less
#WHY is CTM listed as population #4
#So basically PGBSpider did not listen to my popmap, next time try site codes A,B,C,D
tail -n +8 SNP_JPfilter105noLD_simonly.structure > SNP_105_simonly.structure
tail -n +2 SNP_JPfilter105noLD.structure > SNP_105_all.structure
tail -n +2 SNP_JPfilter1012noLD.structure > SNP_1012_all.structure
tail -n +2 SNP_JPfilter1012noLD_simonly.structure > SNP_1012_sim.structure

#^M issue
tail -n +8 SNP_JPfilter105noLD_simonly.structure | awk '{print $0" "}' | sed 's/....$//'  > SNP_105_simonly.structure
#Then decrease number of loci by 1
tail -n +2 SNP_JPfilter105noLD.structure | awk '{print $0" "}' | sed 's/....$//' > SNP_105_all.structure
tail -n +2 SNP_JPfilter1012noLD.structure | awk '{print $0" "}' | sed 's/....$//' > SNP_1012_all.structure
tail -n +8 SNP_JPfilter1012noLD_simonly.structure| awk '{print $0" "}' | sed 's/....$//' > SNP_1012_sim.structure

#reset this each time
for f in {1..15}
do
echo "structure -K $f -o filter_1012/out_sim_K$f  >& filter_1012/my_run_sim_K$f.log"
done > j_structureKtest
sbatch --nodes=1 --ntasks=6 --mem=40000 s_STRUCT_ddoc.sh --output strct_1012sim


#find number of snps
head -1 SNP_JPfilter1012noLD_simonly.structure | wc  #word count the number of SNPs in header
      1    1521    3295
      #so 1519
	#change for each
		#INFILE, OUTFILE, NUM LOCI, NUM INDS, MAX POPS (7-3)
SNP_105_simonly.structure
	filter_105/out_105_sim 
	1022 - 1
	130						#after removing BLP
SNP_105_all.structure
	filter_105/out_105_all
	796 - 1
	143
SNP_1012_all.structure
	filter_1012/out_1012_all
	927 - 1 
	143
SNP_1012_sim.structure
	filter_1012/out_1012_sim
	1472 - 1
	133	-3					#after removing BLP

#1012 sim - several loci with all missing data??
#1012 all - also missing

#For some reason when I get to K=5 in the alls it switches to only including similis? WHY?
#Does it not maintain main params?
#It checks it each time it runs a new k
-m (mainparams)

cp mainparams mainp_105a
cp mainparams mainp_105sim
cp mainparams mainp_1012a
cp mainparams mainp_1012sim

#reset this each time
for f in {1..15}
do
echo "structure -K $f -o filter_105/out_a_K$f  >& filter_105/my_run_a_K$f.log -m mainp_105a"
done > j_structureKtest_5a

#concatenate all the K jobs
cat j_structureKtest_* >> j_batchK

sbatch --nodes=1 --ntasks=24 --mem=160000 s_batchSTRUCT_ddoc.sh
#Oops, even though I changed it to n-tasks=6, I needed to change it to run more jobs at once in the slurm code

#Great, finished, now graph K likelihoods
Estimated Ln Prob of Data   = -61275.9
Mean value of ln likelihood = -61064.4
Variance of ln likelihood   = 422.9

for f in {1..15}
do
grep "Estimated Ln Prob" filter_105/my_run_a_K$f.log 
done
#Then use data to collumns in excel



#Date: Oct 6th 2021
#Relatedness test across individuals
#in vcftools
--relatedness
#This option is used to calculate and output a relatedness statistic based on the method of Yang et al, Nature Genetics 2010 (doi:10.1038/ng.608). Specifically, calculate the unadjusted Ajk statistic. Expectation of Ajk is zero for individuals within a populations, and one for an individual with themselves. The output file has the suffix ".relatedness".
--relatedness2
#This option is used to calculate and output a relatedness statistic based on the method of Manichaikul et al., BIOINFORMATICS 2010 (doi:10.1093/bioinformatics/btq559). The output file has the suffix ".relatedness2".
vcftools --vcf finish_105/SNP_JPfilter105a.vcf --relatedness2 --out SNP_105a
vcftools --vcf finish_105/SNP_JPfilter105a_simonly.vcf --relatedness2 --out SNP_105a_simonly
vcftools --vcf finish_105/SNP_JPfilter105noLD.vcf --relatedness2 --out SNP_105noLD
vcftools --vcf finish_105/SNP_JPfilter105noLD_simonly.vcf --relatedness2 --out SNP_105noLD_simonly
			#Note, I definitely do not actually need to do this across all 4
			#Can only do one at a time between relatedness and relate2
			#mv *relatedness* kins/
			#what do the negative values for relatedness PHI mean? Shouldn't it be 0-0.5?
					#A negative kinship coefficient estimation indicates an unrelated relationship. The reason that a negative kinship coefficient is not set to zero is a very negative value may indicate the population structure between the two individuals.
							#From: https://www.chen.kingrelatedness.com/
					#So I could plot all negatives as 0
#How do I plot?
	#in R
	#https://r-coder.com/correlation-plot-r/
#Now to do across groups only
	#vcffilter by only each pop at a time (did I already do that with 1.keep etc <- use those)
	#then set maf to 0.05 or whatever and then run it (maybe instead mac to 1)
	#SFJ is gonna be weird (I could combine that and GBE, I guess) <- do all solidissima together

#Then population structure is most often calculated from neutral loci
#How do do?
	#https://onlinelibrary.wiley.com/doi/10.1111/eva.13248
	#We then used BayeScan with prior odds of 1000 and other parameters set to default. For both tests, SNPs with a false detection rate (q-value) under 0.05 were considered putatively under selection.
	#Good general protocol
		#discuss that RADseq is not the best to look for adaptive loci
		#Figure S4, explain environment variables by sites with PCAs
		#RDA and LFMM are made to separate drift from selection
			#decrease confounding by sampling less range and more indv
	#Alternative for population heterozygosity
	
	
#Try STRUCTURE with K values > 8 for the number of sites total


######################### NOTES STACKS ########################
#Date: Aug 5th
#I am remaking the catalog without using any solidissima
#Additionally, I will be aligning heterozygotes but not using them for catalog creation either

#I will be using the set up where I remove duplicates from raw reads and then trim
#Therefore I can start from (dedup > trim adapter and padding > quality filter > make same length)
sync_5_length4STACKS/resync/

#Check how STACKS 2.53 relates to the 2019 paper
	#Current version is 2.59 (came out July 21 2021)
	#2.53 came out on March 28th 2021 so proceed with this one
export LD_LIBRARY_PATH=/usr/local/gcc-7.3.0/lib64:/usr/local/gcc-7.3.0/lib
export PATH=/programs/stacks-2.53/bin:$PATH
	#put that in scripts too so they run smoothly
#Try Mastretta-Yanes 2014(5) paper method for identifying best parameters for STACKS
# https://onlinelibrary.wiley.com/doi/pdf/10.1111/1755-0998.12291?casa_token=eUt974BGUtsAAAAA:QEWBevkTfDarQrnLdjm6rfxjsyIkqOTtGyQl2lJqzUqFuVbxnb1nlNeqXuQqPVZASdtX2WKAM0r5

#after trimming all reads are 125bp long

#Begin ustacks
#signify everything in this run with _f2 (full, similis, second attempt)

touch s_sim_uSTACK_f2.sh
nano s_sim_uSTACK_f2.sh

#!/bin/bash -l

#SBATCH --partition=regular
#SBATCH --job-name=ustacks_f2
#SBATCH --output=ustacks_f2.out
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --mail-user=hh693@cornell.edu

export LD_LIBRARY_PATH=/usr/local/gcc-7.3.0/lib64:/usr/local/gcc-7.3.0/lib
export PATH=/programs/stacks-2.53/bin:$PATH


#Date: Aug 9th
#Add in parallel to this step

count=1
while read p; do
echo echo "$p"
echo echo "R1"
samplea="$p"_R1
sample_indexa="$count"
fq_filea=../sync_5_length4STACKS/cutadapted/"$p"_R1.lengthtrim.fastq
log_filea=./ulogs/"$p"_R1.ustacks.oe
echo "ustacks -f $fq_filea -i $sample_indexa -o ./ -M 4 -m 3 -p 2 &> $log_filea"
echo echo "R2"
sampleb="$p"_R2
sample_indexb="$(($count + 1))"
fq_fileb=../sync_5_length4STACKS/cutadapted/"$p"_R2.lengthtrim.fastq
log_fileb=./ulogs/"$p"_R2.ustacks.oe
echo "ustacks -f $fq_fileb -i $sample_indexb -o ./ -M 4 -m 3 -p 2 &> $log_fileb"
count="$(($count + 2))"
done <../sim_sample.txt > j_sim_ustack.txt

parallel -j 12 < j_ustacks_f2.txt

sbatch --nodes=1 --ntasks=24 --mem=90000 s_j_ustacks_f2.sh

#Tasks needed to be only set to p cause this is only doing one R1 or R2 at a time
#If I do this again, set low p (~2-4) and do my own parallelization because this is not super efficient
#got to BLP179 in 8 minutes - 8/300 - should take about 5 hours then
#Setting my own parallelizaion worked great! It finished in less than an hour


#Date: Aug 6th
#Run cstacks with only similis

#partials
#List of important samples
#MW1-3 - Nick's Admixed
#MW16 - has a low tag score but non-failure in raw reads
#MW22 - My heterozygote from primer testing
#MW31
#MW33
#PEC13
#CTM008

touch s_sim_cSTACK_f2.sh
nano s_sim_cSTACK_f2.sh

#!/bin/bash -l

#SBATCH --partition=regular
#SBATCH --job-name=cstacks_f2
#SBATCH --output=cstacks_f2.out
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --mail-user=hh693@cornell.edu

export LD_LIBRARY_PATH=/usr/local/gcc-7.3.0/lib64:/usr/local/gcc-7.3.0/lib
export PATH=/programs/stacks-2.53/bin:$PATH

cstacks -P ./ -M ./popmap.catalog.simonly.tsv -n 4 -p 20 &> cstacks.oe

sbatch --nodes=1 --ntasks=20 --mem=30000 s_sim_cSTACK_f2.sh

#For some reason ELP wasn't in the catalog on Aug 6th... add it back in
ELP_003_S28_R1.lengthtrim	SCC
ELP_003_S28_R2.lengthtrim	SCC
ELP_004_S40_R1.lengthtrim	SCC
ELP_004_S40_R2.lengthtrim	SCC
ELP_005_S52_R1.lengthtrim	SCC
ELP_005_S52_R2.lengthtrim	SCC
ELP_006_S64_R1.lengthtrim	SCC
ELP_006_S64_R2.lengthtrim	SCC
ELP_007_S76_R1.lengthtrim	SCC
ELP_007_S76_R2.lengthtrim	SCC
ELP_008_S87_R1.lengthtrim	SCC
ELP_008_S87_R2.lengthtrim	SCC
ELP_009_S5_R1.lengthtrim	SCC
ELP_009_S5_R2.lengthtrim	SCC
#104 total samples this time (r1 and r2) so 52 samples/150

#No room on our server so running it on a different one (Aug 8th)
#running in screen not as slurm, just the two export commands and then cstacks
Final catalog contains 571650 loci. #from Aug 8th

#August 9th catalog:
Final catalog contains 607818 loci.


#Date: Aug 9th
#Run sstacks in parallel

while read p; do
echo "echo $p"
echo "echo "R1""
echo "sstacks -c ./batch_1 -s ./"$p"_R1.lengthtrim -o ./ &> slogs/"$p"_R1.sstacks.oe"
echo "echo "R2""
echo "sstacks -c ./batch_1 -s ./"$p"_R2.lengthtrim -o ./ &> slogs/"$p"_R2.sstacks.oe"
done <../sim_sample.txt > j_sstacks_f2.txt

#make that job list into a parallel that then gets fed into slurm

#!/bin/bash -l

#SBATCH --partition=regular
#SBATCH --job-name=j_sstacks_thread10_mem160G
#SBATCH --output=sstacks_jlist_test.out
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --mail-user=hh693@cornell.edu

export LD_LIBRARY_PATH=/usr/local/gcc-7.3.0/lib64:/usr/local/gcc-7.3.0/lib
export PATH=/programs/stacks-2.53/bin:$PATH
parallel -j 8 < j_sstacks_f2.txt

sbatch --nodes=1 --ntasks=8 --mem=120000

#Unable to parse catalog again
#I do not see catalog... It seems to have run cstacks in full according to the log but didn't save anything?
#"Writing catalog in directory './'."
#Yup, it's just in here instead of having created a batch_1 folder. I could put it in a batch_1 folder I guess or just change it to look in here rather than there

#create a batch_1 folder and put it in there

#ran out of memory (on 89G) within 4 minutes, try decreasing threads and upping mem
#looks like it might fail again with 160G, and it hasn't even gotten through the first batch, this just takes a lot of memory
#stabilizing around 135G? - 12 Hong = about 12.5G required per sample, but also these were not nessecarily the biggest samples, it may still fail BUT the decreasing threads with increased memory does help
#decrease it to 5 threads and increase it to 180G if it fails again

#Not needed, takes a while to run: after 2 hours, it's about half way done.


#tsv to bam and then gstacks
rename .lengthtrim. . *.tsv
rename _R1 .1 *.tsv
rename _R2 .2 *.tsv

#Instead copy them to make it
rename _R1 .1 *.gz
rename _R2 .2 *.gz
rename _001. . *.gz

#I made a HUGE mistake here when I accidentally did rename _R2 .1 *.tsv which overwrote all of my R1s with my R2s so I have to make them again.
#Then I made another mistake by rm *.tsv which also removed my catalog
#No that wasnt a mistake cause it was in the batch folder

#first run tsv2bam
# Run tsv2bam to transpose the data so it is stored by locus, instead of by sample. We will include
# paired-end reads using tsv2bam. tsv2bam expects the paired read files to be in the samples
# directory and they should be named consistently with the single-end reads,
# e.g. sample_01.1.fq.gz and sample_01.2.fq.gz, which is how process_radtags will output them.
tsv2bam -P ./ -M ../info/popmap.all.tsv --pe-reads-dir ../rename_seq/ #-t 8
		#make a folder of empty renamed raw read files for the paired end list
		FILES="./*.gz"
		for f in $FILES
		do
		  touch ../faux_samples/"$f"
		done

		#now in that folder
		rename _R . *gz
		rename _001.fastq .fq
		ls > pop_all
		#remove stuff from file with sed
		sed 's/...fq.gz//' pop_all | uniq > pop_all_p
		#first and last . are actual . and the middle one it the one or two
		#this should also work
		sed 's/.1.fq.gz//' pop_all | sed 's/.2.fq.gz//' | uniq > pop_all_t

#Date: Aug 10th
	
#Did not work but it could have been an issue of all the memory being used up on the server
#or the raw reads
# Now it seems to be looking for non paired reads:
#Processing sample '422_014_S128'...
#Error: Unable to load matches from './422_014_S128.matches.tsv(.gz)'.

#https://groups.google.com/g/stacks-users/c/x0DvGQvic2A
#This suggests that renaming F reads to not have a 1 but leaving R reads with a 2 will work
rename .1. . *.tsv

#to fix back do rename 0.alleles > 0.1.alleles or add something for the R2s first. This'll be a pain to redo, hope it works

#looks like it is working! Even on only one thread, I think it'll finish in less than 20 minutes

#gstacks

#Date: Aug 12th
export LD_LIBRARY_PATH=/usr/local/gcc-7.3.0/lib64:/usr/local/gcc-7.3.0/lib
export PATH=/programs/stacks-2.53/bin:$PATH

#For non-reference genomes:
gstacks -P ./ -M ../info/popmap.all.tsv -t 22

#Options that I did not include
#Paired-end options:
#  --rm-pcr-duplicates: remove all but one set ofread pairs of the same sample that 
#                       have the same insert length (implies --rm-unpaired-reads)
#  --rm-unpaired-reads: discard unpaired reads
#  --ignore-pe-reads: ignore paired-end reads even if present in the input
#  --unpaired: ignore read pairing (only for paired-end GBS; treat READ2's as if they were READ1's)

sbatch --nodes=1 --ntasks=24 --mem=60000 s_gstacks.sh
		#Oh boy, this is gonna take a long time - ehh maybe not but at least an hour probably less than 2

mkdir stacks.denovo/rxstacks
cd stacks.denovo/rxstacks
rxstacks -P ../ -o ./ --prune_haplo --conf_lim 0.10 &> rxstacks.oe
#Run the cstacks and sstacks units again, this time in the rxstacks/ subdirectory, using the same commands as at Step 17A(vi–vii) 

#Date: Aug 13th
#Use the rxstacks unit to improve SNP genotype calls, to filter out unlikely haplotypic combinations, and to remove loci having high error rates.
#But I think this is only used with v1
#https://catchenlab.life.illinois.edu/stacks/manual/#denovobyhand     4.4.1 just says run populations after gstacks so I'm going with that - but also check 2019 paper
#it is no longer in the stacks manual
#so move on to populations

#this is the r80 part
#from v1 paper
min_samples=0.80
min_maf=0.05
max_obs_het=0.70
populations -P ./ -r $min_samples --min_maf $min_maf --max_obs_het $max_obs_het --genepop &> populations.oe
#from v2 manual
populations -P $src/stacks/ -M $src/popmaps/popmap -r 0.65 --vcf --genepop --structure --fstats --hwe -t 8
#this exports several files


#for me
populations -P ./ -M ../info/popmap.all.tsv -O ./pop_out -r 0.80 --vcf --genepop --structure --fstats --hwe -t 36 &> populations.oe
#what did I do in the past with vcf files if i'm about to get there? Make markers?
#-r,--min-samples-per-pop [float]: minimum percentage of individuals in a population required to process a locus for that population.
		#TODO: try with multiple r values
#output in the pop.oe
Removed 254052 loci that did not pass sample/population constraints from 275669 loci.
Kept 21617 loci, composed of 6606108 sites; 22258 of those sites were filtered, 278036 variant sites remained
#more detail in populations.sumstats.tsv and populations.hapstats.tsv

#It looks like it didn't actually remove anything with r, theres still ~207k loci whereas there should only be ~20k and all have pass under filter which is not true
populations -P ./ -M ../info/popmap.all.tsv -O ./pop_out_r1 -r 1 --vcf --genepop --structure --fstats --hwe &> ./pop_out_r1/populations.oe
Removed 260232 loci that did not pass sample/population constraints from 275669 loci.
Kept 15437 loci, composed of 4703128 sites; 11891 of those sites were filtered, 92859 variant sites remained.
#I also tried applying this in excel myself and got ~15900 loci with a strict no samples can have missing data filter.
#Still includes 92k loci. What the heck?
#So it filtered more but still not doing what I thought I was telling it to do.



Population pair divergence statistics (more in populations.fst_summary.tsv and populations.phistats_summary.tsv):
  SFJ-CCB: mean Fst: 0.18329; mean Phi_st: 0.040913; mean Fst: 0.017592; mean Dxy: 0.0056048
  SFJ-SLI: mean Fst: 0.16207; mean Phi_st: 0.12197; mean Fst: 0.16179; mean Dxy: 0.0057043
  SFJ-SCC: mean Fst: 0.023021; mean Phi_st: 0.3576; mean Fst: 0.58331; mean Dxy: 0.011094
  SFJ-GA: mean Fst: 0.11134; mean Phi_st: 0.35073; mean Fst: 0.6513; mean Dxy: 0.01297
  SFJ-GBE: mean Fst: 0.10216; mean Phi_st: 0.0085766; mean Fst: -0.0030001; mean Dxy: 0.0054162
  SFJ-NLI: mean Fst: 0.094351; mean Phi_st: 0.22399; mean Fst: 0.44728; mean Dxy: 0.011729
  CCB-SLI: mean Fst: 0.158; mean Phi_st: 0.11034; mean Fst: 0.13193; mean Dxy: 0.004909
  CCB-SCC: mean Fst: 0.022513; mean Phi_st: 0.33506; mean Fst: 0.54593; mean Dxy: 0.0102
  CCB-GA: mean Fst: 0.1177; mean Phi_st: 0.3696; mean Fst: 0.65174; mean Dxy: 0.012486
  CCB-GBE: mean Fst: 0.1101; mean Phi_st: 0.028315; mean Fst: 0.036873; mean Dxy: 0.0052186
  CCB-NLI: mean Fst: 0.098689; mean Phi_st: 0.23949; mean Fst: 0.45796; mean Dxy: 0.011919
  SLI-SCC: mean Fst: 0.035499; mean Phi_st: 0.38292; mean Fst: 0.59842; mean Dxy: 0.0094777
  SLI-GA: mean Fst: 0.11002; mean Phi_st: 0.35524; mean Fst: 0.6189; mean Dxy: 0.010645
  SLI-GBE: mean Fst: 0.11837; mean Phi_st: 0.11292; mean Fst: 0.15009; mean Dxy: 0.0053237
  SLI-NLI: mean Fst: 0.1041; mean Phi_st: 0.26463; mean Fst: 0.48203; mean Dxy: 0.010069
  SCC-GA: mean Fst: 0.011281; mean Phi_st: 0.058125; mean Fst: 0.099009; mean Dxy: 0.0062466
  SCC-GBE: mean Fst: 0.032951; mean Phi_st: 0.36335; mean Fst: 0.59296; mean Dxy: 0.010773
  SCC-NLI: mean Fst: 0.0077522; mean Phi_st: 0.017842; mean Fst: 0.028827; mean Dxy: 0.0063822
  GA-GBE: mean Fst: 0.11605; mean Phi_st: 0.38046; mean Fst: 0.66638; mean Dxy: 0.013003
  GA-NLI: mean Fst: 0.030059; mean Phi_st: 0.058157; mean Fst: 0.13091; mean Dxy: 0.0090143
  GBE-NLI: mean Fst: 0.10416; mean Phi_st: 0.25751; mean Fst: 0.46995; mean Dxy: 0.0118
#This is not really relevant for similis but would be for solidissima, especially because some of these pops have 2 indiivduals

#However comparing GA-NLI to GA-SCC to NLI-SCC could be interesting  

populations -P ./ -M ../info/popmap.all.tsv -O ./unlinked_pop_out -r 0.80  --write-single-snp --vcf --genepop --structure --fstats --hwe -t 1 &> ./unlinked_pop_out/unlinked_populations.oe &
#Since STRUCTURE does not want linked loci, you will typically also supply the --write_single_snp flag so that you do not get more than one SNP per RAD locus (SNPs at the same RAD locus are almost certainly linked).
#genepop format with .gen suffix is for adegenet

#does not seem like it's threading
#takes 5 minutes or less
		
#How to get to pca
#v1
cd stacks.denovo/rxstacks
popmap=../../info/popmap.tsv
out_dir=pops.r80-maf05-het70
log_file=pops.r80-maf05-het70.oe
populations -P ./ -M $popmap -O $out_dir -p 4 -r 0.80 --min_maf 0.05 --max_obs_het 0.70 --genepop &> $log_file
#gene pop is the outcome for pca?
cp pops.r80-maf05-het70/batch_1.genepop pops.r80-maf05-het70/batch_1.gen
#Then in R (from v1)
library(adegenet)
x = read.genepop(′pops.r80-maf05-het70/batch_1.gen′)
pop(x) = sapply(strsplit(indNames(x), ′_′), function(x){x[1]})
x.pca = dudi.pca(x, scannf=FALSE)
s.class(x.pca$li, pop(x), col=rainbow(nPop(x)))
add.scatter.eig(x.pca$eig[1:10], xax=1, yax=2)

#I did not use Adegenet, I used autoplot as a part of ggfortify
#I imported data from the VCF file.
#populationsr1.snps

#Adegenet in R
library("adegenet")
x <- read.genepop("populations.gen")
#I changed genpop files to .gen > takes a long time to load into R studio
Warning message:
In df2genind(X = X, pop = as.character(pop), ploidy = 2, ncode = ncode,  :
  Individuals with no scored loci have been removed
  #why is it running df2genid?
#adegenet can run with structure formatted data? Try that
#wont read it in, says theres nothing provided with that extension
#Removing some of the extra stuff: sim_str  <- read.structure("populations.STR")
#works but it asks me a lot of questions I don't have the answers to


#How do I deal with multiple rare alleles/haplotype data?
https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0045520#s4
#Just change homozygotes 0
#heterozygotes are 1
#homozygotes rare are 2

#grep on my home computer to do only pass
#then change them to the thing above, then flip and put into the same format as iris (samples in rows, variables = loci on the top row)
#should probably just remove some samples

grep "1/1" snps_filter.vcf | wc -l
   13263
grep "0/0" snps_filter.vcf | wc -l
   28409
grep "0/1" snps_filter.vcf | wc -l
   20792

grep "1/0" snps_filter.vcf | wc -l
       0

#No "2/2", "0/2", or "1/2" so it's only the one rare allele at each site.

#so I went into excel and copied the full first row and then draged out a collumn on a new tab that equaled exactly the sample titles from before. Then I've also marked the successful vs failures and should
#replace 0/0 with 0, 0/1 with 1 and 1/1 with 2 using grep and remove the fails
#I messed up the filtering before!!!
#loci shared by at least 130/150 of samples
7438 loci
#(hoping that NAs are still okay in PCAs and I don't need a sample at each locus)

sed 's/word1/word2/g' input.file
#g = global otherwise just find the first
# -i updates file rather than outputing (or maybe in addition to)

sed 's/0\/0/0/g' simplifiedfilvervcf.txt | sed 's/0\/1/1/g' | sed 's/1\/1/2/g' > simvcfsimplereadyforR.txt

sed 's/.\/./NA/g' simvcfsimplereadyforR.txt > vcfwithnanforR.txt
#Then remove the header to import into R
#And the # infront of CHROM so that R does not ignore the first line as a comment

#vcftools can convert to plink
#plink might be .bed or .ped + .map


#HETEROZYGOSITY -  Freq Distribution
#Date: Aug 30th
#ls only directories
ls -d */
#pops not for STRUCTURE - difference being loci per contig
#options are:
pop_out_r1
#populations -P ./ -M ../info/popmap.all.tsv -O ./pop_out_r1 -r 1 --vcf --genepop --structure --fstats --hwe &> ./pop_out_r1/populations.oe 
pop_out
#populations -P ./ -M ../info/popmap.all.tsv -O ./pop_out -r 0.80 --vcf --genepop --structure --fstats --hwe -t 36
#search in notes what each mean

#The r=0.8 is likely more informative because it eliminates less. Start there, then do other
#Where is heterozygosity statistic?
populations.sumstats.tsv#: This file contains summary statistics describing every SNP found in every population defined in the population map. This includes the frequecy of alleles, expected/observed heterozygosity, π, FIS, and so on.
#I can apply the filter --max-obs-het [float] — specify a maximum observed heterozygosity required to process a nucleotide site at a locus (applied to the metapopulation).
populations.sumstats_summary.tsv #has expected and observed

#Rest done in R

#REPEAT ASSEMBLY WITH NON-DEDUP
#Date: Aug 30th
#Notes from previous
#dedup 2 is deduplicating from raw and then sync_5 is going through everything again post dups removed at the beginning
#1. remove dups from raw
export PATH=/programs/seqkit-0.15.0:$PATH
while read p; do
	echo "$p"
	seqkit rmdup similis_seq_raw/"$p"_R1_001.fastq.gz -s -o dedup2_raw/"$p"_R1.rawdedup.fastq -D dups2/"$p"_R1_dups.txt -d dups2/"$p"_R1_dup_seq.fastq -j 10
	seqkit rmdup similis_seq_raw/"$p"_R2_001.fastq.gz -s -o dedup2_raw/"$p"_R2.rawdedup.fastq -D dups2/"$p"_R2_dups.txt -d dups2/"$p"_R2_dup_seq.fastq -j 10	
done <sim_sample.txt
			#somehow it seems like it is remove less duplicates before the trimming - likely because of the poor read quality on the R2s not registering as dup and the unique padding on the R1s
			#but still removing a LOT
			#also keep in mind that rmdup is a rather crude way of doing this without any regard for PCR dups or otherwise and perhaps a better program exists.
#2. after removing dups from raw do fastqc
export PATH=/programs/FastQC-0.11.8:$PATH
export LC_ALL=en_US.UTF-8
export PYTHONPATH=/programs/multiqc-1.10.1/lib64/python3.6/site-packages:/programs/multiqc-1.10.1/lib/python3.6/site-packages
export PATH=/programs/multiqc-1.10.1/bin:$PATH

fastqc -t 20 -o dedup2_fastqc dedup2_raw/*.fastq
multiqc -n multiqc_dedup2_report.html -o dedup2_multiqc ./dedup2_fastqc

#3. then use perl on R1 and R2, adding back in the R2s
#4. then trimmomatic on both but differently
#6. resync
#5/7. cut lengths and resync

#so I need to start from the raw reads
#3. then use perl on R1 and R2, adding back in the R2s
#4. then trimmomatic on both but differently
#6. resync
#5/7. cut lengths and resync


#WHAT I'm DOING

#START SCRIPT 
#!/bin/bash -l

#SBATCH --partition=regular
#SBATCH --job-name=trim_seq_10t_40G
#SBATCH --output=s_undeduptrim.out
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --mail-user=hh693@cornell.edu


export PATH=/programs/seqtk:$PATH
export PATH=/programs/seqkit-0.15.0:$PATH

while read p; do
echo "$p"
#remove duplicate reads
#  seqkit rmdup similis_seq_raw/"$p"_R1_001.fastq.gz -s -o dedup2_raw/"$p"_R1.rawdedup.fastq -D dups2/"$p"_R1_dups.txt -d dups2/"$p"_R1_dup_seq.fastq -j 10
#  seqkit rmdup similis_seq_raw/"$p"_R2_001.fastq.gz -s -o dedup2_raw/"$p"_R2.rawdedup.fastq -D dups2/"$p"_R2_dups.txt -d dups2/"$p"_R2_dup_seq.fastq -j 10	

#remove padding on R1 #possible that I should do this for R2 and just add back in the no cut sites but seems unnessecary
perl gbstrim.pl --enzyme1 psti --enzyme2 mspi --fastqfile similis_seq_raw/"$p"_R1_001.fastq.gz --read R1 --outputfile similis_clean_dups/padtrim/"$p"_R1.trim.fastq --verbose --threads 10 --minlength 50 >& similis_clean_dups/gbstrim_stats/"$p"_R1.log

#remove adapter and padding on R2 and quality check
java -jar /programs/trimmomatic/trimmomatic-0.39.jar SE -phred33 -threads 8 similis_seq_raw/"$p"_R2_001.fastq.gz similis_clean_dups/ommatic/"$p"_R2.fastq ILLUMINACLIP:/programs/trimmomatic/adapters/NexteraPE-PE.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:50 >& similis_clean_dups/ommatic_logs/"$p"_R2_trimmo.log
c -b 8 -e 8 similis_clean_dups/ommatic/"$p"_R2.fastq > similis_clean_dups/ommatic_phase2/"$p"_R2.fastq

#quality check R1
java -jar /programs/trimmomatic/trimmomatic-0.39.jar SE -phred33 -threads 8 similis_clean_dups/padtrim/"$p"_R1.trim.fastq similis_clean_dups/trim_R1s_phase2/"$p"_R1.trim.fastq SLIDINGWINDOW:4:15 MINLEN:50 >& similis_clean_dups/ommatic_logs/"$p"_R1_trimmo.log

#resync
perl resync.pl similis_clean_dups/trim_R1s_phase2/"$p"_R1.trim.fastq similis_clean_dups/ommatic_phase2/"$p"_R2.fastq similis_clean_dups/sync_trim_6/"$p"_R1.trim.sync.fastq similis_clean_dups/sync_trim_6/"$p"_R2.trim.sync.fastq >& similis_clean_dups/sync_trim_6/"$p"_resync6.log
done <sim_sample.txt 

#> j_undedup.txt

#Cannot do in parallele unless I set up the order to be different
#just make multiple echo while loops
sbatch --ntasks=10 --mem=40000 s_undepdup.sh 

tail s_undeduptrim.out
#To check how far along

#Forgot to include trimming for STACKS in the first loop
export PYTHONPATH=/programs/cutadapt-3.4/lib/python3.6/site-packages:/programs/cutadapt-3.4/lib64/python3.6/site-packages
export PATH=/programs/cutadapt-3.4/bin:$PATH
while read p; do
echo "$p"
cutadapt -l 125 -m 125 -j 8 -o similis_clean_dups/STACKS_cut6/"$p"_R1.lengthtrim.fastq -p similis_clean_dups/STACKS_cut6/"$p"_R2.lengthtrim.fastq similis_clean_dups/sync_trim_6/"$p"_R1.trim.sync.fastq similis_clean_dups/sync_trim_6/"$p"_R2.trim.sync.fastq
perl resync.pl similis_clean_dups/STACKS_cut6/"$p"_R1.lengthtrim.fastq similis_clean_dups/STACKS_cut6/"$p"_R2.lengthtrim.fastq similis_clean_dups/resync_STACKS_cut6/"$p"_R1.trim.sync.fastq similis_clean_dups/resync_STACKS_cut6/"$p"_R2.trim.sync.fastq >& similis_clean_dups/resync_STACKS_cut6/"$p"_resync6cut.log 
done <sim_sample.txt
#Cut about 10%-15% of reads


mkdir batch_1
mkdir clogs

#After doing this, repeat the denovo assembly with the undedups
cd f3_denovo_STACKS

#ustacks, cstacks, sstacks, tsv2bam, (fix names), gstacks, populations
#Grab from the top of notes section. Organize this differently...
#How do I separate what I did previously from the best while also knowing that there's a strong likelihood I'll redo it again
#But just by date doesn't make sense either cause I'm jumping between different things
touch s_sim_uSTACK_f3.sh
nano s_sim_uSTACK_f3.sh

#!/bin/bash -l

#SBATCH --partition=regular
#SBATCH --job-name=ustacks_f3_24t_90G
#SBATCH --output=ustacks_f3.out
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --mail-user=hh693@cornell.edu

export LD_LIBRARY_PATH=/usr/local/gcc-7.3.0/lib64:/usr/local/gcc-7.3.0/lib
export PATH=/programs/stacks-2.53/bin:$PATH
parallel -j 12 < j_ustacks_f3.txt

sbatch --nodes=1 --ntasks=24 --mem=90000 s_sim_uSTACK_f3.sh
#Started run at 8:52pm finished around 10:30

count=1
while read p; do
echo echo "$p"
echo echo "R1"
samplea="$p"_R1
sample_indexa="$count"
fq_filea=../similis_clean_dups/STACKS_cut6/"$p"_R1.lengthtrim.fastq
log_filea=./ulogs/"$p"_R1.ustacks.oe
echo "ustacks -f $fq_filea -i $sample_indexa -o ./ -M 4 -m 3 -p 2 &> $log_filea"
echo echo "R2"
sampleb="$p"_R2
sample_indexb="$(($count + 1))"
fq_fileb=../similis_clean_dups/STACKS_cut6/"$p"_R2.lengthtrim.fastq
log_fileb=./ulogs/"$p"_R2.ustacks.oe
echo "ustacks -f $fq_fileb -i $sample_indexb -o ./ -M 4 -m 3 -p 2 &> $log_fileb"
count="$(($count + 2))"
done <../sim_sample.txt > j_sim_ustackf3.txt

#cstacks
#!/bin/bash -l

#SBATCH --partition=regular
#SBATCH --job-name=cstacks_f3_thread20_mem30G
#SBATCH --output=cstacks_f3.out
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --mail-user=hh693@cornell.edu

export LD_LIBRARY_PATH=/usr/local/gcc-7.3.0/lib64:/usr/local/gcc-7.3.0/lib
export PATH=/programs/stacks-2.53/bin:$PATH

cstacks -P ./ -M ../info/popmap.catalog.simonly.tsv -n 4 -p 20 &> cstacks.oe

sbatch --nodes=1 --ntasks=20 --mem=30000 s_cstack_f3.sh


#ustacks
#make new parrellel script
while read p; do
echo "echo $p"
echo "echo "R1""
echo "sstacks -c ./ -s ../similis_clean_dups/STACKS_cut6/"$p"_R1.lengthtrim.fastq -o ./ &> slogs/"$p"_R1.sstacks.oe"
echo "echo "R2""
echo "sstacks -c ./ -s ../similis_clean_dups/STACKS_cut6/"$p"_R2.lengthtrim.fastq -o ./ &> slogs/"$p"_R2.sstacks.oe"
done <../sim_sample.txt > j_sstacks_f3.txt
#NOT SURE ABOUT FILE PATH LOCATION, but these are the trimmed reads used to make the catalog


#!/bin/bash -l

#SBATCH --partition=regular
#SBATCH --job-name=j_sstacksf3_thread10_mem160G
#SBATCH --output=sstacks_jlist_f3.out
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --mail-user=hh693@cornell.edu

export LD_LIBRARY_PATH=/usr/local/gcc-7.3.0/lib64:/usr/local/gcc-7.3.0/lib
export PATH=/programs/stacks-2.53/bin:$PATH

parallel -j 8 < j_sstacks_f3.txt

sbatch --nodes=1 --ntasks=10 --mem=160000 --dependency=afterok:3993 s_j_sstack_f3.sh
#Set this one to run only after cstacks has finished successfully

#Date: Sept 3rd 2021

#So the dependent did work but sstacks did not?
#It looks like it did though. Check sstacks_jlist_f3.out
#got to end (?)
#the only thing saying that it failed is the email, but everything else looked like it worked
#except I don't see matches.tsv? for any of them. So none of them worked?

Processing sample '../similis_clean_dups/STACKS_cut6/GA12_012_S119_R1.lengthtrim.fastq' [1 of 1]
 Unable to open '../similis_clean_dups/STACKS_cut6/GA12_012_S119_R1.lengthtrim.fastq'
Error: Unable to parse '../similis_clean_dups/STACKS_cut6/GA12_012_S119_R1.lengthtrim.fastq'

#That file does exist, so why can't it align?
#for sstacks, it has me provide sample name, not path.
#so why was I wanting to use earlier cuts instead of those in this directory?
#I said not sure

while read p; do
echo "echo $p"
echo "echo "R1""
echo "sstacks -c ./ -s "$p"_R1.lengthtrim -o ./ &> slogs/"$p"_R1.sstacks.oe"
echo "echo "R2""
echo "sstacks -c ./ -s "$p"_R2.lengthtrim -o ./ &> slogs/"$p"_R2.sstacks.oe"
done <../sim_sample.txt > j_sstacks_f3.txt

#Run on server cause hare lab is full
/programs/bin/labutils/mount_server cbsuhare /storage
#It did not seem possible to mount only a subdirectory of storage

export LD_LIBRARY_PATH=/usr/local/gcc-7.3.0/lib64:/usr/local/gcc-7.3.0/lib
export PATH=/programs/stacks-2.53/bin:$PATH

parallel -j 10 < j_sstacks_f3.txt

20378 Killed                  sstacks -c ./ -s CTM_002_S13_R1.lengthtrim -o ./ &>slogs/CTM_002_S13_R1.sstacks.oe
#I did not, it did....
#Maybe running 22 jobs at once was too much so instead I'm doing 10
#It does seem like it's using a lot of memory, even with only 10. That may have been why it killed one of them
#they have all crawel to each only using about 15% of cpu and the memory is maxed out
#It killed another one
#Trying 4

#Each maxes out at around 12.8% of 126G memory, so I could probably do up to 6 but 4 is good

#Started at 10:30, by 5:05, it was almost done but not quite

#tsv2bam
#tsv to bam and then gstacks
rename .lengthtrim. . *.tsv
rename _R1 . *.tsv
rename _R2 .2 *.tsv
# e.g. sample_01.1.fq.gz and sample_01.2.fq.gz, which is how process_radtags will output them.
tsv2bam -P ./ -M ./popmap.all.tsv --pe-reads-dir ../rename_seq/ #-t 8
		#make a folder of empty renamed raw read files for the paired end list
		FILES="./*alleles.tsv"
		for f in $FILES
		do
		touch ../faux_samples/"$f"
		done
		#now in that folder
		rename _R . *tsv
		rename lengthtrim.alleles.tsv fq *tsv
		rename .fq fq *fq
		ls > pop_all
		#remove stuff from file with sed
		sed 's/...fq.gz//' pop_all | uniq > pop_all_p
		#first and last . are actual . and the middle one it the one or two
		#this should also work
		sed 's/.1.fq.gz//' pop_all | sed 's/.2.fq.gz//' | uniq > pop_all_t

#not working
Error: Unable to load matches from './422_014_S128.matches.tsv(.gz)'.
tsv2bam -P stacks_dir -s sample [-s sample ...] [-R paired_reads_dir]
tsv2bam -P ./ -s MW_038_S71 -R ../rename_seq/
#Because read 1 has to have no .1
rename .1. . *.tsv

tsv2bam -P ./ -s $SAMPLE -R ../rename_seq/ &

SAMPLES=(
GBE0819_171_S122
GBE0819_172_S129
GBE_332_S107
GBE_333_S114
GBE_335_S121
GLD0819_001_S29
GLD0819_002_S41
GLD0819_003_S53
GLD0819_004_S65
GLD0819_005_S77
GLD0819_006_S88
GLD0819_007_S6
GLD0819_008_S18
GA12_025_S100
GA12_024_S148
GA12_023_S141
GA12_022_S134
GA12_021_S127
GA12_020_S120
GA12_019_S113
GA12_018_S106
GA12_017_S99
GA12_016_S147
GA12_015_S140
GA12_014_S133
GA12_013_S126
GA12_012_S119
GA12_010_S112
GA12_009_S105
GA12_008_S98
GA12_007_S146
GA12_006_S139
GA12_004_S132
GA12_003_S125
GA12_002_S118
GA12_001_S111
)

for SAMPLE in "${SAMPLES[@]}"; do
echo "$SAMPLE"
tsv2bam -P ./ -s $SAMPLE -R ../rename_seq/ &
done

ls *.bam | wc -l

tsv2bam -P ./ -s MW_055_S137 -R ../rename_seq/ &
tsv2bam -P ./ -s MW_056_S144 -R ../rename_seq/ &
tsv2bam -P ./ -s MW_052_S116 -R ../rename_seq/ &
#redid these cause I was worried that they were too small, but I think these were just the bad reads

#gstacks
export LD_LIBRARY_PATH=/usr/local/gcc-7.3.0/lib64:/usr/local/gcc-7.3.0/lib
export PATH=/programs/stacks-2.53/bin:$PATH
#For non-reference genomes:
gstacks -P ./ -M ./popmap.all.tsv -t 22

#produced
#catalog.fa.gz

#Date: Sept 5th 2021
#copy data back onto server to make sure I don't lose it
Spisula > Mounting > Unmount_Date
#Date: Sept 6th 2021

#(then populations)
mkdir pop_out_r80_undedup
populations -P ./ -M ../info/popmap.all.tsv -O ./pop_out_r80_undedup -r 0.80 --vcf --fstats --hwe -t 10 &> ./pop_out_r80_undedup/populations.oe &
#I started with this one and then filtered more in R
Kept 25658 loci, composed of 7777023 sites; 25013 of those sites were filtered, 346104 variant sites remained.

#one per p
mkdir pop_out_r90_p7_undedup
populations -P ./ -M ../info/popmap.all.tsv -O ./pop_out_r90_p7_undedup -r 0.90 -p 7 --vcf --fstats --hwe -t 10 &> ./pop_out_r90_p7_undedup/populations.oe &
grep "Kept" populations.oe
Kept 91 loci, composed of 27861 sites; 809 of those sites were filtered, 1323 variant sites remained.

#populations -P ./ -M ../info/popmap.all.tsv -O ./pop_out_r1 -r 1 --vcf --genepop --structure --fstats --hwe &> ./pop_out_r1/populations.oe 

#to match undedup pca
mkdir pop_out_r95_undedup
mkdir pop_out_r95_p7_undedup
populations -P ./ -M ../info/popmap.all.tsv -O ./pop_out_r95_undedup -r 0.95 --vcf --fstats --hwe -t 5 &> ./pop_out_r95_undedup/populations.oe &
populations -P ./ -M ../info/popmap.all.tsv -O ./pop_out_r95_p7_undedup -r 0.95 -p 7 --vcf --fstats --hwe -t 5 &> ./pop_out_r95_p7_undedup/populations.oe &

#Then, I can also try whitelisting it without the solidissima

#turn vcf into pca

grep -v "##" populations.snps.vcf | sed 's/#//g' > simplifiedfilvervcf.txt
sed 's/0\/0/0/g' simplifiedfilvervcf.txt | sed 's/0\/1/1/g' | sed 's/1\/1/2/g' > simvcfsimplereadyforR.txt

sed 's/.\/./NA/g' simvcfsimplereadyforR.txt > vcfwithnanforR.txt
#Then remove the header to import into R
#And the # infront of CHROM so that R does not ignore the first line as a comment

sed -E $'s/:\d\t//' simplifiedfilvervcf.txt > testa.txt

sed $'s/\b:\b.*\b\t&\b//' simplifiedfilvervcf.txt > testa.txt
sed 's/:*$(printf '\t')//' simplifiedfilvervcf.txt | head

head simplifiedfilvervcf.txt | perl -pi -e 's{ \b : \b .*? \b \t \b}{}x'
head simplifiedfilvervcf.txt | sed -re 's/\:.*\t/\t/g'
# need .* to do *
head simplifiedfilvervcf.txt | sed -re 's/\:.*\t/\t/'
head simplifiedfilvervcf.txt | sed -re 's/\t....*\t/\t/'

head simplifiedfilvervcf.txt | sed -re 's/\:..:/:/g'  | sed -re 's/\:....:/:/g' | sed -re 's/\:..................\t/:/g'	 


head simplifiedfilvervcf.txt | sed -e 's/\:*\t/a/g'
sed 's/from here.*to here/OVER THERE/'

head simplifiedfilvervcf.txt | awk 'BEGIN{FS=OFS="\t"} {sub(/:.*/,"",$2)} 1'
head simplifiedfilvervcf.txt | sed '/<p/,/<\/p/d'

#How the heck did I do this? Try just downloading it
#And, solved in 5 minutes dang
#Open in excel, make new tab, left(D3,3) etc then save again, but next time I could do this next step first and then do left (1)
sed 's/0\/0/0/g' simplifiedfilvervcf_nocolons.txt | sed 's/0\/1/1/g' | sed 's/1\/1/2/g' | sed 's/.\/./NA/g' > simvcfsimplereadyforR_nocolon.txt



#Don't forget about this:
sed 's/SFJ/2/g' WL_format2.structure |  sed 's/GBE/1/g'| sed 's/CCB/3/g'| sed 's/SCC/4/g'| sed 's/NLI/5/g'| sed 's/SLI/6/g'| sed 's/GA /7 /g' > WL_format4.structure




#######################################################################################
####START HERE TODAY####
#######################################################################################



#Create phylogenetic tree. 
#What data does it need?
	#Sequence data - could be recreated from VCF + reference fasta (but there is a VCF2POP tree software)
	#Straight from FASTA seems like the main way
	#What about from ddrad specifically. Filtered?
	#?Neutral loci?
	
	#BEAST commonly used - requires NEXUS file input
	#Nexus contains aligned genes
	seqmagick convert --output-format nexus --alphabet dna input.fasta output.nex
	#I think I could probably stack the aligned fastas on top of eachother:
		#Using this: http://www.ggvaidya.com/taxondna/
	#BEAST can also import FASTA files (as long as the sequences have been aligned) 
		#Likely still has the issue of multiple loci
		#catalog from cstacks produces a catalog.fa.gz
			#Typically a NNNNNNNN in the middle, cause ddrad
			#Can I use this with another alignment software with my stuff.
				#However, the computation needs of matching loci to any other locus is way more intense than matching up a bunch of 18S reads.
			#Can I do bwamem or bwa aln against the catalog or do it by each locus
	#An advantage of pyRAD over other RADseq data set assembly tools such as Stacks (Catchen et al. 2013) is that it is designed to assemble data for phylogenetic studies containing divergent species using global alignment clustering, which may include indel variation.
		# The filtered reads for each sample were clustered using the program USEARCH v.6.0.307 (Edgar 2010), and then aligned with MUSCLE (Edgar 2004).
		# From: https://academic.oup.com/gbe/article/7/3/706/602128
	#For this reason pyRAD is commonly employed for RADseq studies at deeper phylogenetic scales, however, it works equally well at shallow scales.
		#However, since my data is shallow, I should be fine without it
	#Align with MUSCLE
		#I'm Looking to create aligned fasta files
		#can ustacks save aligned fastas?
	
	





## STRUCTURE
#Date: August 23rd 2021
#230 G mem and 12 cpu taken up

#STRUCTURE runs with
structure >& my_run.log &
#Version 2.3.4
#After setting a directory with parameters in it

#Recommendations for running it:
#You will probably get similar answers with a random set of 10K SNPs as you do with 500K SNPs. 
#he default settings for BURNIN and NUMREPS are very conservative (i.e., longer than strictly necessary) for typical data sets. We anticipate that you may be able to get accurate results using much shorter runs than the default. In particular, you should be able to get accurate estimates of the ancestry using small values of NUMREPS.

#Export data from populations to structure
export LD_LIBRARY_PATH=/usr/local/gcc-7.3.0/lib64:/usr/local/gcc-7.3.0/lib
export PATH=/programs/stacks-2.53/bin:$PATH

populations -P ./ -M ../info/popmap.all.tsv -O ./unlinked_pop_out_4STRUCT -r 0.80  --write-single-snp --vcf --genepop --structure --fstats --hwe -t 3 &> ./unlinked_pop_out_4STRUCT/unlinked_populations_4STRUCT.oe &

#Date: August 25th 2021
#only 45000 avaliable mem
#make a file named mainparams

#looks like structure file from populations does not contain sample names
populations -P ./ -M ../info/popmap.all.tsv -O ./unlinked_pop_out_4STRUCT -r 0.90  --write-single-snp --vcf --genepop --structure --fstats --hwe -t 3 &> ./unlinked_pop_out_4STRUCT/unlinked_populations_4STRUCT.oe &
-p? #mimimum populations for loci
-f p_value?

#Include a whitelist of 1,000 random loci so that we output a computationally manageable amount of data to STRUCTURE:
populations -P ./stacks/ --popmap ./samples/popmap --smooth -p 10 -r 0.75 -f p_value -t 12 --structure --genepop --write-single-snp -W ./wl_1000
#Here is one method to generate a list of random loci from a populations summary statistics file (this command goes all on one line):
grep -v "^#" populations.sumstats.tsv |
cut -f 1 |
sort |
uniq |
shuf |
head -n 1000 |
sort -n > whitelist.tsv

populations -P ./ -M ../info/popmap.all.tsv -O ./unlinked_pop_out_4STRUCT_WL -r 0.90 -t 8 --structure --write-single-snp -W ./whitelist.tsv &> ./unlinked_pop_out_4STRUCT_WL/unlinked_populations_4STRUCT_WL.oe &

#there's a heading issue in the structure file. Woohoo no suprise
grep -v #
#then maybe structure also does not like to know the names of the loci
awk NR\>1 WL_noheader.structure > WL_noheader.structure
#slightly worried that 0 = missing data and not -9
#search for -9
grep -e -9 WL_NOHEAD.structure | wc -l
#yup, no negative nines, but there's a lot of zeros.
#Try it with both or check documentation

#running!
chmod u+x structure.sh

sbatch --ntasks=2 --mem=40000 structure.sh
#Uh, putting it in slurm was a little weird, it finished instantly with the & at the end and
#after removing that, it seems to not be adding to the log file after the version title
#my input file had been zeroed out so that probably wasn't helping
#AHHH did I change the number of loci?
#No I had not! That was definitely messing with it
#Still gave this error?
WARNING! Probable error in the input file.  
Individual 1, locus 838;  encountered the following data 
"SFJ" when expecting an integer
#There should be no locus 838?!?
head -1 WL_NOHEAD.structure | wc 
 1     800    1613
#Lines Words Characters..... uh
#So like locus = words - 2 (for name and pop) and I assumed other than the start...
#characters = words + 20ish
#some of the tabs are spaces or maybe just tabs are included = close to 2xwords

#set loci to 798
#OH inidividual ID is divided into 2 words?
#HOW WAS THIS WORKING BEFORE SLURM?
#Gave a bunch of locus 799 errors which makes NO SENSE
Individual 1, locus 1;  encountered the following data 
"SFJ" when expecting an integer

#test
test.structure
LABEL=1, POPDATA=1, NUMLOCI=440, PLOIDY=2, MISSING=9 (sic), ONEROWPERIND=0
Need to set NUMINDS>0 in the file mainparams 24+23
Need to set POPFLAG=0 or 1 in the file mainparams
Need to set PHENOTYPE=0 or 1 in the file mainparams
Need to set EXTRACOLS>=0 in the file mainparams		

#looks like it is working, so this really is an issue of formatting so the it understands.
#can I replace all spaces with the same number of spaces?
sed 's/\t/     /g' WL_NOHEAD.structure > WL_format1.structure
#remove one or more spaces
sed 's/ \{1,\}/ /g' WL_format1.structure > WL_format2.structure
head -1 WL_format2.structure | wc 
      1     800    1613
#The same as before. the x2 is the spaces
#There really should be 798 loci in this file given this. Where did the other 40 go?
#What about its not including missing data at all so made empty slots instead so they are variable lengths?
sed 'NUMq;d' WL_format2.structure | wc #second line
#looking for variable word counts between lines

for NUM in {1..300}
do
head -$NUM WL_format2.structure | tail -1 | wc
done
#The only difference is in sample name ID length
#which should nto be a problem

#From here: https://groups.google.com/g/structure-software/c/TKvHUx_eOI0
Use numerical designations for the POPDATA column.  Also, Structure expects missing data coded as -9, unless you explicitly tell it to use 0.
#AHHHHHHHHHHHHHH

'GBE '	'1 ' #I forgot to do this the first time so the site codes are wrong
'SFJ'	2
'CCB'	3
'SCC'	4
'NLI'	5
'SLI'	6
'GA '	'7 '

sed 's/SFJ/2/g' WL_format2.structure | 
sed 's/GBE/1/g'|
sed 's/CCB/3/g'|
sed 's/SCC/4/g'|
sed 's/NLI/5/g'|
sed 's/SLI/6/g'|
sed 's/GA /7 /g' > WL_format4.structure

#I SHOULD HAVE CHOSEN NUMBERS THAT MAKE SENSE
#ITS WORKING! HUZZAH!

#Looks like by setting max pops it assumed that population number which isn't what I want
#Arg! Now it's saying not enough memory.
#I guess because it has to try a bunch of things
#but not even 200 worked. So instead lets just try setting different ones
#define MAXPOPS 7
#working now

#took about 15 minutes to run for max pops 15
#maybe can I set a minimum and maximum K
#I can import the output (along with the log file) into the desktop structure program to get the graphs

#Structure is quarantined when installed on a mac
xattr -d com.apple.quarantine Structure.app/
#Nope, still don't work, probably that catalina nonsense. Gotta try it on a PC I guess

-K (MAXPOPS) Change the number of populations.
-o (output file) Print results to a different output file.

[hh693@cbsuhare STRUCTURE]$ less tests/structure.sh 

#!/bin/bash -l

#SBATCH --partition=regular
#SBATCH --job-name=strctr_100G_4thread
#SBATCH --output=strctr_parallel.out
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --mail-user=hh693@cornell.edu

parallel -j 4 < j_structure_miss0_Ktest

for f in {1..9}
do
echo "structure -K $f -o out_K$f  >& my_run_K$f.log"
done > j_structureKtest

sbatch --ntasks=4 --mem=100000 s_paraSTRUCTURE.sh 


#Cape cod similis are more similar to solidissima than they are to GA sim
#but GA sim and NLI sim group together a little
#I wonder what is going on with the 1999 samples cause they are so differenciated
#Is this possibly showing change in genes over time where newer groups have some resemblance to eachother that 1999 doesn't have?


#Date: August 16th 2021
#So I've made a structure plot and a PCA what do I do next?

#STRUCTURE WITHOUT SOLIDISSIMA
grep -v 422 WL_format4.structure | grep -v BLP | 
grep -v CUP | grep -v BAR | grep -v PT | grep -v GBE > WL_nosolidis.structure

#This did not work in full, there's still some from pop 1, 3, and 6
#BUT IT DID WORK IN missingis0 so I must have just done it wrong
#Yeah, I forgot to set the input file

wc -l WL_nosolidis.structure
284 #/2 = 142

422					SJF		South Jersey Federal Survey
BAR					CCB		Cape Cod Bay 2017 inside bay
BLP					SLI		Long Island 2019
CTM					SCC		Cape Cod Bay 2017 south of bay
CUP					SLI		Long Island 2019
ELP					SCC		Cape Cod Bay 2017 south of bay
GA					GA		Georgia
GBE					GBE		Georges Bank
GLD					NLI		Long Island 2019 
MW					SCC		Southern Cape Cod 2012 south of bay
PEC					NLI		Long Island 2019
PPB					SCC		Cape Cod Bay 2017 south of bay
PT					CCB 	Cape Cod Bay 2012
RP20				NLI
WFH					SCC		Cape Cod Bay 2017 south of bay Buzzard Bay

GBE
SJF
CCB
SCC
NLI
SLI
GA

for f in {1..7}
do
echo "structure -K $f -o nosolidis/out_K$f  >& nosolidis/my_run_K$f.log"
done > j_structure_nosol_Ktest

sbatch --ntasks=4 --mem=80000 s_paraSTRUCTnosol.sh

#WITHOUT 1999
	grep -v 422 WL_format4.structure > WL_no1999.structure

#RUN WITH AND WITHOY SOME SAMPLES SETTING MISSING DATA TO ZERO

for f in {1..7}
do
echo "structure -K $f -i WL_format4.structure -N 150 -o missingis0/out_K$f  >& missingis0/my_run_K$f.log"
echo "structure -K $f -i WL_no1999.structure -N 148 -o missingis0/no1999/out_K$f  >& missingis0/no1999/my_run_K$f.log"
echo "structure -K $f -i WL_nosolidis.structure -N 142 -o missingis0/nosolidis/out_K$f  >& missingis0/nosolidis/my_run_K$f.log"
done > j_structure_miss0_Ktest

sbatch --ntasks=4 --mem=100000 s_STRUCT_miss0.sh

#The missing is zero graphs look...less....good? More mixed.
#This suggests that missing is marked with 0s 
https://simison.com/brian/Structure_notes.html
https://groups.google.com/g/stacks-users/c/pZgvAqup6YA

#Also helpful
https://github.com/enormandeau/stacks_workflow
#Limitations of the ancestry-based missing data imputation
#Light: Using all the SNPs versus using only neutral SNPs with admixture can change the ancestry estimation of samples. For example, the CV could vary differently as a function of K.
#Moderate: Admixture requires that the individuals be unrelated. Some level of half-sibs or full sibs is probably OK, but watch out for datasets with a lot of related samples. You can use the relatedness part of the filtration steps listed above to check that.
#IMPORTANT: Admixture is a poor choice for samples with a continuous genetic gradient, a pattern of isolation by distance or a dataset with a lot of populations with very low or unequal sample numebrs. Using a k-nearest neighbors approach may be better in this case.

#Okay, so I think we need to go back to STACKS and figure out what is going on with the vcf > structure conversion because it should not all be missing data, especially after filtering.
plink
#To use version 1.07, plink is in your path. You can directly type the command "plink"
#To use newer version of plink, plink 1.9  (beta version), you will need to use full path to access it: /programs/plink-1.9-x86_64-beta5/plink
#With plink 1.9:
plink --vcf [vcf filename] --recode structure
plink --vcf [vcf filename] --recode structure --out <base name>

#start from scratch with populations
#so from STACKS, I have matches.bam files from tsv2bam which I then input into gstacks
#gstacks program will output two major files, catalog.fa.gz, which contains the consensus sequence for each assembled locus in the data, as well as catalog.calls, a custom file that contains genotyping data. These files are intended to be read by the populations program, which can apply appropriate filters and export the data.

#Oh! Maybe my all missing data comes from removing the linked loci after filtering the quantity?
#Also if 700 loci is about 15 minutes for structure, I can do a couple more.

export LD_LIBRARY_PATH=/usr/local/gcc-7.3.0/lib64:/usr/local/gcc-7.3.0/lib
export PATH=/programs/stacks-2.53/bin:$PATH

populations -P ./ -M ../info/popmap.all.tsv -O ./run_pops_Aug26/unlinked --write-single-snp --vcf --plink --structure --fstats --hwe -t 4 &> ./run_pops_Aug26/unlinked/unlinked_populations.oe &
populations -P ./ -M ../info/popmap.all.tsv -O ./run_pops_Aug26/rand_unlinked --write-random-snp --vcf --plink --structure --fstats --hwe -t 4 &> ./run_pops_Aug26/rand_unlinked/rand_unlinked_populations.oe &
populations -P ./ -M ../info/popmap.all.tsv -O ./run_pops_Aug26/p7_unlinked -p 7 --write-single-snp --vcf --plink --structure --fstats --hwe -t 4 &> ./run_pops_Aug26/p7_unlinked/p7_unlinked_populations.oe &

-p 7
#the locus has to be present in each population to be included
#then I can probably filter a vcf after populations for % of missing data





#Examine K likelihoods

#FST

		
		
		
		
		
		
		
		


############################ OLDER/MESSIER ######################################

#Date: July 2021
#get a list of your samples
ls samplefolder > samplelist
sed 's/_R._001.fastq.gz//' samplelist_sim.txt | uniq > sim_sample.txt
 
#run a while loop over that in a slurm script which trims raw data and gives stats
mkdir #(all directories used in script)

touch s_scriptNAME_trim.sh
nano s_scriptNAME_trim.ch
#START SCRIPT 
#!/bin/bash -l

#SBATCH --partition=regular
#SBATCH --job-name=NAME
#SBATCH --output=s_NAME.out
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --mail-user=hh693@cornell.edu

export PATH=/programs/seqtk:$PATH
export PATH=/programs/seqkit-0.15.0:$PATH

while read p; do
echo "$p"
#remove duplicate reads
  seqkit rmdup similis_seq_raw/"$p"_R1_001.fastq.gz -s -o dedup2_raw/"$p"_R1.rawdedup.fastq -D dups2/"$p"_R1_dups.txt -d dups2/"$p"_R1_dup_seq.fastq -j 10
  seqkit rmdup similis_seq_raw/"$p"_R2_001.fastq.gz -s -o dedup2_raw/"$p"_R2.rawdedup.fastq -D dups2/"$p"_R2_dups.txt -d dups2/"$p"_R2_dup_seq.fastq -j 10	

#remove padding on R1 #possible that I should do this for R2 and just add back in the no cut sites but seems unnessecary
  perl gbstrim.pl --enzyme1 psti --enzyme2 mspi --fastqfile dedup2_raw/"$p"_R1.rawdedup.fastq --read R1 --outputfile padtrim_dedup/"$p"_R1.trim.fastq --verbose --threads 10 --minlength 50 >& gbstrim_stats/"$p"_R1_dedup.log 

#remove adapter and padding on R2 and quality check
  java -jar /programs/trimmomatic/trimmomatic-0.39.jar SE -phred33 -threads 8 dedup2_raw/"$p"_R2.rawdedup.fastq ommatic_dedup2/"$p"_R2.fastq ILLUMINACLIP:/programs/trimmomatic/adapters/NexteraPE-PE.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:50 >& ommatic_logs/"$p"_R2_trimmo_dedup.log
  seqtk trimfq -b 8 -e 8 ommatic_dedup2/"$p"_R2.fastq > ommatic_dedup2_phase2/"$p"_R2.fastq

#quality check R1
  java -jar /programs/trimmomatic/trimmomatic-0.39.jar SE -phred33 -threads 8 padtrim_dedup/"$p"_R1.trim.fastq padtrim_dedup/R1s_with_slidewindow/"$p"_R1.trim.fastq SLIDINGWINDOW:4:15 MINLEN:50 >& ommatic_logs/"$p"_R1_trimmo_dedup.log

#resync
  perl resync.pl padtrim_dedup/R1s_with_slidewindow/"$p"_R1.trim.fastq ommatic_dedup2_phase2/"$p"_R2.fastq sync_trim_5/"$p"_R1.trim.sync.fastq sync_trim_5/"$p"_R2.trim.sync.fastq >& sync_trim_5/"$p"_resync5.log 
done <sim_sample.txt
#run quality analysis in same script or new if desired on...
#raw, removed dups, resynced
fastqc -t 8 -o sync5_fastqc sync_trim_5/*.fastq
multiqc -n multiqc_sync5_report.html -o sync5_multiqc ./sync5_fastqc
#repeat for each step listed above

#END SCRIPT
sbatch --nodes=1 --ntasks=30 --mem=10000 s_scriptNAME_trim.sh

#there are more stats to be gleaned from the created gbs trim stats file using grep if desired.


########################## NOTES ################################

#TRIMMING
#Date: July 27th
#STACKS protocol has a recommendation or I could do it like I did for the pilots
#https://www.nature.com/articles/nprot.2017.123
#They use something other than trimming, their own process_rad_tags

#TRIMMOMATIC
for SAMPLE in ${SAMPLELIST[@]} 
do
echo $SAMPLE
java -jar /programs/trimmomatic/trimmomatic-0.36.jar PE -threads 10 $SAMPLE'_R1.fastq.gz' $SAMPLE'_R2.fastq.gz' $SAMPLE'_AdapterClipped_F_paired.fastq' $SAMPLE'_AdapterClipped_F_unpaired.fastq' $SAMPLE'_AdapterClipped_R_paired.fastq' $SAMPLE'_AdapterClipped_R_unpaired.fastq' ILLUMINACLIP:/programs/trimmomatic/adapters/NexteraPE-PE.fa:2:30:10
done
#REMOVED FRONT END ADAPTERS ON MOST
Input Read Pairs: 43052885 Both Surviving: 43052759 (100.00%) Forward Only Surviving: 126 (0.00%) Reverse Only Surviving: 0 (0.00%) Dropped: 0 (0.00%)
#NOT WORKING ON A LATER DATE

#https://www.nature.com/articles/s41438-020-00353-6#Sec2
#Demultiplexing of raw Illumina reads was carried out using the process_radtags utility included in Stacks v2.0

#I used PstI-MspI
# meaning after my barcode there should be GGC/CGG, or ACGTC/CTGCA
zcat similis_seq_raw/MW_001_S7_R1_001.fastq.gz | head -20
													#TGCAG
													#shows up in about 80% of the reads near the beginning	
# how long are my barcodes?
#U of M info on processing their stuff:
#https://bitbucket.org/jgarbe/gbstrim/src/master/
# variable length adjusters: can be from 1-8 bases long
# The padding sequences have variable length, therefore after trimming the 5' padding sequence the reads will have variable length. 
# Some GBS software, such as Stacks, requires all reads to be the same length, in which case you should use the 
--croplength #option to crop all reads down to the same length and discard reads shorter than the croplength.

#requires the cutadapt to use but cutadapt is already installed
#Try with MspI as the first enzyme first

#my reads are 152 bases with the barcode and enzymes. so what should min length be?
#start with 50 and see where that gets me
perl gbstrim.pl --enzyme1 mspi --enzyme2 psti --fastqfile similis_seq_raw/422_014_S128_R1_001.fastq.gz --read R1 --outputfile trim_test/422_014_R1_test.trim.fastq --verbose --threads 8 --minlength 50
#in this order it removed everything so try switching order
perl gbstrim.pl --enzyme1 psti --enzyme2 mspi --fastqfile similis_seq_raw/422_014_S128_R1_001.fastq.gz --read R1 --outputfile trim_test/422_014_R1_test_switch.trim.fastq --verbose --threads 8 --minlength 50 >& gbstrim_stats/422_014.log 
#STATS End
#GBStrim discarded 7.89% of the reads. Less than 10% is good.
#WOOOHOOO!
#now try saving the output from that to a log file (used >&)



perl gbstrim.pl --enzyme1 mspi --enzyme2 psti --read R1 --fastqfile 422_014_S128_R1_001.fastq.gz --outputfile out.fastq --verbose


#I removed this error code by removing the part that would call this:
#Error:
#Name "main::enzyme1" used only once: possible typo at gbstrim.pl line 87.
#--fastqfile, --outputfile, --read, --enzyme1 and enzyme2 are required
#Usage:
#    gbstrim.pl --enzyme1 bamhi --enzyme2 psti --read R1 [--threads 1]
#    [--minlength 20] --fastqfile file.fastq --outputfile out.fastq


#For paired end data, after running R1 and R2 run separately through the first thing:
resync.pl sample1_R1.trim.fastq sample1_R2.trim.fastq sample1_R1.trim.sync.fastq sample1_R2.trim.sync.fastq

#sample list of file names turn into a list of just the sample ids
sed 's/_R._001.fastq.gz//' samplelist_sim.txt | uniq > sim_sample.txt
#remove (aka replace with the thing between the second // which is nothing)
#where R. is the single position wildcard (like * but only one)
#now I have two of each row
#uniq sim_sample.txt > sim_sample.txt 

#also what happened to the naming of BLP and CUP???


#while loop (instead of for loop) over all samples
while read p; do
echo "$p"
  perl gbstrim.pl --enzyme1 psti --enzyme2 mspi --fastqfile similis_seq_raw/"$p"_R1_001.fastq.gz --read R1 --outputfile trimmed/"$p"_R1.trim.fastq --verbose --threads 8 --minlength 50 --croplength 142 >& gbstrim_stats/"$p"_R1.log 
  perl gbstrim.pl --enzyme1 psti --enzyme2 mspi --fastqfile similis_seq_raw/"$p"_R2_001.fastq.gz --read R2 --outputfile trimmed/"$p"_R2.trim.fastq --verbose --threads 8 --minlength 50 --croplength 142 >& gbstrim_stats/"$p"_R2.log 
done <sim_sample.txt

#did it finish? if all in. Last sample is WFH_008_S86
#woot all worked


#Also for analysis pull the line in each GBS trim log file that describes how much was removed for each one
grep "inputreads" 
grep "GBStrim"

#rm list_trimstats
touch list_trimstats
while read p; do
echo "$p" >> list_trimstats
echo "R1" >> list_trimstats
grep "inputreads" gbstrim_stats/"$p"_R1.log >> list_trimstats
grep "GBStrim" gbstrim_stats/"$p"_R1.log >> list_trimstats
echo "R2" >> list_trimstats
grep "inputreads" gbstrim_stats/"$p"_R2.log >> list_trimstats
grep "GBStrim" gbstrim_stats/"$p"_R2.log >> list_trimstats
done <sim_sample.txt

#ALL of my R2s were wrong so therefore I'm gonna conclude that they needed to have enzymes reversed.
#Nope! I forgot to assign them as -read R2 which fixes it so it looks for adapter at the other end
#But it still removed 70% of the reads, mostly because no 5' cut site was found (flipping the enzymes and saying R2 does not help)
zcat similis_seq_raw/MW_009_S8_R1_001.fastq.gz | wc -l
4782540
4310800 trimmed/MW_009_S8_R1.trim.fastq
 384488 trimmed/MW_009_S8_R1.trim.fastq.nocutsite
  87240 trimmed/MW_009_S8_R1.trim.fastq.no3cutsite

zcat similis_seq_raw/MW_009_S8_R2_001.fastq.gz | wc -l
4782540
   4244 trimmed/MW_009_S8_R2.trim.fastq
4696368 trimmed/MW_009_S8_R2.trim.fastq.nocutsite
  81928 trimmed/MW_009_S8_R2.trim.fastq.no3cutsite

#So the trimming just separates them into new files but also probably is shortening each read as needed

#all are 143 long R1 same with R2. So maybe add together trim.fastq and trim.fastq.nocutsite
while read p; do
echo "$p"
cat trimmed/"$p"_R1.trim.fastq trimmed/"$p"_R1.trim.fastq.nocutsite > trimmed/"$p"_R1.trim.including5.fastq
cat trimmed/"$p"_R2.trim.fastq trimmed/"$p"_R2.trim.fastq.nocutsite > trimmed/"$p"_R2.trim.including5.fastq
wc -l trimmed/"$p"_R1.trim.including5.fastq
wc -l trimmed/"$p"_R2.trim.including5.fastq

done <sim_sample.txt

#Not perfect but why the heck doesn't it make a folder for no5cutsite and also why is it doing this in the first place?


#now resync them
while read p; do
echo "$p"
perl resync.pl trimmed/"$p"_R1.trim.including5.fastq trimmed/"$p"_R2.trim.including5.fastq sync_trim/"$p"_R1.trim.sync.fastq sync_trim/"$p"_R2.trim.sync.fastq >& sync_trim/"$p"_resync.log 
done <sim_sample.txt

#Results look like:
1000000 reads processed in first file
1000000 reads processed in second file
1598491 total reads processed in first file
1555845 total reads processed in second file
1598491 reads in trimmed/422_014_S128_R1.trim.including5.fastq, 71491 without mate
1555845 reads in trimmed/422_014_S128_R2.trim.including5.fastq, 28845 without mate
#So then I assume that they removed those 90k without mates

#repeat resync as a test with new file
perl resync.pl sync_trim/422_014_S128_R1.trim.sync.fastq sync_trim/422_014_S128_R2.trim.sync.fastq test_R1.fasta test_R2.fasta>& test_resync.log 
1527000 total reads processed in first file
1527000 total reads processed in second file
1527000 reads in sync_trim/422_014_S128_R1.trim.sync.fastq, 0 without mate
1527000 reads in sync_trim/422_014_S128_R2.trim.sync.fastq, 0 without mate
#works, but what a weirdly even number

#Now R1 and R2 are always the same length but is it the same across samples (important for STACKS)?
#No, they are mostly around 140 but are not all equal.
while read p; do
echo "$p"
grep "Trimming all reads down to" gbstrim_stats/"$p"_R1.log
grep "Trimming all reads down to" gbstrim_stats/"$p"_R2.log
done <sim_sample.txt


#Full while loop that does it all while I'm at the gym
#trim, grab both sets, get the stats, resync
while read p; do
echo "$p"
  perl gbstrim.pl --enzyme1 psti --enzyme2 mspi --fastqfile similis_seq_raw/"$p"_R1_001.fastq.gz --read R1 --outputfile pad_trimmed/"$p"_R1.trim.fastq --verbose --threads 10 --minlength 50 --croplength 142 >& gbstrim_stats/"$p"_R1.log 
  perl gbstrim.pl --enzyme1 psti --enzyme2 mspi --fastqfile similis_seq_raw/"$p"_R2_001.fastq.gz --read R2 --outputfile pad_trimmed/"$p"_R2.trim.fastq --verbose --threads 10 --minlength 50 --croplength 142 >& gbstrim_stats/"$p"_R2.log 
  cat pad_trimmed/"$p"_R1.trim.fastq pad_trimmed/"$p"_R1.trim.fastq.nocutsite > pad_trimmed/"$p"_R1.trim.including5.fastq
  cat pad_trimmed/"$p"_R2.trim.fastq pad_trimmed/"$p"_R2.trim.fastq.nocutsite > pad_trimmed/"$p"_R2.trim.including5.fastq
  wc -l pad_trimmed/"$p"_R1.trim.including5.fastq
  wc -l pad_trimmed/"$p"_R2.trim.including5.fastq
  echo "$p" >> list_trimstats
  echo "R1" >> list_trimstats
  grep "inputreads" gbstrim_stats/"$p"_R1.log >> list_trimstats
  grep "GBStrim" gbstrim_stats/"$p"_R1.log >> list_trimstats
  echo "R2" >> list_trimstats
  grep "inputreads" gbstrim_stats/"$p"_R2.log >> list_trimstats
  grep "GBStrim" gbstrim_stats/"$p"_R2.log >> list_trimstats
  perl resync.pl pad_trimmed/"$p"_R1.trim.including5.fastq pad_trimmed/"$p"_R2.trim.including5.fastq sync_trim/"$p"_R1.trim.sync.fastq sync_bartrim/"$p"_R2.trim.sync.fastq >& sync_bartrim/"$p"_resync.log 
done <sim_sample.txt


#Looking good!

#What would Trimmomatic do to them now?
#Sliding window quality.
#But this trimming could make them be uneven length which would mess with STACKS so anything that was tagged with a sliding window would be removed rather than cut.

#Check fastQC also

#STACKS
#To check for M parameter, chose a dozen or so 
#From across read coverage averages

#First four are big (1.4G)
#Then four medium (800M), then four smaller (600M)
CTM_002_S13_R1.trim.sync	CTM
CTM_009_S2_R1.trim.sync	CTM
PPB_001_S26_R1.trim.sync	PPB
PPB_006_S85_R1.trim.sync	PPB
MW_010_S20_R1.trim.sync	MW
MW_063_S97_R1.trim.sync	MW
CTM_001_S1_R1.trim.sync	CTM
CTM_006_S61_R1.trim.sync	CTM
422_033_S135_R1.trim.sync	S422
422_014_S128_R1.trim.sync	S422
CTM_004_S37_R1.trim.sync	CTM
#This does only test R1s and not R2s
CTM_002_S13_R2.trim.sync	CTM
CTM_009_S2_R2.trim.sync	CTM
PPB_001_S26_R2.trim.sync	PPB
PPB_006_S85_R2.trim.sync	PPB
MW_010_S20_R2.trim.sync	MW
MW_063_S97_R2.trim.sync	MW
CTM_001_S1_R2.trim.sync	CTM
CTM_006_S61_R2.trim.sync	CTM
422_033_S135_R2.trim.sync	S422
422_014_S128_R2.trim.sync	S422
CTM_004_S37_R2.trim.sync	CTM

touch popmap.test_samples.tsv
nano popmap.test_samples.tsv

touch popmap.test_samplesR2.tsv
nano popmap.test_samplesR2.tsv

#Failed due to memory without setting it.
sbatch --nodes=1 --ntasks=20 --mem=48000 slurm_stacks_mtest2.sh
sbatch --nodes=1 --ntasks=20 --mem=48000 slurm_stacks_mtest_R2.sh

#Date: July 28th

#MultiQC
#I think my data is already demultiplexed?
export LC_ALL=en_US.UTF-8
export PYTHONPATH=/programs/multiqc-1.10.1/lib64/python3.6/site-packages:/programs/multiqc-1.10.1/lib/python3.6/site-packages
export PATH=/programs/multiqc-1.10.1/bin:$PATH
multiqc --help

multiqc ./similis_seq_raw/Hare_Project_003/Analysis/illumina-basicQC/fastqc/
#Okay so the problem is that U of M only provided the html of each fastqc but multi qc needs the rest of the products from the fastqc

#So first run fast qc on all reads
/programs/FastQC-0.11.8/fastqc [options]
#OR
export PATH=/programs/FastQC-0.11.8:$PATH

fastqc 

fastqc -t 8 -o raw_fastqc similis_seq_raw/*.gz
fastqc -t 8 -o barcodetrim_fastqc sync_bartrim/*.fastq

multiqc -n multiqc_raw_report.html -o raw_multi_temp ./raw_fastqc
multiqc -n multiqc_bartrim_report.html -o bartrim_multi_temp ./barcodetrim_fastqc


#Compare to BioHPC text file that has adapter sequences in it:
#Also, some reads may read through into the Illumina adapter sequence which begins with 
#CTGTCTCTTATACACATCTCCGAG
#CTGTCTCTTATACACATCTCCGAGCCCACGAGAC < NexteraPE-PE.fa Trans2_rc
#Does not match exactly but it's the only close match. Is too long okay? Try trimmomatic calling nextera

#To find match
#or write to the code author to ask for adapter, John Garbe
#ask question about the 5' end on the R2s

#So my good R1s are in 
pad_trimmed/*R1.trim.fastq
#Now R2, I want 1) Trimmomatic for illumina 2) cut of 8 bases at the front and back

java -jar /programs/trimmomatic/trimmomatic-0.39.jar 
#Common adapter files are in
/programs/trimmomatic/adapters

java -jar /programs/trimmomatic/trimmomatic-0.39.jar PE -phred33 input_1.fastq input_2.fastq \
kept_paired_end_reads_1.fastq unpaired_1.fastq kept_paired_end_reads_2.fastq unpaired_2.fastq \
ILLUMINACLIP:/programs/trimmomatic/adapters/NexteraPE-PE.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:50

java -jar trimmomatic-0.35.jar PE -phred33 input_forward.fq.gz input_reverse.fq.gz 
output_forward_paired.fq.gz output_forward_unpaired.fq.gz output_reverse_paired.fq.gz output_reverse_unpaired.fq.gz 
ILLUMINACLIP:TruSeq3-PE.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36

:2:30:10:2:keepBothReads LEADING:3 TRAILING:3 #?
#Should I do trimmomatic paired or unpaired?

#My original plan was just unpaired, do R1s and then resync
java -jar trimmomatic-0.39.jar SE -phred33 input.fq.gz output.fq.gz ILLUMINACLIP:/programs/trimmomatic/adapters/NexteraPE-PE.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:50
#And my R2s in need of help start from raw


touch s_trimmo_R2.sh
nano s_trimmo_R2.sh

#!/bin/bash -l

#SBATCH --partition=regular
#SBATCH --job-name=trimmo_R2
#SBATCH --output=s_trimmomatic.out
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --mail-user=hh693@cornell.edu

while read p; do
echo "$p"
java -jar /programs/trimmomatic/trimmomatic-0.39.jar SE -phred33 -threads 8 similis_seq_raw/"$p"_R2_001.fastq.gz ommatic_phase1/"$p"_R2_001.fastq.gz ILLUMINACLIP:/programs/trimmomatic/adapters/NexteraPE-PE.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:50 >& ommatic_logs/"$p"_R2_trimmomatic.log
done <sim_sample.txt
sbatch --nodes=1 --ntasks=8 --mem=48000 s_trimmo_R2.sh

#to trim 8 bases off front and end, use fastx
fastx_trimmer -h

Usage:   seqtk trimfq [options] <in.fq>

Options: -q FLOAT    error rate threshold (disabled by -b/-e) [0.05]
         -l INT      maximally trim down to INT bp (disabled by -b/-e) [30]
         -b INT      trim INT bp from left (non-zero to disable -q/-l) [0]
         -e INT      trim INT bp from right (non-zero to disable -q/-l) [0]
         -L INT      retain at most INT bp from the "5'"-end (non-zero to disable -q/-l) [0]
         -Q          force FASTQ output 

export PATH=/programs/seqtk:$PATH
seqtk trimfq -b 8 -e 8 input.fastq > output.fastq


touch s_paddingremove_R2.sh
nano s_paddingremove_R2.sh

#!/bin/bash -l

#SBATCH --partition=regular
#SBATCH --job-name=pad_R2
#SBATCH --output=s_padR2.out
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --mail-user=hh693@cornell.edu

export PATH=/programs/seqtk:$PATH
while read p; do
	echo "$p"
	seqtk trimfq -b 8 -e 8 ommatic_phase1/"$p"_R2_001.fastq.gz > o_pad_phase2/"$p"_R2_001.fastq
	perl resync.pl pad_trimmed/"$p"_R1.trim.fastq o_pad_phase2/"$p"_R2_001.fastq sync_trim_2/"$p"_R1.trim.sync.fastq sync_trim_2/"$p"_R2.trim.sync.fastq >& sync_trim_2/"$p"_resync.log 
	wc -l pad_trimmed/"$p"_R1.trim.fastq 
	wc -l o_pad_phase2/"$p"_R2_001.fastq
done <sim_sample.txt

sbatch --nodes=1 --ntasks=8 --mem=10000 s_paddingremove_R2.sh
#running sbatch with multiple threads means it will try to do the wc of phase2 before seqtk has finished
#No, that was not the problem. I had not yet made the o_pad_phase2 directory

#Check sizes
#These are resynced after this protocol
-rw-rw-r--  1 hh693 hh693 479M Jul 28 18:29 422_014_S128_R1.trim.sync.fastq
-rw-rw-r--  1 hh693 hh693 452M Jul 28 18:29 422_014_S128_R2.trim.sync.fastq
-rw-rw-r--  1 hh693 hh693 540M Jul 28 18:30 422_033_S135_R1.trim.sync.fastq
-rw-rw-r--  1 hh693 hh693 504M Jul 28 18:30 422_033_S135_R2.trim.sync.fastq
-rw-rw-r--  1 hh693 hh693 559M Jul 28 18:30 BAR_002_S136_R1.trim.sync.fastq
-rw-rw-r--  1 hh693 hh693 523M Jul 28 18:30 BAR_002_S136_R2.trim.sync.fastq
-rw-rw-r--  1 hh693 hh693 747M Jul 28 18:30 BLP0819_179_S101_R1.trim.sync.fastq
-rw-rw-r--  1 hh693 hh693 703M Jul 28 18:30 BLP0819_179_S101_R2.trim.sync.fastq
-rw-rw-r--  1 hh693 hh693 419M Jul 28 18:30 BLP0819_180_S108_R1.trim.sync.fastq
-rw-rw-r--  1 hh693 hh693 391M Jul 28 18:30 BLP0819_180_S108_R2.trim.sync.fastq

#Compare to sync bartrim (my first idea)
-rw-r-----  1 hh693 hh693 515M Jul 27 18:02 422_014_S128_R1.trim.sync.fastq
-rw-r-----  1 hh693 hh693 540M Jul 27 18:02 422_014_S128_R2.trim.sync.fastq
-rw-r-----  1 hh693 hh693 589M Jul 27 18:03 422_033_S135_R1.trim.sync.fastq
-rw-r-----  1 hh693 hh693 616M Jul 27 18:03 422_033_S135_R2.trim.sync.fastq
-rw-r-----  1 hh693 hh693 626M Jul 27 18:03 BAR_002_S136_R1.trim.sync.fastq
-rw-r-----  1 hh693 hh693 655M Jul 27 18:03 BAR_002_S136_R2.trim.sync.fastq
-rw-r-----  1 hh693 hh693 819M Jul 27 18:03 BLP0819_179_S101_R1.trim.sync.fastq
-rw-r-----  1 hh693 hh693 856M Jul 27 18:03 BLP0819_179_S101_R2.trim.sync.fastq
-rw-r-----  1 hh693 hh693 667M Jul 27 18:04 BLP0819_180_S108_R1.trim.sync.fastq
-rw-r-----  1 hh693 hh693 697M Jul 27 18:04 BLP0819_180_S108_R2.trim.sync.fastq

#100-200 MB smaller on average.
#Should probably be comparing though read count rather than file size but it does give me an idea


#Run trimmomatic for sliding window on R1

#!/bin/bash -l

#SBATCH --partition=regular
#SBATCH --job-name=slidewin_R1
#SBATCH --output=s_slidewin_R1.out
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --mail-user=hh693@cornell.edu
while read p; do
	echo "$p"
	java -jar /programs/trimmomatic/trimmomatic-0.39.jar SE -phred33 pad_trimmed/"$p"_R1.trim.fastq pad_trimmed/R1s_with_slidewindow/"$p"_R1.trim.fastq SLIDINGWINDOW:4:15
done <sim_sample.txt

sbatch --nodes=1 --ntasks=8 --mem=10000 s_R1_slidewindow.sh
#Finished quickly, compare size of 
-rw-rw-r-- 1 hh693 hh693 480M Jul 28 19:05 422_014_S128_R1.trim.fastq
-rw-rw-r-- 1 hh693 hh693 579M Jul 28 19:06 422_033_S135_R1.trim.fastq
-rw-rw-r-- 1 hh693 hh693 593M Jul 28 19:06 BAR_002_S136_R1.trim.fastq
-rw-rw-r-- 1 hh693 hh693 764M Jul 28 19:06 BLP0819_179_S101_R1.trim.fastq
-rw-rw-r-- 1 hh693 hh693 632M Jul 28 19:06 BLP0819_180_S108_R1.trim.fastq
#meaningfully but not aggressively reduced the size of the R1s

#What about if I also remove adapters?
while read p; do
	echo "$p"
	java -jar /programs/trimmomatic/trimmomatic-0.39.jar SE -phred33 pad_trimmed/"$p"_R1.trim.fastq pad_trimmed/R1s_with_slidewindow_adapters2/"$p"_R1.trim.fastq ILLUMINACLIP:/programs/trimmomatic/adapters/NexteraPE-PE.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15
done <sim_sample.txt
#Seems like it is dropping very few if any additional in comparison to just sliding window especially
-rw-r----- 1 hh693 hh693 480M Jul 28 19:21 422_014_S128_R1.trim.fastq
-rw-r----- 1 hh693 hh693 579M Jul 28 19:21 422_033_S135_R1.trim.fastq
-rw-r----- 1 hh693 hh693 593M Jul 28 19:21 BAR_002_S136_R1.trim.fastq
-rw-r----- 1 hh693 hh693 764M Jul 28 19:22 BLP0819_179_S101_R1.trim.fastq
-rw-r----- 1 hh693 hh693 632M Jul 28 19:22 BLP0819_180_S108_R1.trim.fastq
#Woohooo no adapters!!

#Date: July 29
#So then just do a new resync
while read p; do
	echo "$p"
	perl resync.pl pad_trimmed/R1s_with_slidewindow/"$p"_R1.trim.fastq o_pad_phase2/"$p"_R2_001.fastq sync_trim_3/"$p"_R1.trim.sync.fastq sync_trim_3/"$p"_R2.trim.sync.fastq >& sync_trim_3/"$p"_resync3.log 
done <sim_sample.txt

-rw-r-----  1 hh693 hh693  460M Jul 29 06:58 422_014_S128_R1.trim.sync.fastq
-rw-r-----  1 hh693 hh693  452M Jul 29 06:58 422_014_S128_R2.trim.sync.fastq
-rw-rw-r--  1 hh693 hh693  533M Jul 29 06:58 422_033_S135_R1.trim.sync.fastq
-rw-rw-r--  1 hh693 hh693  504M Jul 29 06:58 422_033_S135_R2.trim.sync.fastq
-rw-rw-r--  1 hh693 hh693  552M Jul 29 06:58 BAR_002_S136_R1.trim.sync.fastq
-rw-rw-r--  1 hh693 hh693  523M Jul 29 06:58 BAR_002_S136_R2.trim.sync.fastq
-rw-rw-r--  1 hh693 hh693  729M Jul 29 06:58 BLP0819_179_S101_R1.trim.sync.fastq
-rw-rw-r--  1 hh693 hh693  703M Jul 29 06:58 BLP0819_179_S101_R2.trim.sync.fastq
-rw-rw-r--  1 hh693 hh693  588M Jul 29 06:59 BLP0819_180_S108_R1.trim.sync.fastq
-rw-rw-r--  1 hh693 hh693  558M Jul 29 06:59 BLP0819_180_S108_R2.trim.sync.fastq
#Sync removed a little but not much.

#Looking good!

#Now run fastqc on them and compare to raw data and then do length adjustment
mkdir sync3_fastqc
mkdir sync3_multiqc

export PATH=/programs/FastQC-0.11.8:$PATH
export LC_ALL=en_US.UTF-8
export PYTHONPATH=/programs/multiqc-1.10.1/lib64/python3.6/site-packages:/programs/multiqc-1.10.1/lib/python3.6/site-packages
export PATH=/programs/multiqc-1.10.1/bin:$PATH

fastqc -t 12 -o sync3_fastqc sync_trim_3/*.fastq
multiqc -n multiqc_sync3_report.html -o sync3_multiqc ./sync3_fastqc

#If I remove duplicates should I do it before everything else or after?
#Try both?
export PATH=/programs/seqkit-0.15.0:$PATH
seqkit rmdup -h

#dedup 1 is this first attempt from already trimmed
#sync_trim_4 is the sync after this duplication

export PATH=/programs/seqkit-0.15.0:$PATH
while read p; do
	echo "$p"
	seqkit rmdup sync_trim_3/"$p"_R1.trim.sync.fastq -s -o dedup1/"$p"_R1.dedup.fastq -D dups1/"$p"_R1_dups.txt -d dups1/"$p"_R1_dup_seq.fastq -j 10
	seqkit rmdup sync_trim_3/"$p"_R2.trim.sync.fastq -s -o dedup1/"$p"_R2.dedup.fastq -D dups1/"$p"_R2_dups.txt -d dups1/"$p"_R2_dup_seq.fastq -j 10	
	perl resync.pl dedup1/"$p"_R1.dedup.fastq dedup1/"$p"_R2.dedup.fastq sync_trim_4/"$p"_R1.trim.sync.fastq sync_trim_4/"$p"_R2.trim.sync.fastq >& sync_trim_4/"$p"_resync4.log 
done <sim_sample.txt
#Yup, that's removing most all of the data

#dedup 2 is deduplicating from raw and then sync_5 is going through everything again post dups removed at the beginning

#1. remove dups from raw
export PATH=/programs/seqkit-0.15.0:$PATH
while read p; do
	echo "$p"
	seqkit rmdup similis_seq_raw/"$p"_R1_001.fastq.gz -s -o dedup2_raw/"$p"_R1.rawdedup.fastq -D dups2/"$p"_R1_dups.txt -d dups2/"$p"_R1_dup_seq.fastq -j 10
	seqkit rmdup similis_seq_raw/"$p"_R2_001.fastq.gz -s -o dedup2_raw/"$p"_R2.rawdedup.fastq -D dups2/"$p"_R2_dups.txt -d dups2/"$p"_R2_dup_seq.fastq -j 10	
done <sim_sample.txt
			#somehow it seems like it is remove less duplicates before the trimming - likely because of the poor read quality on the R2s not registering as dup and the unique padding on the R1s
			#but still removing a LOT
			#also keep in mind that rmdup is a rather crude way of doing this without any regard for PCR dups or otherwise and perhaps a better program exists.
#2. after removing dups from raw do fastqc
export PATH=/programs/FastQC-0.11.8:$PATH
export LC_ALL=en_US.UTF-8
export PYTHONPATH=/programs/multiqc-1.10.1/lib64/python3.6/site-packages:/programs/multiqc-1.10.1/lib/python3.6/site-packages
export PATH=/programs/multiqc-1.10.1/bin:$PATH

fastqc -t 20 -o dedup2_fastqc dedup2_raw/*.fastq
multiqc -n multiqc_dedup2_report.html -o dedup2_multiqc ./dedup2_fastqc

#3. then use perl on R1 and R2, adding back in the R2s
#4. then trimmomatic on both but differently
#6. resync

#5/7. cut lengths and resync



#!/bin/bash -l

#SBATCH --partition=regular
#SBATCH --job-name=rmdup_compare
#SBATCH --output=s_rmdup_compare.out
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --mail-user=hh693@cornell.edu

export PATH=/programs/seqtk:$PATH

while read p; do
echo "$p"
  perl gbstrim.pl --enzyme1 psti --enzyme2 mspi --fastqfile dedup2_raw/"$p"_R1.rawdedup.fastq --read R1 --outputfile padtrim_dedup/"$p"_R1.trim.fastq --verbose --threads 10 --minlength 50 >& gbstrim_stats/"$p"_R1_dedup.log 
  java -jar /programs/trimmomatic/trimmomatic-0.39.jar SE -phred33 -threads 8 dedup2_raw/"$p"_R2.rawdedup.fastq ommatic_dedup2/"$p"_R2.fastq ILLUMINACLIP:/programs/trimmomatic/adapters/NexteraPE-PE.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:50 >& ommatic_logs/"$p"_R2_trimmo_dedup.log
  seqtk trimfq -b 8 -e 8 ommatic_dedup2/"$p"_R2.fastq > ommatic_dedup2_phase2/"$p"_R2.fastq
  java -jar /programs/trimmomatic/trimmomatic-0.39.jar SE -phred33 -threads 8 padtrim_dedup/"$p"_R1.trim.fastq padtrim_dedup/R1s_with_slidewindow/"$p"_R1.trim.fastq SLIDINGWINDOW:4:15 MINLEN:50 >& ommatic_logs/"$p"_R1_trimmo_dedup.log
  perl resync.pl padtrim_dedup/R1s_with_slidewindow/"$p"_R1.trim.fastq ommatic_dedup2_phase2/"$p"_R2.fastq sync_trim_5/"$p"_R1.trim.sync.fastq sync_trim_5/"$p"_R2.trim.sync.fastq >& sync_trim_5/"$p"_resync5.log 
done <sim_sample.txt


sbatch --nodes=1 --ntasks=8 --mem=10000 s_rmdup_compare.sh


#read count compare sync_5 (dedup first) with sync_4 (dedup last)
#wc -l /4 so just wc -l will be fine for comparative reasons
fastqc -t 20 -o sync4_fastqc sync_trim_4/*.fastq
multiqc -n multiqc_sync4_report.html -o sync4_multiqc ./sync4_fastqc

wc -l sync_trim_4/*.fastq > reads_after_dedup &
wc -l sync_trim_3/*.fastq > reads_before_dedup &
#what percent of reads were lost on average?

    1833628 sync_trim_4/422_014_S128_R1.trim.sync.fastq
    1833628 sync_trim_4/422_014_S128_R2.trim.sync.fastq
    1414692 sync_trim_4/422_033_S135_R1.trim.sync.fastq
    1414692 sync_trim_4/422_033_S135_R2.trim.sync.fastq
    1409844 sync_trim_4/BAR_002_S136_R1.trim.sync.fastq
    1409844 sync_trim_4/BAR_002_S136_R2.trim.sync.fastq
    2217964 sync_trim_4/BLP0819_179_S101_R1.trim.sync.fastq
    2217964 sync_trim_4/BLP0819_179_S101_R2.trim.sync.fastq
    1487848 sync_trim_4/BLP0819_180_S108_R1.trim.sync.fastq
    1487848 sync_trim_4/BLP0819_180_S108_R2.trim.sync.fastq
    
    2208676 sync_trim_4/WFH_004_S39_R2.trim.sync.fastq
    2569780 sync_trim_4/WFH_005_S51_R1.trim.sync.fastq
    2569780 sync_trim_4/WFH_005_S51_R2.trim.sync.fastq
    1655264 sync_trim_4/WFH_006_S63_R1.trim.sync.fastq
    1655264 sync_trim_4/WFH_006_S63_R2.trim.sync.fastq
    2014900 sync_trim_4/WFH_007_S75_R1.trim.sync.fastq
    2014900 sync_trim_4/WFH_007_S75_R2.trim.sync.fastq
    2142952 sync_trim_4/WFH_008_S86_R1.trim.sync.fastq
    2142952 sync_trim_4/WFH_008_S86_R2.trim.sync.fastq
    
    #vs
    
    5721476 sync_trim_3/422_014_S128_R1.trim.sync.fastq
    5721476 sync_trim_3/422_014_S128_R2.trim.sync.fastq
    6443564 sync_trim_3/422_033_S135_R1.trim.sync.fastq
    6443564 sync_trim_3/422_033_S135_R2.trim.sync.fastq
    6670624 sync_trim_3/BAR_002_S136_R1.trim.sync.fastq
    6670624 sync_trim_3/BAR_002_S136_R2.trim.sync.fastq
    8973092 sync_trim_3/BLP0819_179_S101_R1.trim.sync.fastq
    8973092 sync_trim_3/BLP0819_179_S101_R2.trim.sync.fastq
    7145024 sync_trim_3/BLP0819_180_S108_R1.trim.sync.fastq
    7145024 sync_trim_3/BLP0819_180_S108_R2.trim.sync.fastq
    
    9228956 sync_trim_3/WFH_004_S39_R2.trim.sync.fastq
    8854644 sync_trim_3/WFH_005_S51_R1.trim.sync.fastq
    8854644 sync_trim_3/WFH_005_S51_R2.trim.sync.fastq
    8418292 sync_trim_3/WFH_006_S63_R1.trim.sync.fastq
    8418292 sync_trim_3/WFH_006_S63_R2.trim.sync.fastq
    9819924 sync_trim_3/WFH_007_S75_R1.trim.sync.fastq
    9819924 sync_trim_3/WFH_007_S75_R2.trim.sync.fastq
    9708284 sync_trim_3/WFH_008_S86_R1.trim.sync.fastq
    9708284 sync_trim_3/WFH_008_S86_R2.trim.sync.fastq
#By removing duplicates after everything else...
#32%, 21%, 21%, 24%, 21%, 23%, 29%, 19%, 20.5&%, 22%  of reads remain

#Now compare to sync5 - dedup done before trimming
wc -l sync_trim_5/*.fastq > reads_dedup_thentrim &

    3128404 sync_trim_5/422_014_S128_R1.trim.sync.fastq
    3128404 sync_trim_5/422_014_S128_R2.trim.sync.fastq
    2622852 sync_trim_5/422_033_S135_R1.trim.sync.fastq
    2622852 sync_trim_5/422_033_S135_R2.trim.sync.fastq
    2643124 sync_trim_5/BAR_002_S136_R1.trim.sync.fastq
    2643124 sync_trim_5/BAR_002_S136_R2.trim.sync.fastq
    3972900 sync_trim_5/BLP0819_179_S101_R1.trim.sync.fastq
    3972900 sync_trim_5/BLP0819_179_S101_R2.trim.sync.fastq
    2905416 sync_trim_5/BLP0819_180_S108_R1.trim.sync.fastq
    2905416 sync_trim_5/BLP0819_180_S108_R2.trim.sync.fastq
    
#this is better, it preserves closer to 55% and thus 700k reads
mkdir sync5_multiqc
fastqc -t 2 -o sync5_fastqc sync_trim_5/*.fastq
multiqc -n multiqc_sync5_report.html -o sync5_multiqc ./sync5_fastqc

#Higher % duplication by fastqcs standards but also more reads. 
#Go with this method.

#Trim to equal lengths resync 5
export PYTHONPATH=/programs/cutadapt-3.4/lib/python3.6/site-packages:/programs/cutadapt-3.4/lib64/python3.6/site-packages
export PATH=/programs/cutadapt-3.4/bin:$PATH
cutdapt -l 125 -m 125 -j 8 -o file1.trimmed.fq -p file2.trimmed.fq file1.fq file2.fq
#-l cuts to a certain length, -m is discard reads shorter than



#!/bin/bash -l

#SBATCH --partition=regular
#SBATCH --job-name=cutlength
#SBATCH --output=s_cutlength.out
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --mail-user=hh693@cornell.edu

export PYTHONPATH=/programs/cutadapt-3.4/lib/python3.6/site-packages:/programs/cutadapt-3.4/lib64/python3.6/site-packages
export PATH=/programs/cutadapt-3.4/bin:$PATH

while read p; do
echo "$p"
cutdapt -l 125 -m 125 -j 8 -o sync_5_length4STACKS/cutadapted/"$p"_R1.lengthtrim.fastq -p sync_5_length4STACKS/cutadapted/"$p"_R2.lengthtrim.fastq sync_trim_5/"$p"_R1.trim.sync.fastq sync_trim_5/"$p"_R2.trim.sync.fastq
perl resync.pl sync_5_length4STACKS/cutadapted/"$p"_R1.lengthtrim.fastq sync_5_length4STACKS/cutadapted/"$p"_R2.lengthtrim.fastq sync_5_length4STACKS/resync/"$p"_R1.trim.sync.fastq sync_5_length4STACKS/resync/"$p"_R2.trim.sync.fastq >& sync_5_length4STACKS/resync/"$p"_resync5cut.log 
done <sim_sample.txt

sbatch --nodes=1 --ntasks=8 --mem=15000 s_cutlength4STACKS.sh

export PATH=/programs/FastQC-0.11.8:$PATH
export LC_ALL=en_US.UTF-8
export PYTHONPATH=/programs/multiqc-1.10.1/lib64/python3.6/site-packages:/programs/multiqc-1.10.1/lib/python3.6/site-packages
export PATH=/programs/multiqc-1.10.1/bin:$PATH
mkdir sync5_multiqc_length
mkdir sync5_lengthfastqc
fastqc -t 30 -o sync5_lengthfastqc sync_5_length4STACKS/cutadapted/*.fastq
multiqc -n multiqc_sync5length_report.html -o sync5_multiqc_length ./sync5_lengthfastqc
#WORKED
#May not have needed to be resynced
#Woohoo, tomorrow we ride! Into STACKS M parameter test

#Date: Aug 2
#The data to use is:
sync_5_length4STACKS/resync/"$p"_R1.trim.sync.fastq
sync_5_length4STACKS/resync/"$p"_R2.trim.sync.fastq

#Use the same popmap I already set for these based on raw data size
sbatch --nodes=1 --ntasks=25 --mem=60000 slurm_stacks_mtest3.sh

#check for errors
grep -iE "\b(err|e:|warn|w:|fail|abort)" stacks.M*/denovo_map.log | less
#no errors

#filter to r80
sbatch --nodes=1 --ntasks=25 --mem=20000 slurm_locishared80_test1.sh

#recreate the same graphs
#statistics in...
batch_1.populations.log

#Set M to 4
CTM_002_S13_R1.trim.sync: 9.49483x
CTM_009_S2_R1.trim.sync: 12.4268x
PPB_001_S26_R1.trim.sync: 15.5575x
PPB_006_S85_R1.trim.sync: 13.6634x
MW_010_S20_R1.trim.sync: 10.0904x
MW_063_S97_R1.trim.sync: 11.1579x
CTM_001_S1_R1.trim.sync: 10.9451x
CTM_006_S61_R1.trim.sync: 9.92319x
422_033_S135_R1.trim.sync: 8.89544x
422_014_S128_R1.trim.sync: 11.1429x
CTM_004_S37_R1.trim.sync: 13.2761x

#But woah, my depth is way worse this time. Still largely ~10x but that's a bummer.
#Coverage is underestimated because of heterozygotes

#!/bin/bash -l

#SBATCH --partition=regular
#SBATCH --job-name=ustacks_full
#SBATCH --output=ustacks_full.out
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --mail-user=hh693@cornell.edu

count=1
while read p; do
echo "$p"
echo "R1"
samplea="$p"_R1
sample_indexa="$count"
fq_filea=../sync_5_length4STACKS/cutadapted/"$p"_R1.lengthtrim.fastq
log_filea=./ulogs/"$p"_R1.ustacks.oe
/programs/stacks/bin/ustacks -f $fq_filea -i $sample_indexa -o ./ -M 4 -m 3 -p 15 &> $log_filea
echo "R2"
sampleb="$p"_R2
sample_indexb="$(($count + 1))"
fq_fileb=../sync_5_length4STACKS/cutadapted/"$p"_R2.lengthtrim.fastq
log_fileb=./ulogs/"$p"_R2.ustacks.oe
/programs/stacks/bin/ustacks -f $fq_fileb -i $sample_indexb -o ./ -M 4 -m 3 -p 30 &> $log_fileb
count="$(($count + 2))"
done <../sim_sample.txt

sbatch --nodes=1 --ntasks=25 --mem=89000 s_sim_uSTACK.sh

#Finished, not sure how long it took

#For some reason R2 did not work....
#named my sample b wrong

#Now c stacks
#try to do parallel

#select 40-200 samples for catalog
#Pick samples that have high coverages and are representative of the genetic diversity in the data set (alleles that are not observed in the selected samples will subsequently be ignored)
#As with the test samples (Step 14), write this list as a population map popmap.catalog.tsv and place it in the info/ directory.

#So as to not take forever time, I could base this on file size/number of reads
#compare my coverage calculations for the 12 sample pilot to raw reads file size and ustacks file sizes

#Set M to 4									RawR1	allele	model	snps	tags
422_014_S128_R1.trim.sync: 	11	.1429x		145		342		11		116		114
422_033_S135_R1.trim.sync: 	8	.89544x		152		359		13		133		105
CTM_001_S1_R1.trim.sync: 	10	.9451x		295		548		19		198		183
CTM_002_S13_R1.trim.sync: 	9	.49483x		246		505		18		185		150
CTM_004_S37_R1.trim.sync: 	13	.2761x		203		373		12		125		140	
CTM_006_S61_R1.trim.sync: 	9	.92319x		213		417		15		156		132
CTM_009_S2_R1.trim.sync: 	12	.4268x		366		493		17		179		185
MW_010_S20_R1.trim.sync: 	10	.0904x		199		382		13		144		119
MW_063_S97_R1.trim.sync: 	11	.1579x		209		394		13		143		129	
PPB_001_S26_R1.trim.sync: 	15	.5575x		300		477		15		163		202
PPB_006_S85_R1.trim.sync: 	13	.6634x		322		483		16		173		190

#after sstacks, use gstacks to combine paired end reads
#Generally if tags > 115M then coverage will be greater than or around 10

#Sort by size to determine which ones to leave off
ls -aShl *tags.tsv

#check if these were also the ones with the lowest raw read file size
#not the same but some familiar names

#Date: Aug 3rd

#how many do I have?
#150 x 2 =300
#atleast one per site

#should I be separating them in to populations different from just their site codes?
#all LI goes together
#what about CTM and PPB?

#site codes
422					SJF		South Jersey Federal Survey
BAR					CCB		Cape Cod Bay 2017 inside bay
BLP					SLI		Long Island 2019
CTM					SCC		Cape Cod Bay 2017 south of bay
CUP					SLI		Long Island 2019
ELP					SCC		Cape Cod Bay 2017 south of bay
GA					GA		Georgia
GBE					GBE		Georges Bank
GLD					NLI		Long Island 2019 
MW					SCC		Southern Cape Cod 2012 south of bay
PEC					NLI		Long Island 2019
PPB					SCC		Cape Cod Bay 2017 south of bay
PT					CCB 	Cape Cod Bay 2012
RP20				NLI
WFH					SCC		Cape Cod Bay 2017 south of bay Buzzard Bay

GBE
SJF
CCB
SCC
NLI
SLI
GA



#divide into these populations but still make sure to include at least one from each population
#also check R2 tag sizes for potential removal
touch ../info/popmap.catalog.tsv
#for just the samples in the catalog, including R2s of each as well

#For their data set, we picked the ten samples with the highest coverage in each population (ignoring one sample with extremely high coverage), for a total of 40 samples.
#Include the top 5 from each site or all from a given site

ls -ahl *tag* > taglist.txt
#import into excel
#Data > text to collumn to separate

#Make sure you get the interesting samples in the catalog
#Who are interesting
#MW1-3 - Nick's Admixed
#MW16 - has a low tag score but non-failure in raw reads
#MW22 - My heterozygote from primer testing
#MW31
#MW33
#PEC13

#I've been keeping closer to 6 per site, but also probably still have too many (146 including R2s, down to 138)

#Setting n=M=4 here

/programs/stacks/bin/cstacks -P ./ -M ../info/popmap.catalog.tsv -n 4 -p 24 &> cstacks.oe

sbatch --nodes=1 --ntasks=26 --mem=30000 s_cstacks.sh
#takes about 7 hours
#I think that the second half might take more time cause each one you add has to cross reference with each of the others
#Also doesn't really seem like it takes good advantage of the cpu

#Check progress
less cstacks.oe | tail



#run sstacks in parallel to align each sample to the catalog
sample=cs_1335.01
log_file=$sample.sstacks.oe
sstacks -c ./batch_1 -s ./$sample -o ./ &> slogs/$log_file

#make job list
while read p; do
echo "echo $p"
echo "echo "R1""
echo "/programs/stacks/bin/sstacks -c ./batch_1 -s ./"$p"_R1 -o ./ &> slogs/"$p"_R1.sstacks.oe"
echo "echo "R2""
echo "/programs/stacks/bin/sstacks -c ./batch_1 -s ./"$p"_R2 -o ./ &> slogs/"$p"_R2.sstacks.oe"
done <../sim_sample.txt > j_sstacks.txt

#make that job list into a parallel that then gets fed into slurm

#!/bin/bash -l

#SBATCH --partition=regular
#SBATCH --job-name=j_sstacks
#SBATCH --output=sstacks_jlist_test.out
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --mail-user=hh693@cornell.edu

parallel -j 20 < j_sstacks.txt

sbatch --nodes=1 --ntasks=26 --mem=89000 s_j_sstacks.sh
#FORGOT TO include /programs/stacks/bin/ in job list so I pulled the path for 2.53
#ERROR: unable to parse batch_1.catalog
#that's the issue I'm seeing noww, because the format is slightly different for the catalog in later versions
#this may not play nice with others...
#Fixed it to all be the same version but it still can't parse it
#then I pulled the batch1 stuff out of its own folder, back into the main zone (copied so in both places actually and it seems to be running!)

#did not work because popmap did not include length trim
#change file names
#try the function
rename

#rename the tsvs in this folder and make sure those names match the ones in the read file
#keeping Rs at first but if it isn't working then replace R with .
# https://catchenlab.life.illinois.edu/stacks/manual/#gfiles - 4.4.1


#first run tsv2bam
# Run tsv2bam to transpose the data so it is stored by locus, instead of by sample. We will include
# paired-end reads using tsv2bam. tsv2bam expects the paired read files to be in the samples
# directory and they should be named consistently with the single-end reads,
# e.g. sample_01.1.fq.gz and sample_01.2.fq.gz, which is how process_radtags will output them.
tsv2bam -P ./ -M ../info/popmap.all.tsv --pe-reads-dir $src/samples -t 8

#Make new popmap with all
#Make a directory were you can read in 

#run gstacks
/programs/stacks/bin/gstacks #does not exist
#but it does exist under 
/programs/stacks-2.0/bin/
#so I did everything else in STACKS 1.48
#but gstacks in 2.53
#for solidissima, do everything in 2.53?
export LD_LIBRARY_PATH=/usr/local/gcc-7.3.0/lib64:/usr/local/gcc-7.3.0/lib
export PATH=/programs/stacks-2.53/bin:$PATH
#first do tsv2bam because gstacks input takes bam
